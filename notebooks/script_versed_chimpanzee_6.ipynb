{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaRsjuhwdGkL"
   },
   "source": [
    "## Common Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26823,
     "status": "ok",
     "timestamp": 1643399504606,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "MaWqZsOKUcSW",
    "outputId": "3613266f-824a-4e45-fcc4-4fef32c1b4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "'AC Use Cases Draft.pdf'\t     interim_data\n",
      " customers.csv\t\t\t     Plan.gdoc\n",
      "'data columns with colors.pdf'\t     submission_random.csv\n",
      "'Feature Importance Scores.gsheet'   transactions.csv\n",
      " geo.csv\t\t\t    'Versed Chimpanzee.ipynb'\n",
      "'Info - Analytics Cup 2022.pdf'\n"
     ]
    }
   ],
   "source": [
    "# Google Drive Operations - Only for Google Drive, Delete in Local Settings\n",
    "# Reference for using R in Colab: https://towardsdatascience.com/how-to-use-r-in-google-colab-b6e02d736497\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Versed Chimpanzee - AC Group\")\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50pscwa67S3q"
   },
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLlrPqha6VHF"
   },
   "source": [
    "### 1.1 Prepare Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2121,
     "status": "ok",
     "timestamp": 1643399506718,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "DW9le4Sy6plU",
    "outputId": "a0236eff-cead-46c6-bf8f-2182453f651b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "R[write to console]: ✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n",
      "✔ tibble  3.1.6     ✔ stringr 1.4.0\n",
      "✔ tidyr   1.1.4     ✔ forcats 0.5.1\n",
      "✔ readr   2.1.1     \n",
      "\n",
      "R[write to console]: ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(dplyr, warn.conflicts = F, quietly = T)\n",
    "library(tidyverse, warn.conflicts = F, quietly = T)\n",
    "library(lubridate, warn.conflicts = F, quietly = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxpfOCPV7Lap"
   },
   "source": [
    "### 1.2 Load and Inspect Transactions (transactions.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3041,
     "status": "ok",
     "timestamp": 1643399509751,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "kLCC7zAac_H6",
    "outputId": "71e62d53-0a83-4b47-847a-a988be5fc75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 26151 Columns: 23\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (12): MO_ID, SO_ID, CUSTOMER, END_CUSTOMER, PRICE_LIST, MO_CREATED_DATE,...\n",
      "dbl (11): OFFER_PRICE, SERVICE_LIST_PRICE, MATERIAL_COST, SERVICE_COST, ISIC...\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"MO_ID: 0/26151\"\n",
      "[1] \"SO_ID: 0/26151\"\n",
      "[1] \"CUSTOMER: 0/26151\"\n",
      "[1] \"END_CUSTOMER: 20114/26151\"\n",
      "[1] \"OFFER_PRICE: 0/26151\"\n",
      "[1] \"SERVICE_LIST_PRICE: 0/26151\"\n",
      "[1] \"MATERIAL_COST: 0/26151\"\n",
      "[1] \"SERVICE_COST: 0/26151\"\n",
      "[1] \"PRICE_LIST: 0/26151\"\n",
      "[1] \"ISIC: 1675/26151\"\n",
      "[1] \"MO_CREATED_DATE: 0/26151\"\n",
      "[1] \"SO_CREATED_DATE: 0/26151\"\n",
      "[1] \"TECH: 0/26151\"\n",
      "[1] \"OFFER_TYPE: 0/26151\"\n",
      "[1] \"BUSINESS_TYPE: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_A: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_B: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_C: 0/26151\"\n",
      "[1] \"OFFER_STATUS: 2576/26151\"\n",
      "[1] \"COSTS_PRODUCT_D: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_E: 0/26151\"\n",
      "[1] \"SALES_LOCATION: 37/26151\"\n",
      "[1] \"TEST_SET_ID: 23575/26151\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "trnsc_df = as_tibble(read_csv(\"transactions.csv\"))\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(trnsc_df)) {\n",
    "  print(paste0(names(trnsc_df)[i], \": \", sum(is.na(trnsc_df[, i])), \"/\", nrow(trnsc_df)))\n",
    "}\n",
    "\n",
    "trnsc_df$CUSTOMER = (substring(trnsc_df$CUSTOMER, 2, nchar(trnsc_df$CUSTOMER)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QriuLKgN783y"
   },
   "source": [
    "### 1.3 Load, Inspect and Merge Geographic Data (geo.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1643399510076,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "464Zi2qI6NRF",
    "outputId": "6e5035ea-30cc-40fa-9bb2-a528e6675049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 46 Columns: 4\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (4): COUNTRY, SALES_OFFICE, SALES_BRANCH, SALES_LOCATION\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"COUNTRY: 0/46\"\n",
      "[1] \"SALES_OFFICE: 2/46\"\n",
      "[1] \"SALES_BRANCH: 1/46\"\n",
      "[1] \"SALES_LOCATION: 1/46\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "geo_df = as_tibble(read_csv(\"geo.csv\"))\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(geo_df)) {\n",
    "  print(paste0(names(geo_df)[i], \": \", sum(is.na(geo_df[, i])), \"/\", nrow(geo_df)))\n",
    "}\n",
    "# Rename column COUNTRY COUNTRY_CODE, since it only contains codes like CH, FR\n",
    "geo_df = rename(geo_df, COUNTRY_CODE = COUNTRY)\n",
    "\n",
    "# Perform left join using dplyr\n",
    "trnsc_geo_df = left_join(trnsc_df, geo_df, by = 'SALES_LOCATION')\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = trnsc_geo_df, file = \"interim_data/trnsc_geo_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dj7zs3GH8ZJB"
   },
   "source": [
    "### 1.4 Load, Inspect and Merge Customers Data (customers.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1643399511504,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "ZyDvvzSp8jt7",
    "outputId": "f0e362fd-6418-4e85-8498-cffe20914adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8452 Columns: 8\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (5): REV_CURRENT_YEAR, CREATION_YEAR, OWNERSHIP, COUNTRY, CURRENCY\n",
      "dbl (3): CUSTOMER, REV_CURRENT_YEAR.1, REV_CURRENT_YEAR.2\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"CUSTOMER: 0/8452\"\n",
      "[1] \"REV_CURRENT_YEAR: 0/8452\"\n",
      "[1] \"REV_CURRENT_YEAR.1: 0/8452\"\n",
      "[1] \"REV_CURRENT_YEAR.2: 0/8452\"\n",
      "[1] \"CREATION_YEAR: 0/8452\"\n",
      "[1] \"OWNERSHIP: 0/8452\"\n",
      "[1] \"COUNTRY: 0/8452\"\n",
      "[1] \"CURRENCY: 0/8452\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "cst_df = as_tibble(read_csv(\"customers.csv\"))\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(cst_df)) {\n",
    "  print(paste0(names(cst_df)[i], \": \", sum(is.na(cst_df[, i])), \"/\", nrow(cst_df)))\n",
    "}\n",
    "\n",
    "##### NOTE FROM TEO: I THINK THE MERGE MAY BE WRONG, I'M MARKING THE SECTION ##########\n",
    "##### WHICH I THINK MAY NEED REPLACEMENT (SEE MY SECTION) #################\n",
    "##### REPLACE FROM HERE ###################\n",
    "# Change data type of CUSTOMER column in customers dataset\n",
    "cst_df$CUSTOMER <- as.character(cst_df$CUSTOMER)\n",
    "\n",
    "# Create IDX_CUSTOMER for trnsc_geo_df\n",
    "trnsc_geo_df = mutate(trnsc_geo_df,\n",
    "                      IDX_CUSTOMER = paste0(COUNTRY_CODE, \"_\", CUSTOMER))\n",
    "\n",
    "# Create IDX_CUSTOMER for customers\n",
    "cst_df = cst_df %>% mutate(COUNTRY_CODE = case_when(\n",
    "  COUNTRY == 'Switzerland' ~ \"CH\",\n",
    "  COUNTRY == 'France' ~ \"FR\"\n",
    "))\n",
    "\n",
    "cst_df = mutate(cst_df, IDX_CUSTOMER = paste0(COUNTRY_CODE, \"_\", CUSTOMER))\n",
    "\n",
    "# Perform left join using dplyr\n",
    "all_merged = left_join(trnsc_geo_df, cst_df, by = 'IDX_CUSTOMER')\n",
    "####### TO HERE ###############################\n",
    "\n",
    "# FIXME: Delete this before submisssion.\n",
    "write_csv(x = all_merged, file = \"interim_data/all_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcIb0u_B9FQ-"
   },
   "source": [
    "### 1.5 Fix Basic Problems in Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1643399511925,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "_2v-a-dD9cZJ",
    "outputId": "47bc2733-02b1-446e-c03b-02e26aef096f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] Number Of Cols: 36\"\n",
      "[1] \"[INFO] Number Of Rows: 26151\"\n",
      "[INFO] Names Of Columns:\n",
      " \"MO_ID\",  \"SO_ID\",  \"CUSTOMER.x\",  \"END_CUSTOMER\",  \"OFFER_PRICE\",  \"SERVICE_LIST_PRICE\",  \"MATERIAL_COST\",  \"SERVICE_COST\",  \"PRICE_LIST\",  \"ISIC\",  \"MO_CREATED_DATE\",  \"SO_CREATED_DATE\",  \"TECH\",  \"OFFER_TYPE\",  \"BUSINESS_TYPE\",  \"COSTS_PRODUCT_A\",  \"COSTS_PRODUCT_B\",  \"COSTS_PRODUCT_C\",  \"OFFER_STATUS\",  \"COSTS_PRODUCT_D\",  \"COSTS_PRODUCT_E\",  \"SALES_LOCATION\",  \"TEST_SET_ID\",  \"COUNTRY_CODE.x\",  \"SALES_OFFICE\",  \"SALES_BRANCH\",  \"IDX_CUSTOMER\",  \"CUSTOMER.y\",  \"REV_CURRENT_YEAR\",  \"REV_CURRENT_YEAR.1\",  \"REV_CURRENT_YEAR.2\",  \"CREATION_YEAR\",  \"OWNERSHIP\",  \"COUNTRY\",  \"CURRENCY\",  \"COUNTRY_CODE.y\", \n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "[1] \"[INFO] Glimpse:\"\n",
      "Rows: 26,151\n",
      "Columns: 36\n",
      "$ MO_ID              <chr> \"a050N000013fnfrQAA\", \"a050N000013fgL1QAI\", \"a050N0…\n",
      "$ SO_ID              <chr> \"a030N00001EochoQAB\", \"a030N00001EociNQAR\", \"a030N0…\n",
      "$ CUSTOMER.x         <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", …\n",
      "$ END_CUSTOMER       <chr> NA, NA, NA, \"4\", NA, NA, NA, NA, \"272\", NA, NA, \"No…\n",
      "$ OFFER_PRICE        <dbl> 1711.00, 26687.60, 6264.70, 4300.20, 13693.00, 2340…\n",
      "$ SERVICE_LIST_PRICE <dbl> 1395, 14651, 2296, 310, 5815, 5932, 930, 2310, 1207…\n",
      "$ MATERIAL_COST      <dbl> 1107, 9282, 1722, 246, 4674, 4674, 738, 1845, 7650,…\n",
      "$ SERVICE_COST       <dbl> 186.30, 7768.34, 2168.56, 2775.92, 4179.38, 15186.3…\n",
      "$ PRICE_LIST         <chr> \"SFT Standard\", \"CMT Installer\", \"SFT Standard\", \"S…\n",
      "$ ISIC               <dbl> 2100, 7110, 6820, 3821, 4719, 6419, 2710, 4742, 811…\n",
      "$ MO_CREATED_DATE    <chr> \"14.01.2019 08:43\", \"12.01.2019 16:36\", \"14.01.2019…\n",
      "$ SO_CREATED_DATE    <chr> \"14.01.2019 08:45\", \"14.01.2019 08:50\", \"14.01.2019…\n",
      "$ TECH               <chr> \"S\", \"C\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"C\", \"F\", \"…\n",
      "$ OFFER_TYPE         <chr> \"IN\", \"D\", \"FIR\", \"FIR\", \"FIR\", \"FIR\", \"FIR\", \"FIR\"…\n",
      "$ BUSINESS_TYPE      <chr> \"E\", \"N\", \"E\", \"M\", \"E\", \"N\", \"E\", \"E\", \"E\", \"E\", \"…\n",
      "$ COSTS_PRODUCT_A    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1000, …\n",
      "$ COSTS_PRODUCT_B    <dbl> 59.48, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.…\n",
      "$ COSTS_PRODUCT_C    <dbl> 0.00, 0.00, 0.00, 0.00, 1854.01, 0.00, 0.00, 0.00, …\n",
      "$ OFFER_STATUS       <chr> \"LOsT\", \"Lost\", \"WIN\", \"Win\", \"WIN\", NA, \"WIN\", \"Wo…\n",
      "$ COSTS_PRODUCT_D    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n",
      "$ COSTS_PRODUCT_E    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n",
      "$ SALES_LOCATION     <chr> \"Luzern Central\", \"Zürich East\", \"Luzern Central\", …\n",
      "$ TEST_SET_ID        <dbl> NA, NA, NA, NA, NA, 6, NA, NA, 9, NA, NA, NA, NA, 1…\n",
      "$ COUNTRY_CODE.x     <chr> \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH…\n",
      "$ SALES_OFFICE       <chr> \"Luzern\", \"Zürich\", \"Luzern\", \"Basel\", \"Geneva\", \"G…\n",
      "$ SALES_BRANCH       <chr> \"Branch Central\", \"Branch East\", \"Branch Central\", …\n",
      "$ IDX_CUSTOMER       <chr> \"CH_1\", \"CH_2\", \"CH_3\", \"CH_4\", \"CH_5\", \"CH_6\", \"CH…\n",
      "$ CUSTOMER.y         <chr> \"1\", \"2\", NA, \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"…\n",
      "$ REV_CURRENT_YEAR   <chr> \"\\\"81283.9230769231\\\"\", \"\\\"0\\\"\", NA, \"\\\"12668.84\\\"\"…\n",
      "$ REV_CURRENT_YEAR.1 <dbl> 81283.92, 0.00, NA, 12668.84, 7130.98, 518017.92, 2…\n",
      "$ REV_CURRENT_YEAR.2 <dbl> 32203.62, 0.00, NA, 32731.18, 10210.86, 736630.99, …\n",
      "$ CREATION_YEAR      <chr> \"01/01/2004\", \"01.01.2004\", NA, \"01.01.2003\", \"01/0…\n",
      "$ OWNERSHIP          <chr> \"Privately Owned/Publicly Traded\", \"Privately Owned…\n",
      "$ COUNTRY            <chr> \"Switzerland\", \"Switzerland\", NA, \"Switzerland\", \"S…\n",
      "$ CURRENCY           <chr> \"Chinese Yuan\", \"Chinese Yuan\", NA, \"Euro\", \"Euro\",…\n",
      "$ COUNTRY_CODE.y     <chr> \"CH\", \"CH\", NA, \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\",…\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "[1] \"[INFO] First 20 Rows:\"\n",
      "# A tibble: 20 × 36\n",
      "   MO_ID              SO_ID CUSTOMER.x END_CUSTOMER OFFER_PRICE SERVICE_LIST_PR…\n",
      "   <chr>              <chr> <chr>      <chr>              <dbl>            <dbl>\n",
      " 1 a050N000013fnfrQAA a030… 1          <NA>               1711              1395\n",
      " 2 a050N000013fgL1QAI a030… 2          <NA>              26688.            14651\n",
      " 3 a050N000013fnwdQAA a030… 3          <NA>               6265.             2296\n",
      " 4 a050N000013foAGQAY a030… 4          4                  4300.              310\n",
      " 5 a050N000013foKVQAY a030… 5          <NA>              13693              5815\n",
      " 6 a050N000013fpJ9QAI a030… 6          <NA>              23404.             5932\n",
      " 7 a050N000013fpYOQAY a030… 7          <NA>               1287               930\n",
      " 8 a050N000013fqA8QAI a030… 8          <NA>               8620.             2310\n",
      " 9 a050N00001B3D8RQAV a030… 9          272               52884.            12075\n",
      "10 a050N00001B3DXOQA3 a030… 10         <NA>               6485              2325\n",
      "11 a050N00001B3EpyQAF a030… 11         <NA>               1065.             1040\n",
      "12 a050N00001B3DePQAV a030… 12         No                 3935              3730\n",
      "13 a050N00001B3FICQA3 a030… 13         <NA>              29279              8050\n",
      "14 a050N00001B3FHTQA3 a030… 14         <NA>               6704              3275\n",
      "15 a050N00001B3FQBQA3 a030… 15         <NA>               2863.              805\n",
      "16 a050N000013fKrNQAU a030… 16         <NA>              30942.            14480\n",
      "17 a050N000013fLN9QAM a030… 17         <NA>               5605              2480\n",
      "18 a050N000013fN8EQAU a030… 18         <NA>               4325              1365\n",
      "19 a050N000013fNDYQA2 a030… 19         <NA>               1076.              775\n",
      "20 a050N000013fNTqQAM a030… 20         <NA>               1486.              915\n",
      "# … with 30 more variables: MATERIAL_COST <dbl>, SERVICE_COST <dbl>,\n",
      "#   PRICE_LIST <chr>, ISIC <dbl>, MO_CREATED_DATE <chr>, SO_CREATED_DATE <chr>,\n",
      "#   TECH <chr>, OFFER_TYPE <chr>, BUSINESS_TYPE <chr>, COSTS_PRODUCT_A <dbl>,\n",
      "#   COSTS_PRODUCT_B <dbl>, COSTS_PRODUCT_C <dbl>, OFFER_STATUS <chr>,\n",
      "#   COSTS_PRODUCT_D <dbl>, COSTS_PRODUCT_E <dbl>, SALES_LOCATION <chr>,\n",
      "#   TEST_SET_ID <dbl>, COUNTRY_CODE.x <chr>, SALES_OFFICE <chr>,\n",
      "#   SALES_BRANCH <chr>, IDX_CUSTOMER <chr>, CUSTOMER.y <chr>, …\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(paste(\"[INFO] Number Of Cols:\", ncol(all_merged)))\n",
    "print(paste(\"[INFO] Number Of Rows:\", nrow(all_merged)))\n",
    "cat(\"[INFO] Names Of Columns:\\n\", sprintf(\"\\\"%s\\\", \", names(all_merged)))\n",
    "\n",
    "cat(\"\\n\\n----------------------------------------------------\\n\\n\")\n",
    "\n",
    "print(\"[INFO] Glimpse:\")\n",
    "glimpse(all_merged)\n",
    "\n",
    "cat(\"\\n----------------------------------------------------\\n\\n\")\n",
    "\n",
    "print(\"[INFO] First 20 Rows:\")\n",
    "print(head(all_merged, n = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1643399511925,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "2x9cPHRc-qlR"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Feature Elimination & Fix\n",
    "all_merged = select(all_merged, -c(COUNTRY_CODE.y, CUSTOMER.y, COUNTRY, MO_ID, SO_ID))\n",
    "\n",
    "# Feature Renaming\n",
    "all_merged = rename(all_merged, COUNTRY_CODE = COUNTRY_CODE.x)\n",
    "all_merged = rename(all_merged, CUSTOMER = CUSTOMER.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1643399512471,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "TNHb5bxY9tfi",
    "outputId": "02f621c6-bdaf-472e-f471-7d36f32f2dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"CUSTOMER: 0/26151\"\n",
      "[1] \"END_CUSTOMER: 20114/26151\"\n",
      "[1] \"OFFER_PRICE: 0/26151\"\n",
      "[1] \"SERVICE_LIST_PRICE: 0/26151\"\n",
      "[1] \"MATERIAL_COST: 0/26151\"\n",
      "[1] \"SERVICE_COST: 0/26151\"\n",
      "[1] \"PRICE_LIST: 0/26151\"\n",
      "[1] \"ISIC: 1675/26151\"\n",
      "[1] \"MO_CREATED_DATE: 0/26151\"\n",
      "[1] \"SO_CREATED_DATE: 0/26151\"\n",
      "[1] \"TECH: 0/26151\"\n",
      "[1] \"OFFER_TYPE: 0/26151\"\n",
      "[1] \"BUSINESS_TYPE: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_A: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_B: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_C: 0/26151\"\n",
      "[1] \"OFFER_STATUS: 2576/26151\"\n",
      "[1] \"COSTS_PRODUCT_D: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_E: 0/26151\"\n",
      "[1] \"SALES_LOCATION: 37/26151\"\n",
      "[1] \"TEST_SET_ID: 23575/26151\"\n",
      "[1] \"COUNTRY_CODE: 0/26151\"\n",
      "[1] \"SALES_OFFICE: 38/26151\"\n",
      "[1] \"SALES_BRANCH: 37/26151\"\n",
      "[1] \"IDX_CUSTOMER: 0/26151\"\n",
      "[1] \"REV_CURRENT_YEAR: 2885/26151\"\n",
      "[1] \"REV_CURRENT_YEAR.1: 2885/26151\"\n",
      "[1] \"REV_CURRENT_YEAR.2: 2885/26151\"\n",
      "[1] \"CREATION_YEAR: 2885/26151\"\n",
      "[1] \"OWNERSHIP: 2885/26151\"\n",
      "[1] \"CURRENCY: 2885/26151\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(all_merged)) { # for-loop over columns\n",
    "  print(paste0(names(all_merged)[i], \": \", sum(is.na(all_merged[, i])), \"/\", nrow(all_merged)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1643399513440,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "4EJQ1aUm-BQe",
    "outputId": "6245ded8-0d56-4a78-b581-43f64b66a95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] Unique values in each column:\"\n",
      "[1] \"PRICE_LIST: c(\\\"SFT Standard\\\", \\\"CMT Installer\\\", \\\"CMT End Customer\\\", \\\"Tarif public\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"TECH: c(\\\"S\\\", \\\"C\\\", \\\"F\\\", \\\"BP\\\", \\\"FP\\\", \\\"EPS\\\", \\\"E\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"BUSINESS_TYPE: c(\\\"E\\\", \\\"N\\\", \\\"M\\\", \\\"C\\\", \\\"T\\\", \\\"Exp\\\", \\\"New\\\", \\\"Mig\\\", \\\"S\\\", \\\"F\\\", \\\"R\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"OFFER_STATUS: c(\\\"LOsT\\\", \\\"Lost\\\", \\\"WIN\\\", \\\"Win\\\", NA, \\\"Won\\\", \\\"LOST\\\", \\\"Lose\\\", \\\"WON\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"COUNTRY_CODE: c(\\\"CH\\\", \\\"FR\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"SALES_BRANCH: c(\\\"Branch Central\\\", \\\"Branch East\\\", \\\"Branch West\\\", \\\"EPS CH\\\", NA, \\\"Grand Paris\\\", \\\"Sud Ouest\\\", \\\"Nord FR\\\", \\\"Ouest\\\", \\\"Centre-Est\\\", \\\"Grand Est\\\", \\\"Sud-Est\\\", \\\"Enterprise Business France\\\", \\\"SI\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"OWNERSHIP: c(\\\"Privately Owned/Publicly Traded\\\", NA, \\\"Governmental\\\", \\\"No information\\\", \\\"Individual Person\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"CURRENCY: c(\\\"Chinese Yuan\\\", NA, \\\"Euro\\\", \\\"US Dollar\\\", \\\"Pound Sterling\\\")\"\n",
      "[1] \"---------------\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(\"[INFO] Unique values in each column:\")\n",
    "for (i in 1:ncol(all_merged)) {\n",
    "  count_unq_vals = count(unique(all_merged[, i]))\n",
    "  if (count_unq_vals <20) {\n",
    "    print(paste0(names(all_merged)[i], \": \", unique(all_merged[, i])))\n",
    "    print(\"---------------\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = all_merged, file = \"interim_data/all_merged_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1643399513847,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "MT5oWavbcb6l"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "merged_df = all_merged\n",
    "write_csv(x = all_merged, file = \"interim_data/merged_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7hDf8Sr_-dW"
   },
   "source": [
    "### 1.6 Split Data into Labeled and Unlabeled (Test) Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1643399514667,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "5PopjRAoAI43",
    "outputId": "6952487e-f09d-497b-94b0-4704f690e187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2576\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## TODO: YOU CAN SPLIT AFTERWARDS\n",
    "labeled_data = all_merged[is.na(all_merged$TEST_SET_ID),]\n",
    "test_data = all_merged[!is.na(all_merged$TEST_SET_ID),]\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = test_data, file = \"interim_data/labeled_set.csv\")\n",
    "write_csv(x = test_data, file = \"interim_data/test_set.csv\")\n",
    "\n",
    "labeled_data %>% nrow()\n",
    "test_data %>% nrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBsNJ7ktkzTe"
   },
   "source": [
    "## 2. Feature Elimination and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1643399514668,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "np2m7fQU_k7D"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df = data.frame(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbK8S6Wu304x"
   },
   "source": [
    "### 2.1 Delete Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1643399515101,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "vz6dt9cc361w",
    "outputId": "a123b6a0-e2e2-4b2c-848c-39e07791b89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"CUSTOMER\"           \"END_CUSTOMER\"       \"OFFER_PRICE\"       \n",
      " [4] \"SERVICE_LIST_PRICE\" \"MATERIAL_COST\"      \"SERVICE_COST\"      \n",
      " [7] \"PRICE_LIST\"         \"ISIC\"               \"SO_CREATED_DATE\"   \n",
      "[10] \"TECH\"               \"OFFER_TYPE\"         \"BUSINESS_TYPE\"     \n",
      "[13] \"COSTS_PRODUCT_A\"    \"COSTS_PRODUCT_B\"    \"COSTS_PRODUCT_C\"   \n",
      "[16] \"OFFER_STATUS\"       \"COSTS_PRODUCT_D\"    \"COSTS_PRODUCT_E\"   \n",
      "[19] \"SALES_LOCATION\"     \"TEST_SET_ID\"        \"COUNTRY_CODE\"      \n",
      "[22] \"SALES_OFFICE\"       \"IDX_CUSTOMER\"       \"REV_CURRENT_YEAR.1\"\n",
      "[25] \"REV_CURRENT_YEAR.2\" \"CREATION_YEAR\"      \"OWNERSHIP\"         \n",
      "[28] \"CURRENCY\"          \n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# Delete columns: MO_CREATED_DATE, SALES_BRANCH, REV_CURRENT_YEAR\n",
    "df = select (df,-c(MO_CREATED_DATE,SALES_BRANCH,REV_CURRENT_YEAR))\n",
    "\n",
    "names(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkBkY9ye9wlX"
   },
   "source": [
    "### 2.2 Process Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1643399515102,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "l3_PjxA9AIC3"
   },
   "outputs": [],
   "source": [
    "# Processed Features in 2.2:\n",
    "#  * OFFER_STATUS         [Modified]\n",
    "#  * END_CUSTOMER         [Deleted]\n",
    "#  * HAS_END_CUSTOMER     [Created]\n",
    "#  * ISIC                 [Deleted]\n",
    "#  * HAS_ISIC             [Created]\n",
    "#  * HAS_COSTS_PRODUCT_A  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_B  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_C  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_D  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_E  [Created]\n",
    "#  * COUNTRY_CODE         [Deleted]\n",
    "#  * IS_COUNTRY_CODE_CH   [Created]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515102,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "oFpbqYdb_ZwH"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"OFFER_STATUS\" Feature: Replace string values with binary values\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    OFFER_STATUS = case_when(\n",
    "      OFFER_STATUS %in% c(\"WIN\",\"Win\",\"Won\",\"WON\") ~ \"1\",\n",
    "      OFFER_STATUS %in% c(\"LOsT\",\"Lost\",\"LOST\",\"Lose\") ~ \"0\",\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515103,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "EYyzZsuy-CO0"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"END_CUSTOMER\" Feature: Convert to HAS_END_CUSTOMER\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    HAS_END_CUSTOMER = case_when(\n",
    "      END_CUSTOMER %in% c(NA,\"No\") ~ 0,\n",
    "      TRUE ~ 1 # Includes numbers and \"Yes\" values\n",
    "      \n",
    "    )\n",
    "  )\n",
    "\n",
    "# Delete column: END_CUSTOMER\n",
    "df = select (df,-c(END_CUSTOMER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515104,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "PyYHc4Cf-ZqC"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"ISIC\" Feature: Convert to HAS_ISIC\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    HAS_ISIC = case_when(\n",
    "      ISIC %in% c(NA) ~ 0,\n",
    "      TRUE ~ 1 # Includes numbers\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Delete column: ISIC\n",
    "df = select (df,-c(ISIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515105,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "P4G6ov4t-qsf"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"COUNTRY_CODE\" Feature: Convert to IS_COUNTRY_CODE_CH\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    IS_COUNTRY_CODE_CH = case_when(\n",
    "      COUNTRY_CODE %in% c(\"CH\") ~ 1,\n",
    "      COUNTRY_CODE %in% c(\"FR\") ~ 0,\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Delete column: COUNTRY_CODE\n",
    "df = select (df,-c(COUNTRY_CODE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515106,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "bo42X4me-jtd"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"COSTS_PRODUCT_*\" Features: Convert to HAS_PRODUCT_*\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    HAS_COSTS_PRODUCT_A = case_when(\n",
    "      COSTS_PRODUCT_A %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_B = case_when(\n",
    "      COSTS_PRODUCT_B %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_C = case_when(\n",
    "      COSTS_PRODUCT_C %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_D = case_when(\n",
    "      COSTS_PRODUCT_D %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_E = case_when(\n",
    "      COSTS_PRODUCT_E %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktMjLWJG96W7"
   },
   "source": [
    "### 2.3 Process Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515107,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "sMsW18q8Ba4H"
   },
   "outputs": [],
   "source": [
    "# Processed Features in 2.3:\n",
    "#  * TOTAL_COSTS_PRODUCT      [Created]\n",
    "#  * COSTS_PRODUCT_A          [Deleted]\n",
    "#  * COSTS_PRODUCT_B          [Deleted]\n",
    "#  * COSTS_PRODUCT_C          [Deleted]\n",
    "#  * COSTS_PRODUCT_D          [Deleted]\n",
    "#  * COSTS_PRODUCT_E          [Deleted]\n",
    "#  * CREATION_YEAR            [Modified]\n",
    "#  * SINCE_CREATION_YEAR      [Created]\n",
    "#  * REV_CURRENT_YEAR.1       [Modified]\n",
    "#  * REV_CURRENT_YEAR.2       [Modified]\n",
    "#  * REV_PERCENTAGE_INCREASE  [Created]\n",
    "#  * OWNERSHIP_NO_INFO_AS_NA  [Created]\n",
    "#  * OWNERSHIP_NA_AS_NO_INFO  [Created]\n",
    "#  * SO_CREATED_DATE_SCALED   [Created]\n",
    "#  * SO_CREATED_DATE          [Deleted]\n",
    "#  * SO_CREATED_DATE_INTEGER  [Deleted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399515107,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "0eJH5Emj-vgl"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Create \"TOTAL_COSTS_PRODUCT\" Feature: Sum of \"COSTS_PRODUCT_*\"\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    TOTAL_COSTS_PRODUCT = COSTS_PRODUCT_A\n",
    "    + COSTS_PRODUCT_B + \n",
    "      COSTS_PRODUCT_C + COSTS_PRODUCT_D + COSTS_PRODUCT_E\n",
    "  )\n",
    "\n",
    "# Delete columns: COSTS_PRODUCT_A to COSTS_PRODUCT_E\n",
    "df = select (df,-c(COSTS_PRODUCT_A,COSTS_PRODUCT_B,\n",
    "                                 COSTS_PRODUCT_C,COSTS_PRODUCT_D,\n",
    "                                 COSTS_PRODUCT_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399515108,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "VZsRAr2U_qsT"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"CREATION_YEAR\" Feature: Extract year\n",
    "df = df %>%\n",
    "  mutate(CREATION_YEAR = case_when(\n",
    "    is.character(CREATION_YEAR) ~ CREATION_YEAR %>%\n",
    "      substr(nchar(CREATION_YEAR) - 3, nchar(CREATION_YEAR)) %>%\n",
    "      as.numeric()\n",
    "  ))\n",
    "\n",
    "# Create \"SINCE_CREATION_YEAR\" Feature: 2021 - CREATION_YEAR\n",
    "df = df %>%\n",
    "  mutate(SINCE_CREATION_YEAR = case_when(!is.na(CREATION_YEAR) ~ as.double(2021 - CREATION_YEAR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515109,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "bpclY2cGCv_B"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"REV_CURRENT_YEAR.1\" and \"REV_CURRENT_YEAR.2\" Feature: Convert to EUR\n",
    "\n",
    "# Convert negative values to zero\n",
    "# df$REV_CURRENT_YEAR.1 = ifelse(df$REV_CURRENT_YEAR.1 < 0,0,df$REV_CURRENT_YEAR.1)\n",
    "# df$REV_CURRENT_YEAR.2 = ifelse(df$REV_CURRENT_YEAR.2 < 0,0,df$REV_CURRENT_YEAR.2)\n",
    "\n",
    "# Create \"REV_PERCENTAGE_INCREASE\" Feature: REV_CURRENT_YEAR.2 to REV_CURRENT_YEAR.1\n",
    "# calculate_percentage_increase = function(new, old) {\n",
    "#   100 * (new - old) / old\n",
    "# }\n",
    "# df = df %>%\n",
    "#   mutate(REV_PERCENTAGE_INCREASE = case_when(\n",
    "#     !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2) ~\n",
    "#       calculate_percentage_increase(REV_CURRENT_YEAR.1, REV_CURRENT_YEAR.2)\n",
    "#   ))\n",
    "\n",
    "\n",
    "# df = df %>% mutate( # If REV_CURRENT_YEAR.1 is 0 or null, fill it from REV_CURRENT_YEAR.2\n",
    "#   REV_CURRENT_YEAR.1 = ifelse(\n",
    "#     ((REV_CURRENT_YEAR.1 <= 0) |\n",
    "#        is.na(REV_CURRENT_YEAR.1)) &\n",
    "#       !is.na(REV_CURRENT_YEAR.2),\n",
    "#     REV_CURRENT_YEAR.2,\n",
    "#     REV_CURRENT_YEAR.1\n",
    "#   ) %>% as.double()\n",
    "# )\n",
    "# \n",
    "# df = df %>% mutate( # If REV_CURRENT_YEAR.2 is 0 or null, fill it from REV_CURRENT_YEAR.1\n",
    "#   REV_CURRENT_YEAR.2 = ifelse(\n",
    "#     ((REV_CURRENT_YEAR.2 <= 0) |\n",
    "#        is.na(REV_CURRENT_YEAR.2)) &\n",
    "#       !is.na(REV_CURRENT_YEAR.1),\n",
    "#     REV_CURRENT_YEAR.1,\n",
    "#     REV_CURRENT_YEAR.2\n",
    "#   ) %>% as.double()\n",
    "# )\n",
    "\n",
    "\n",
    "# Note: 2020 and 2021 annual average exchange rates are used.\n",
    "# Source: https://www.x-rates.com/average/?from=USD&to=EUR&amount=1&year=2021\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    REV_CURRENT_YEAR.1 = case_when(\n",
    "      CURRENCY ==  \"Pound Sterling\" ~ REV_CURRENT_YEAR.1 * 1.1438161149110808,\n",
    "      CURRENCY ==  \"Chinese Yuan\" ~ REV_CURRENT_YEAR.1 * 0.12906362243502054,\n",
    "      CURRENCY ==  \"US Dollar\" ~ REV_CURRENT_YEAR.1 * 0.8614249616963066,\n",
    "      CURRENCY ==  \"Euro\" ~ REV_CURRENT_YEAR.1\n",
    "    )\n",
    "  )\n",
    "\n",
    "\n",
    "#df = df %>%\n",
    "#  mutate(\n",
    "#    REV_CURRENT_YEAR.2 = case_when(\n",
    "#      CURRENCY ==  \"Pound Sterling\" ~ REV_CURRENT_YEAR.2 * 1.1438161149110808,\n",
    "#      CURRENCY ==  \"Chinese Yuan\" ~ REV_CURRENT_YEAR.2 * 0.12906362243502054,\n",
    "#      CURRENCY ==  \"US Dollar\" ~ REV_CURRENT_YEAR.2 * 0.8614249616963066,\n",
    "#      CURRENCY ==  \"Euro\" ~ REV_CURRENT_YEAR.2\n",
    "#    )\n",
    "#  )\n",
    "\n",
    "# Create \"REV_RATE\" Feature: REV_CURRENT_YEAR.2 to REV_CURRENT_YEAR.1\n",
    "df = df %>%\n",
    "  mutate(REV_RATE = case_when(\n",
    "    !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  REV_CURRENT_YEAR.1 !=0 & REV_CURRENT_YEAR.2 !=0 ~\n",
    "     REV_CURRENT_YEAR.1/REV_CURRENT_YEAR.2\n",
    "  ))\n",
    "\n",
    "# Create \"REV_AVG\" Feature: REV_CURRENT_YEAR.2 + REV_CURRENT_YEAR.1\n",
    "df = df %>%\n",
    "  mutate(REV_AVG = case_when(\n",
    "    !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  REV_CURRENT_YEAR.1 !=0 & REV_CURRENT_YEAR.2 !=0 ~\n",
    "     (REV_CURRENT_YEAR.1+REV_CURRENT_YEAR.2)/2,\n",
    "     !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  (REV_CURRENT_YEAR.1 ==0 | REV_CURRENT_YEAR.2 ==0) ~\n",
    "     (REV_CURRENT_YEAR.1+REV_CURRENT_YEAR.2),\n",
    "      !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  REV_CURRENT_YEAR.1 ==0 & REV_CURRENT_YEAR.2 ==0 ~\n",
    "      0.00\n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399515109,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "4XnCrUPEYO3n"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Create \"OWNERSHIP_NO_INFO_AS_NA\" Feature: Treat \"No information\" as NA value\n",
    "df$OWNERSHIP_NO_INFO_AS_NA = ifelse(df$OWNERSHIP ==  \"No information\", NA, df$OWNERSHIP)\n",
    "\n",
    "# Create \"OWNERSHIP_NA_AS_NO_INFO\" Feature: Treat NA values as \"No information\"\n",
    "df$OWNERSHIP_NA_AS_NO_INFO = ifelse(is.na(df$OWNERSHIP), \"No information\", df$OWNERSHIP)\n",
    "\n",
    "\n",
    "# Create \"SO_CREATED_DATE_SCALED\" Feature: Scale \"SO_CREATED_DATE\" x 100\n",
    "\n",
    "df$SO_CREATED_DATE_INTEGER = as_datetime(df$SO_CREATED_DATE, format = \"%d.%m.%Y %H:%M\") # Parse date-format 1\n",
    "date_format_2 = as_datetime(df$SO_CREATED_DATE, format = \"%Y-%m-%d %H:%M:%S\") # Parse date-format 1\n",
    "df$SO_CREATED_DATE_INTEGER[is.na(df$SO_CREATED_DATE_INTEGER)] = date_format_2[!is.na(date_format_2)]\n",
    "\n",
    "df$SO_CREATED_DATE_INTEGER = as.numeric(as.POSIXct(df$SO_CREATED_DATE_INTEGER))# Convert to Unix Time Stamp\n",
    "standart_scale = function (x) (x - mean(x, na.rm = T)) / sd(x, na.rm = T)\n",
    "\n",
    "df$SO_CREATED_DATE_SCALED = standart_scale(df$SO_CREATED_DATE_INTEGER) # Scale x 100\n",
    "\n",
    "# Delete column: \"SO_CREATED_DATE\"\n",
    "df = select (df,-c(SO_CREATED_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1643399516200,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "82eF6BV5ZYbE"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = df, file = \"interim_data/df_completed_1_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_HL0VLnb7o_"
   },
   "source": [
    "## 3. Dealing with Extreme Values, Filling Missing Values and Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFdincpk43N"
   },
   "source": [
    "### 3.1 Deal with Extreme Values in Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1643399516201,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "VyjZSA_ef6ab"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df$TOTAL_COSTS_PRODUCT = ifelse(df$TOTAL_COSTS_PRODUCT < 0,-df$TOTAL_COSTS_PRODUCT,df$TOTAL_COSTS_PRODUCT)\n",
    "df$TOTAL_COSTS_PRODUCT_LOG=log(df$TOTAL_COSTS_PRODUCT+1)\n",
    "\n",
    "df$SERVICE_COST = ifelse(df$SERVICE_COST < 0,-df$SERVICE_COST,df$SERVICE_COST)\n",
    "df$SERVICE_COST_LOG=log(df$SERVICE_COST+1)\n",
    "\n",
    "df$OFFER_PRICE_LOG=log(df$OFFER_PRICE)\n",
    "\n",
    "df$SERVICE_LIST_PRICE_LOG=log(df$SERVICE_LIST_PRICE+1)\n",
    "\n",
    "df$MATERIAL_COST_LOG=log(df$MATERIAL_COST+1)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.1=log(df$REV_CURRENT_YEAR.1+1)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.2=log(df$REV_CURRENT_YEAR.2+1)\n",
    "\n",
    "df$CREATION_YEAR_LOG=log(df$CREATION_YEAR)\n",
    "\n",
    "df$SINCE_CREATION_YEAR_LOG=log(df$SINCE_CREATION_YEAR+1)\n",
    "\n",
    "#For REV_PERCENTAGE_INCREASE\n",
    "#Q1 <- quantile(df$REV_PERCENTAGE_INCREASE, .25,na.rm=T)\n",
    "#Q3 <- quantile(df$REV_PERCENTAGE_INCREASE, .75,na.rm=T)\n",
    "#IQR <- IQR(df$REV_PERCENTAGE_INCREASE,na.rm=T)\n",
    "#df = df %>%  mutate(\n",
    "#  REV_PERCENTAGE_INCREASE_NO_OUTLIER = case_when(\n",
    "#    REV_PERCENTAGE_INCREASE < (Q1 - 3.0*IQR) ~ (Q1 - 3.0*IQR),\n",
    "#    REV_PERCENTAGE_INCREASE > (Q3 + 3.0*IQR) ~ (Q3 + 3.0*IQR),\n",
    "#    TRUE ~ REV_PERCENTAGE_INCREASE\n",
    "#  )\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSsIh60js_dH"
   },
   "source": [
    "### 3.2 Deal with Extreme Values in Character Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1643399516202,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "G8-H0hT9tTWs"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df$TECH_REDUCED_1 = ifelse(df$TECH %in% c(\"E\", \"EPS\", \"FP\", \"BP\"), \"E_EPS_FP_BP\", df$TECH)\n",
    "df$TECH_REDUCED_2_IS_F = ifelse(df$TECH == \"F\", 1, 0)\n",
    "\n",
    "df$OFFER_TYPE_REDUCED_1 = ifelse(\n",
    "  df$OFFER_TYPE %in% c(\n",
    "    \"FD\",\n",
    "    \"EH\",\n",
    "    \"FEI\",\n",
    "    \"MSYS\",\n",
    "    \"DCF\",\n",
    "    \"GAM\",\n",
    "    \"CP\",\n",
    "    \"CS\",\n",
    "    \"CI\",\n",
    "    \"EN\",\n",
    "    \"FIB\",\n",
    "    \"PAT\",\n",
    "    \"XCPS\"\n",
    "  ),\n",
    "  \"FD_EH_FEI_MSYS_DCF_GAM_CP_CS_CI_EN_FIB_PAT_XCPS\",\n",
    "  df$OFFER_TYPE\n",
    ")\n",
    "df$OFFER_TYPE_REDUCED_2 = ifelse(\n",
    "  df$OFFER_TYPE %in% c(\n",
    "    \"FED\",\n",
    "    \"CPP\",\n",
    "    \"ED\",\n",
    "    \"EV\",\n",
    "    \"FD\",\n",
    "    \"EH\",\n",
    "    \"FEI\",\n",
    "    \"MSYS\",\n",
    "    \"DCF\",\n",
    "    \"GAM\",\n",
    "    \"CP\",\n",
    "    \"CS\",\n",
    "    \"CI\",\n",
    "    \"EN\",\n",
    "    \"FIB\",\n",
    "    \"PAT\",\n",
    "    \"XCPS\"\n",
    "  ),\n",
    "  \"FED_CPP_ED_EV_FD_EH_FEI_MSYS_DCF_GAM_CP_CS_CI_EN_FIB_PAT_XCPS\",\n",
    "  df$OFFER_TYPE\n",
    ")\n",
    "\n",
    "df$OWNERSHIP_NA_AS_NO_INFO_REDUCED = ifelse(\n",
    "  df$OWNERSHIP_NA_AS_NO_INFO %in% c(\"Governmental\", \"Individual Person\", \"No information\"),\n",
    "  \"Governmental_IndividualPerson_Noinformation\",\n",
    "  df$OWNERSHIP_NA_AS_NO_INFO\n",
    ")\n",
    "\n",
    "df$OWNERSHIP_NO_INFO_AS_NA_REDUCED = ifelse(\n",
    "  df$OWNERSHIP_NO_INFO_AS_NA %in% c(\"Governmental\", \"Individual Person\"),\n",
    "  \"Governmental_IndividualPerson\",\n",
    "  df$OWNERSHIP_NO_INFO_AS_NA\n",
    ")\n",
    "\n",
    "df$OWNERSHIP_REDUCED = ifelse(\n",
    "  df$OWNERSHIP %in% c(\"Governmental\", \"Individual Person\", \"No information\"),\n",
    "  \"Governmental_IndividualPerson_Noinformation\",\n",
    "  df$OWNERSHIP\n",
    ")\n",
    "\n",
    "df$SALES_OFFICE_REDUCED = ifelse(\n",
    "  df$SALES_OFFICE %in% c(\n",
    "    \"Montpellier\",\n",
    "    \"Monaco\",\n",
    "    \"Limoges\",\n",
    "    \"Vertical Market\",\n",
    "    \"Others Functions\"\n",
    "  ),\n",
    "  \"Montpellier_Monaco_Limoges_Vertical Market_OthersFunctions\",\n",
    "  df$SALES_OFFICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4QqnZXbXx0R"
   },
   "source": [
    "### 3.3 Create IS_NA Columns and Remove the Duplicate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399516203,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "Z8QeYxnZX2aR"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# For numeric columns\n",
    "#df$IS_NA_REV_PERCENTAGE_INCREASE = ifelse(is.na(df$REV_PERCENTAGE_INCREASE) == T,1,0)\n",
    "df$IS_NA_REV_RATE = ifelse(is.na(df$REV_RATE) == T,1,0)\n",
    "df$IS_NA_REV_AVG = ifelse(is.na(df$REV_AVG) == T,1,0)\n",
    "df$IS_NA_REV_CURRENT_YEAR = ifelse(is.na(df$REV_CURRENT_YEAR.1) == T,1,0)\n",
    "\n",
    "\n",
    "# For Categoric Columns\n",
    "df$IS_NA_SALES_LOCATION = ifelse(is.na(df$SALES_LOCATION) == T,1,0)\n",
    "df$IS_NA_SALES_OFFICE = ifelse(is.na(df$SALES_OFFICE) == T,1,0)\n",
    "df$IS_NA_CURRENCY = ifelse(is.na(df$CURRENCY) == T,1,0)\n",
    "df$IS_NA_OWNERSHIP_NO_INFO_AS_NA = ifelse(is.na(df$OWNERSHIP_NO_INFO_AS_NA) == T,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643399516204,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "4NxPAaClAK5m"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = df, file = \"interim_data/df_completed_1_2_3_with_mv_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8YsFnPltjql"
   },
   "source": [
    "### 3.4 Fill Missing Values in Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7G8FcnoXgXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJpoIQv47k8h"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Replace with mean\n",
    "\n",
    "df$REV_CURRENT_YEAR.1[is.na(df$REV_CURRENT_YEAR.1)] = mean(df$REV_CURRENT_YEAR.1, na.rm=TRUE)\n",
    "\n",
    "df$REV_CURRENT_YEAR.2[is.na(df$REV_CURRENT_YEAR.2)] = mean(df$REV_CURRENT_YEAR.2, na.rm=TRUE)\n",
    "\n",
    "df$CREATION_YEAR[is.na(df$CREATION_YEAR)] = mean(df$CREATION_YEAR, na.rm=TRUE)\n",
    "\n",
    "df$SINCE_CREATION_YEAR[is.na(df$SINCE_CREATION_YEAR)] = mean(df$SINCE_CREATION_YEAR, na.rm=TRUE)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.1[is.na(df$REV_CURRENT_YEAR_LOG.1)] = mean(df$REV_CURRENT_YEAR_LOG.1, na.rm=TRUE)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.2[is.na(df$REV_CURRENT_YEAR_LOG.2)] = mean(df$REV_CURRENT_YEAR_LOG.2, na.rm=TRUE)\n",
    "\n",
    "df$CREATION_YEAR_LOG[is.na(df$CREATION_YEAR_LOG)] = mean(df$CREATION_YEAR_LOG, na.rm=TRUE)\n",
    "\n",
    "df$SINCE_CREATION_YEAR_LOG[is.na(df$SINCE_CREATION_YEAR_LOG)] = mean(df$SINCE_CREATION_YEAR_LOG, na.rm=TRUE)\n",
    "\n",
    "df$REV_PERCENTAGE_INCREASE_NO_OUTLIER[is.na(df$REV_PERCENTAGE_INCREASE_NO_OUTLIER)] = mean(df$REV_PERCENTAGE_INCREASE_NO_OUTLIER, na.rm=TRUE)\n",
    "\n",
    "df$REV_PERCENTAGE_INCREASE[is.na(df$REV_PERCENTAGE_INCREASE)] = mean(df$REV_PERCENTAGE_INCREASE, na.rm=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXb9iITP8_fl"
   },
   "source": [
    "### 3.5 Fill Missing Values in Character Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuLsQL2zKHnS"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df$SALES_LOCATION[is.na(df$SALES_LOCATION)] = \"Geneva West\"\n",
    "\n",
    "df$SALES_OFFICE[is.na(df$SALES_OFFICE)] = \"Geneva\"\n",
    "\n",
    "df$SALES_OFFICE_REDUCED[is.na(df$SALES_OFFICE_REDUCED)] = \"Geneva\"\n",
    "\n",
    "df$CURRENCY[is.na(df$CURRENCY)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP[is.na(df$OWNERSHIP)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP_NO_INFO_AS_NA[is.na(df$OWNERSHIP_NO_INFO_AS_NA)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP_NO_INFO_AS_NA_REDUCED[is.na(df$OWNERSHIP_NO_INFO_AS_NA_REDUCED)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP_REDUCED[is.na(df$OWNERSHIP_REDUCED)] = \"NOT_GIVEN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRBBpQSzMPpo"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = df, file = \"interim_data/df_completed_1_2_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QNZP03OgL-R"
   },
   "source": [
    "## 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jhwiJS2CXSpg"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages(\"party\")\n",
    "library(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32gAGeFFXM9j"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "x = df\n",
    "\n",
    "x = x[is.na(x$OFFER_STATUS) == F,]\n",
    "\n",
    "x = x %>% select(-c(TEST_SET_ID,IDX_CUSTOMER,CUSTOMER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtthyQNlYLlU"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "x$OFFER_STATUS = as.numeric(x$OFFER_STATUS)\n",
    "\n",
    "a <- select_if(x, is.numeric)  \n",
    "colnames(a)\n",
    "cf1 <- cforest(a$OFFER_STATUS ~ . , data= a, control=cforest_unbiased(mtry=2,ntree=50)) # fit the random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rElTbYKze-0F"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "varimp(cf1) # get variable importance, based on mean decrease in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot_dQRpboO6e"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"earth\")\n",
    "library(earth)\n",
    "\n",
    "marsModel <- earth(x$OFFER_STATUS ~ ., data=x) # build model\n",
    "ev <- evimp (marsModel) # estimate variable importance\n",
    "\n",
    "plot(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gi9UHXa_pZE6"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"relaimpo\")\n",
    "library(relaimpo)\n",
    "lmMod <- lm(OFFER_STATUS ~ . , data = x)  # fit lm() model\n",
    "relImportance <- calc.relimp(lmMod, type = \"lmg\", rela = TRUE)  # calculate relative importance scaled to 100\n",
    "sort(relImportance$lmg, decreasing=TRUE)  # relative importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1n7WgWfYVmr"
   },
   "outputs": [],
   "source": [
    "\n",
    "names(boruta_output)\n",
    "\n",
    "# Get significant variables including tentatives\n",
    "boruta_signif <-\n",
    "  getSelectedAttributes(boruta_output, withTentative = TRUE)\n",
    "print(boruta_signif)\n",
    "\n",
    "# Do a tentative rough fix\n",
    "roughFixMod <- TentativeRoughFix(boruta_output)\n",
    "boruta_signif <- getSelectedAttributes(roughFixMod)\n",
    "print(boruta_signif)\n",
    "\n",
    "# Variable Importance Scores\n",
    "imps <- attStats(roughFixMod)\n",
    "imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]\n",
    "head(imps2[order(-imps2$meanImp),])  # descending sort\n",
    "\n",
    "x = summary(boruta_output$ImpHistory)\n",
    "# Plot variable importance\n",
    "plot(\n",
    "  boruta_output,\n",
    "  cex.axis = .7,\n",
    "  las = 2,\n",
    "  xlab = \"\",\n",
    "  main = \"Variable Importance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0iqq88ByJkxt"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(df)) {\n",
    "  print(paste0(names(df)[i], \": \", sum(is.na(df[, i])), \"/\", nrow(df)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdfOLo3kabdu"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# fill in missing values\n",
    "customers = customers %>% mutate(REV_CURRENT_YEAR.1 = ifelse(REV_CURRENT_YEAR.1 == 0,REV_CURRENT_YEAR.2, REV_CURRENT_YEAR.1))\n",
    "customers = customers %>% mutate(REV_CURRENT_YEAR = ifelse(REV_CURRENT_YEAR == 0,REV_CURRENT_YEAR.2, REV_CURRENT_YEAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s489vmTWc1xm"
   },
   "source": [
    "## Li - Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1I7UglBc_jo"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "## check NA values in the labeled_data\n",
    "#print(\"[INFO] Number of NA values in each column:\")\n",
    "#for (i in 1:ncol(labeled_data)) { # for-loop over columns\n",
    "#  print(paste0(names(labeled_data)[i], \": \", sum(is.na(labeled_data[, i])), \"/\", nrow(labeled_data)))\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xvm8oF40t0Ig"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#remove REV_CURRENT_YEAR column as it's the same as REV_CURRENT_YEAR.1\n",
    "all_merged = subset(all_merged, select = -c(REV_CURRENT_YEAR))\n",
    "#remove MO_CREATED_DATE column\n",
    "all_merged = subset(all_merged, select = -c(MO_CREATED_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVWDm8ubJfu1"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#nice function from stackoverflow \n",
    "one_hot_encoding = function(df, columns=\"PRICE_LIST\"){\n",
    "  # create a copy of the original data.frame for not modifying the original\n",
    "  df = cbind(df)\n",
    "  # convert the columns to vector in case it is a string\n",
    "  columns = c(columns)\n",
    "  # for each variable perform the One hot encoding\n",
    "  for (column in columns){\n",
    "    unique_values = sort(unique(df[column])[,column])\n",
    "    non_reference_values  = unique_values[c(-1)] # the first element is going \n",
    "                                                 # to be the reference by default\n",
    "    for (value in non_reference_values){\n",
    "      # the new dummy column name\n",
    "      new_col_name = paste0(column,'_',value)\n",
    "      # create new dummy column for each value of the non_reference_values\n",
    "      df[new_col_name] <- with(df, ifelse(df[,column] == value, 1, 0))\n",
    "    }\n",
    "    # delete the one hot encoded column\n",
    "    df[column] = NULL\n",
    "\n",
    "  }\n",
    "  return(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zUYgjAEJlZk"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#one_hot_encoding column PRICE_LIST, TECH, BUSINESS_TYPE, OFFER_TYPE\n",
    "all_merged = one_hot_encoding(all_merged, c(\"PRICE_LIST\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"TECH\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"BUSINESS_TYPE\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"OFFER_TYPE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMgzSR5FJ4UN"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#Convert COUNTRY_CODE to binary\n",
    "all_merged = all_merged %>%\n",
    "  mutate(\n",
    "    COUNTRY_CODE = case_when(\n",
    "      COUNTRY_CODE %in% c(\"CH\") ~ 0,\n",
    "      COUNTRY_CODE %in% c(\"FR\") ~ 1,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLM6OH4HJ-q7"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "glimpse(all_merged)\n",
    "write_csv(x = all_merged, \"interim_data/all_merged_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTlddBKIvCS7"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"Hmisc\")\n",
    "#library(Hmisc)\n",
    "#complete REV_CURRENT_YEAR.1 and REV_CURRENT_YEAR.2 by mean using impute method\n",
    "#labeled_data$REV_CURRENT_YEAR.1 = impute(labeled_data$REV_CURRENT_YEAR.1, mean)\n",
    "#labeled_data$REV_CURRENT_YEAR.2 = impute(labeled_data$REV_CURRENT_YEAR.2, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FBsSGOLv2-G"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#print(\"[INFO] Number of NA values in each column:\")\n",
    "#for (i in 1:ncol(labeled_data)) { # for-loop over columns\n",
    "#  print(paste0(names(labeled_data)[i], \": \", sum(is.na(labeled_data[, i])), \"/\", nrow(labeled_data)))\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nN_4XsPJ4Fiy"
   },
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0F2DyTrc2AU"
   },
   "source": [
    "## Saqib - Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3Lp4TS6jZeB"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "all_merged = one_hot_encoding(all_merged, c(\"SALES_OFFICE\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"SALES_BRANCH\"))\n",
    "\n",
    "glimpse(all_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXeH5TXvc2Nf"
   },
   "source": [
    "## Teofil - Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGjCR3gxYNiy"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "old_all_merged = all_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZB0DVG4cJj0"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "customers = cst_df\n",
    "\n",
    "\n",
    "cst_df = cst_df %>% mutate(REV_CURRENT_YEAR.1 = ifelse(REV_CURRENT_YEAR.1 == 0,REV_CURRENT_YEAR.2, REV_CURRENT_YEAR.1))\n",
    "cst_df = cst_df %>% mutate(PREV_YEAR_PERCENTAGE_INCREASE.1 = ((REV_CURRENT_YEAR.1 - REV_CURRENT_YEAR.2)/REV_CURRENT_YEAR.2)*100)\n",
    "\n",
    "\n",
    "################# REPLACEMENT BLOC FOR FINAL MERGE ########################\n",
    "cst_df = cst_df %>% mutate(COUNTRY_CODE = case_when(\n",
    "  COUNTRY == 'Switzerland' ~ \"CH\",\n",
    "  COUNTRY == 'France' ~ \"FR\"\n",
    "))\n",
    "# Transform customer to integer\n",
    "trnsc_geo_df$CUSTOMER <- as.numeric(trnsc_geo_df$CUSTOMER)\n",
    "all_merged = left_join(trnsc_geo_df, cst_df, by = c(\"CUSTOMER\", \"COUNTRY_CODE\"))\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "########## SPLIT LABELED DATA INTO TRAIN AND VALIDATION BASED ON UNIQUE CUSTOMERS ##############\n",
    "unique_customers = unique(labeled_data$CUSTOMER)\n",
    "train_ids = sample(unique_customers, size= floor(0.8 * length(unique_customers)), replace=FALSE)\n",
    "\n",
    "train_set = labeled_data %>% filter(CUSTOMER %in% train_ids)\n",
    "validation_set = labeled_data %>%  filter(!CUSTOMER %in% train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vRtDs2li9zG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28792499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceking/Desktop/TUM Lecture Docs/3. Business Analytics and Machine Learning (IN2028)/Analytics Cup/Project Files/venv/lib/python3.8/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n",
      "/home/iceking/Desktop/TUM Lecture Docs/3. Business Analytics and Machine Learning (IN2028)/Analytics Cup/Project Files/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"!pip install missingno > /dev/null\\n!pip install category_encoders > /dev/null\\n\\n#Import Libraries\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport matplotlib\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport matplotlib.ticker as ticker\\nimport missingno as msno\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import base\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\\nfrom category_encoders import BinaryEncoder\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\\nfrom IPython.display import Image\\nimport warnings\\nimport pandas as pd\\nimport numpy as np\\nimport collections as c\\nimport sklearn\\nimport os\\n\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nimport category_encoders as ce\\nfrom sklearn import preprocessing\\n\\n# use feature importance for feature selection\\nfrom numpy import loadtxt\\nfrom numpy import sort\\nfrom xgboost import XGBClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import balanced_accuracy_score\\nfrom sklearn.feature_selection import SelectFromModel\\n\\n# example of auto-sklearn for a classification dataset\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom autosklearn.classification import AutoSklearnClassifier\\nimport autosklearn\\n\\nimport copy\\nimport time\\nimport pickle\\nimport itertools\\nfrom typing import List\\nimport datetime\\n\\n%load_ext nb_black\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.max_columns\\\", 1000)  # or None\\npd.set_option(\\\"display.max_rows\\\", 1000)  # or None\\npd.set_option(\\\"display.max_colwidth\\\", -1)  # or -1\";\n",
       "                var nbb_formatted_code = \"!pip install missingno > /dev/null\\n!pip install category_encoders > /dev/null\\n\\n# Import Libraries\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport matplotlib\\n\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport matplotlib.ticker as ticker\\nimport missingno as msno\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import base\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\\nfrom category_encoders import BinaryEncoder\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import (\\n    accuracy_score,\\n    roc_auc_score,\\n    classification_report,\\n    confusion_matrix,\\n)\\nfrom IPython.display import Image\\nimport warnings\\nimport pandas as pd\\nimport numpy as np\\nimport collections as c\\nimport sklearn\\nimport os\\n\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nimport category_encoders as ce\\nfrom sklearn import preprocessing\\n\\n# use feature importance for feature selection\\nfrom numpy import loadtxt\\nfrom numpy import sort\\nfrom xgboost import XGBClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import balanced_accuracy_score\\nfrom sklearn.feature_selection import SelectFromModel\\n\\n# example of auto-sklearn for a classification dataset\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom autosklearn.classification import AutoSklearnClassifier\\nimport autosklearn\\n\\nimport copy\\nimport time\\nimport pickle\\nimport itertools\\nfrom typing import List\\nimport datetime\\n\\n%load_ext nb_black\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.max_columns\\\", 1000)  # or None\\npd.set_option(\\\"display.max_rows\\\", 1000)  # or None\\npd.set_option(\\\"display.max_colwidth\\\", -1)  # or -1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install missingno > /dev/null\n",
    "!pip install category_encoders > /dev/null\n",
    "\n",
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import base\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections as c\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# use feature importance for feature selection\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# example of auto-sklearn for a classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "import autosklearn\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "from typing import List\n",
    "import datetime\n",
    "\n",
    "%load_ext nb_black\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 1000)  # or None\n",
    "pd.set_option(\"display.max_rows\", 1000)  # or None\n",
    "pd.set_option(\"display.max_colwidth\", -1)  # or -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05027a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"\\n## Helper Functions\\n\\ndef type_separator(df: pd.DataFrame, print_results=False):\\n    dtype_names = [\\\"categorical\\\", \\\"binary\\\", \\\"continuous\\\", \\\"integer\\\", \\\"numerical\\\"]\\n    type_to_cols = dict.fromkeys(dtype_names, [])\\n\\n    dtype_char_to_names = {\\n        \\\"O\\\": \\\"categorical\\\",\\n        \\\"i\\\": \\\"integer\\\",\\n        \\\"u\\\": \\\"integer\\\",\\n        \\\"f\\\": \\\"continuous\\\",\\n    }\\n\\n    for col in df.columns:\\n        unique_vals = set(df[col].dropna().unique())\\n\\n        if len(unique_vals) < 2 or (\\n            df[col].dtype == np.dtype(\\\"object\\\") and len(unique_vals) > 500\\n        ):\\n            raise ValueError(\\n                f\\\"[ERROR] Something wrong with column:{col} cannot be this case, check conversions!\\\"\\n            )\\n\\n        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\\n            type_to_cols[\\\"binary\\\"] = type_to_cols[\\\"binary\\\"] + [col]\\n        elif df[col].dtype.str[1] in dtype_char_to_names:\\n            dtype_char = df[col].dtype.str[1]\\n            dtype_name = dtype_char_to_names[dtype_char]\\n            type_to_cols[dtype_name] = type_to_cols[dtype_name] + [col]\\n        else:\\n            raise ValueError(\\n                f'[ERROR] Numpy data type:\\\"{df[col].dtype}\\\" of col:\\\"{col}\\\" not understood.'\\n            )\\n\\n    type_to_cols[\\\"numerical\\\"] = type_to_cols[\\\"continuous\\\"] + type_to_cols[\\\"integer\\\"]\\n    type_to_cols[\\\"nominal\\\"] = type_to_cols[\\\"categorical\\\"] + type_to_cols[\\\"binary\\\"]\\n    for dtype_name, col_names in type_to_cols.items():\\n        assert len(type_to_cols[dtype_name]) == len(\\n            set(type_to_cols[dtype_name])\\n        ), f'For type:\\\"{dtype_name}\\\", some columns are duplicate in: {col_names}.'\\n\\n    if print_results:\\n        for key, val in type_to_cols.items():\\n            print(\\\"type:\\\", key, \\\"columns:\\\")\\n            for col in sorted(val):\\n                print(\\\">\\\", col)\\n            print(\\\"-\\\" * 32)\\n    return type_to_cols\\n\\n\\ndef get_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col].isna())]\\n\\n\\ndef get_non_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if not np.any(df[col].isna())]\\n\\n\\ndef get_inf_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col] == np.inf)]\\n\\n\\ndef print_nan_and_inf_columns(df: pd.DataFrame):\\n    print(\\\"NaN and Infinity Columns and Counts:\\\")\\n    for col in sorted(get_null_columns(df)):\\n        print(\\\"> [NaN     ]\\\", col, \\\"Null Count:\\\", np.sum(df[col].isna()))\\n    for col in sorted(get_inf_columns(df)):\\n        print(\\\"> [INFINITY]\\\", col, \\\"Inf Count:\\\", np.sum(df[col] == np.inf))\\n\\n\\ndef get_labeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[~np.isnan(df[target_col])]\\n\\n\\ndef get_unlabeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[np.isnan(df[target_col])]\\n\\n\\ndef MissingUniqueStatistics(df, show_unique_values=False):\\n\\n    total_entry_list = []\\n    total_missing_value_list = []\\n    missing_value_ratio_list = []\\n    data_type_list = []\\n    unique_values_list = []\\n    number_of_unique_values_list = []\\n    variable_name_list = []\\n\\n    for col in df.columns:\\n\\n        variable_name_list.append(col)\\n        missing_value_ratio = round((df[col].isna().sum() / len(df[col])), 4)\\n        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\\n        total_missing_value_list.append(df[col].isna().sum())\\n        missing_value_ratio_list.append(missing_value_ratio)\\n        data_type_list.append(df[col].dtype)\\n        unique_values_list.append(list(df[col].unique()))\\n        number_of_unique_values_list.append(len(df[col].unique()))\\n\\n    data_info_df = pd.DataFrame(\\n        {\\n            \\\"Variable\\\": variable_name_list,\\n            \\\"#_Total_Entry\\\": total_entry_list,\\n            \\\"#_Missing_Value\\\": total_missing_value_list,\\n            \\\"%_Missing_Value\\\": missing_value_ratio_list,\\n            \\\"Data_Type\\\": data_type_list,\\n            \\\"Unique_Values\\\": unique_values_list,\\n            \\\"#_Uniques_Values\\\": number_of_unique_values_list,\\n        }\\n    )\\n    if not show_unique_values:\\n        data_info_df = data_info_df.drop(\\\"Unique_Values\\\", axis=1)\\n\\n    return data_info_df.sort_values(by=\\\"#_Missing_Value\\\", ascending=False).set_index(\\n        \\\"Variable\\\"\\n    )\\n\\n\\ndef histogram(df, feature):  # Histogram of the target categories\\n    %matplotlib inline\\n    ncount = len(df)\\n    ax = sns.countplot(x=feature, data=df, palette=\\\"hls\\\")\\n    sns.set(font_scale=1)\\n    ax.set_xlabel(\\\"Target Segments\\\")\\n    plt.xticks(rotation=90)\\n    ax.set_ylabel(\\\"Number of Observations\\\")\\n    fig = plt.gcf()\\n    fig.set_size_inches(12, 5)\\n    # Make twin axis\\n    ax2 = ax.twinx()\\n    # Switch so count axis is on right, frequency on left\\n    ax2.yaxis.tick_left()\\n    ax.yaxis.tick_right()\\n    # Also switch the labels over\\n    ax.yaxis.set_label_position(\\\"right\\\")\\n    ax2.yaxis.set_label_position(\\\"left\\\")\\n    ax2.set_ylabel(\\\"Frequency [%]\\\")\\n    for p in ax.patches:\\n        x = p.get_bbox().get_points()[:, 0]\\n        y = p.get_bbox().get_points()[1, 1]\\n        ax.annotate(\\n            \\\"{:.2f}%\\\".format(100.0 * y / ncount),\\n            (x.mean(), y),\\n            ha=\\\"center\\\",\\n            va=\\\"bottom\\\",\\n        )  # set the alignment of the text\\n    # Use a LinearLocator to ensure the correct number of ticks\\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\\n    # Fix the frequency range to 0-100\\n    ax2.set_ylim(0, 100)\\n    ax.set_ylim(0, ncount)\\n    # And use a MultipleLocator to ensure a tick spacing of 10\\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\\n    ax2.grid(None)\\n    plt.title(\\\"Histogram of Binary Target Categories\\\", fontsize=20, y=1.08)\\n    plt.show()\\n    plt.savefig(\\\"target_histogram.png\\\")\\n    del ncount, x, y\\n\\n    # USAGE: histogram(data, \\\"CLASS\\\")\\n\\n## From Submission 3, With H2O\\n\\ndef apply_h2o(data, max_runtime_secs=10 * 60, max_models=20, balance_classes=True,\\n              models_dir_path = './h2o_models_with_data',training_with_all=False):\\n    timestamp = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H:%M:%S\\\")\\n    X_train, X_test, y_train, y_test = data\\n    \\n    from h2o.automl import H2OAutoML\\n    import h2o\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n    \\n    if not os.path.exists(models_dir_path):\\n        os.makedirs(models_dir_path)\\n        \\n    results_dir_path =f'{models_dir_path}/results_ts_{timestamp}'\\n    os.makedirs(results_dir_path)\\n        \\n    train_path =f\\\"{results_dir_path}/train.csv\\\"\\n    test_path =f\\\"{results_dir_path}/test.csv\\\"\\n\\n    pd.concat([X_train, y_train], axis=1).to_csv(train_path, index=False, header=True)\\n    pd.concat([X_test, y_test], axis=1).to_csv(test_path, index=False, header=True)\\n\\n    train = h2o.import_file(train_path)\\n    test = h2o.import_file(test_path)\\n\\n    h2o.init(max_mem_size=\\\"36G\\\")\\n\\n    x = train.columns\\n    train[\\\"OFFER_STATUS\\\"] = train[\\\"OFFER_STATUS\\\"].asfactor()\\n    x.remove(\\\"OFFER_STATUS\\\")\\n    aml = H2OAutoML(\\n        max_models=max_models,\\n        balance_classes=balance_classes,\\n        max_runtime_secs=int(max_runtime_secs),\\n        seed=42,\\n    )\\n    if  training_with_all:\\n        saved_model_path=f'{results_dir_path}/model_withALL'\\n        print(f'[INFO] Model will be saved here:\\\"{saved_model_path}\\\"')\\n    \\n    #print(train)\\n    #import sys;sys.exit(\\\"dsds\\\")\\n    \\n    aml.train(x=x, y=\\\"OFFER_STATUS\\\", training_frame=train)\\n    \\n    print(\\\"[INFO] Timestamp:\\\",timestamp)\\n    print('[INFO] AML Leaderboard',aml.leaderboard)\\n    \\n    model_bac_scores = []\\n    \\n    if  training_with_all:\\n        saved_model_path=f'{results_dir_path}/model_withALL'\\n        h2o.save_model(model=aml.leader, path=saved_model_path, force=True)\\n    else:\\n        for i in range(int(aml.max_models * 1.5)):\\n            current_model = aml.leaderboard[i, 0]\\n            if current_model == \\\"NA\\\":\\n                print(f\\\"[INFO] Found {i} models in total.\\\")\\n                break\\n            current_model = h2o.get_model(current_model)\\n            new_pred = current_model.predict(test)\\n            new_pred = new_pred[0].as_data_frame().values.flatten()\\n            model_bac_score = balanced_accuracy_score(Y_test, new_pred)\\n            print(f'[INFO] Model #{i}, BAC={model_bac_score}.')\\n            model_bac_scores.append(model_bac_score)\\n\\n        index_max_bac_score = model_bac_scores.index(max(model_bac_scores))\\n    \\n        saved_model_path=f'{results_dir_path}/model_{\\\"bac_%.3f\\\" % model_bac_scores[index_max_bac_score]}'\\n        h2o.save_model(model=h2o.get_model(aml.leaderboard[index_max_bac_score, 0]), \\n                   path=saved_model_path, force=True)\\n    \\n    print(\\\"[INFO] RESULTS:\\\")\\n    print(f' > Train data saved to:\\\"{train_path}\\\".')\\n    print(f' > Test data saved to :\\\"{test_path}\\\".')\\n    print(f' > H20 Model saved to :\\\"{saved_model_path}\\\".')\\n\\n    return aml, model_bac_scores\";\n",
       "                var nbb_formatted_code = \"## Helper Functions\\n\\n\\ndef type_separator(df: pd.DataFrame, print_results=False):\\n    dtype_names = [\\\"categorical\\\", \\\"binary\\\", \\\"continuous\\\", \\\"integer\\\", \\\"numerical\\\"]\\n    type_to_cols = dict.fromkeys(dtype_names, [])\\n\\n    dtype_char_to_names = {\\n        \\\"O\\\": \\\"categorical\\\",\\n        \\\"i\\\": \\\"integer\\\",\\n        \\\"u\\\": \\\"integer\\\",\\n        \\\"f\\\": \\\"continuous\\\",\\n    }\\n\\n    for col in df.columns:\\n        unique_vals = set(df[col].dropna().unique())\\n\\n        if len(unique_vals) < 2 or (\\n            df[col].dtype == np.dtype(\\\"object\\\") and len(unique_vals) > 500\\n        ):\\n            raise ValueError(\\n                f\\\"[ERROR] Something wrong with column:{col} cannot be this case, check conversions!\\\"\\n            )\\n\\n        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\\n            type_to_cols[\\\"binary\\\"] = type_to_cols[\\\"binary\\\"] + [col]\\n        elif df[col].dtype.str[1] in dtype_char_to_names:\\n            dtype_char = df[col].dtype.str[1]\\n            dtype_name = dtype_char_to_names[dtype_char]\\n            type_to_cols[dtype_name] = type_to_cols[dtype_name] + [col]\\n        else:\\n            raise ValueError(\\n                f'[ERROR] Numpy data type:\\\"{df[col].dtype}\\\" of col:\\\"{col}\\\" not understood.'\\n            )\\n\\n    type_to_cols[\\\"numerical\\\"] = type_to_cols[\\\"continuous\\\"] + type_to_cols[\\\"integer\\\"]\\n    type_to_cols[\\\"nominal\\\"] = type_to_cols[\\\"categorical\\\"] + type_to_cols[\\\"binary\\\"]\\n    for dtype_name, col_names in type_to_cols.items():\\n        assert len(type_to_cols[dtype_name]) == len(\\n            set(type_to_cols[dtype_name])\\n        ), f'For type:\\\"{dtype_name}\\\", some columns are duplicate in: {col_names}.'\\n\\n    if print_results:\\n        for key, val in type_to_cols.items():\\n            print(\\\"type:\\\", key, \\\"columns:\\\")\\n            for col in sorted(val):\\n                print(\\\">\\\", col)\\n            print(\\\"-\\\" * 32)\\n    return type_to_cols\\n\\n\\ndef get_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col].isna())]\\n\\n\\ndef get_non_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if not np.any(df[col].isna())]\\n\\n\\ndef get_inf_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col] == np.inf)]\\n\\n\\ndef print_nan_and_inf_columns(df: pd.DataFrame):\\n    print(\\\"NaN and Infinity Columns and Counts:\\\")\\n    for col in sorted(get_null_columns(df)):\\n        print(\\\"> [NaN     ]\\\", col, \\\"Null Count:\\\", np.sum(df[col].isna()))\\n    for col in sorted(get_inf_columns(df)):\\n        print(\\\"> [INFINITY]\\\", col, \\\"Inf Count:\\\", np.sum(df[col] == np.inf))\\n\\n\\ndef get_labeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[~np.isnan(df[target_col])]\\n\\n\\ndef get_unlabeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[np.isnan(df[target_col])]\\n\\n\\ndef MissingUniqueStatistics(df, show_unique_values=False):\\n\\n    total_entry_list = []\\n    total_missing_value_list = []\\n    missing_value_ratio_list = []\\n    data_type_list = []\\n    unique_values_list = []\\n    number_of_unique_values_list = []\\n    variable_name_list = []\\n\\n    for col in df.columns:\\n\\n        variable_name_list.append(col)\\n        missing_value_ratio = round((df[col].isna().sum() / len(df[col])), 4)\\n        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\\n        total_missing_value_list.append(df[col].isna().sum())\\n        missing_value_ratio_list.append(missing_value_ratio)\\n        data_type_list.append(df[col].dtype)\\n        unique_values_list.append(list(df[col].unique()))\\n        number_of_unique_values_list.append(len(df[col].unique()))\\n\\n    data_info_df = pd.DataFrame(\\n        {\\n            \\\"Variable\\\": variable_name_list,\\n            \\\"#_Total_Entry\\\": total_entry_list,\\n            \\\"#_Missing_Value\\\": total_missing_value_list,\\n            \\\"%_Missing_Value\\\": missing_value_ratio_list,\\n            \\\"Data_Type\\\": data_type_list,\\n            \\\"Unique_Values\\\": unique_values_list,\\n            \\\"#_Uniques_Values\\\": number_of_unique_values_list,\\n        }\\n    )\\n    if not show_unique_values:\\n        data_info_df = data_info_df.drop(\\\"Unique_Values\\\", axis=1)\\n\\n    return data_info_df.sort_values(by=\\\"#_Missing_Value\\\", ascending=False).set_index(\\n        \\\"Variable\\\"\\n    )\\n\\n\\ndef histogram(df, feature):  # Histogram of the target categories\\n    %matplotlib inline\\n    ncount = len(df)\\n    ax = sns.countplot(x=feature, data=df, palette=\\\"hls\\\")\\n    sns.set(font_scale=1)\\n    ax.set_xlabel(\\\"Target Segments\\\")\\n    plt.xticks(rotation=90)\\n    ax.set_ylabel(\\\"Number of Observations\\\")\\n    fig = plt.gcf()\\n    fig.set_size_inches(12, 5)\\n    # Make twin axis\\n    ax2 = ax.twinx()\\n    # Switch so count axis is on right, frequency on left\\n    ax2.yaxis.tick_left()\\n    ax.yaxis.tick_right()\\n    # Also switch the labels over\\n    ax.yaxis.set_label_position(\\\"right\\\")\\n    ax2.yaxis.set_label_position(\\\"left\\\")\\n    ax2.set_ylabel(\\\"Frequency [%]\\\")\\n    for p in ax.patches:\\n        x = p.get_bbox().get_points()[:, 0]\\n        y = p.get_bbox().get_points()[1, 1]\\n        ax.annotate(\\n            \\\"{:.2f}%\\\".format(100.0 * y / ncount),\\n            (x.mean(), y),\\n            ha=\\\"center\\\",\\n            va=\\\"bottom\\\",\\n        )  # set the alignment of the text\\n    # Use a LinearLocator to ensure the correct number of ticks\\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\\n    # Fix the frequency range to 0-100\\n    ax2.set_ylim(0, 100)\\n    ax.set_ylim(0, ncount)\\n    # And use a MultipleLocator to ensure a tick spacing of 10\\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\\n    ax2.grid(None)\\n    plt.title(\\\"Histogram of Binary Target Categories\\\", fontsize=20, y=1.08)\\n    plt.show()\\n    plt.savefig(\\\"target_histogram.png\\\")\\n    del ncount, x, y\\n\\n    # USAGE: histogram(data, \\\"CLASS\\\")\\n\\n\\n## From Submission 3, With H2O\\n\\n\\ndef apply_h2o(\\n    data,\\n    max_runtime_secs=10 * 60,\\n    max_models=20,\\n    balance_classes=True,\\n    models_dir_path=\\\"./h2o_models_with_data\\\",\\n    training_with_all=False,\\n):\\n    timestamp = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H:%M:%S\\\")\\n    X_train, X_test, y_train, y_test = data\\n\\n    from h2o.automl import H2OAutoML\\n    import h2o\\n\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n\\n    if not os.path.exists(models_dir_path):\\n        os.makedirs(models_dir_path)\\n\\n    results_dir_path = f\\\"{models_dir_path}/results_ts_{timestamp}\\\"\\n    os.makedirs(results_dir_path)\\n\\n    train_path = f\\\"{results_dir_path}/train.csv\\\"\\n    test_path = f\\\"{results_dir_path}/test.csv\\\"\\n\\n    pd.concat([X_train, y_train], axis=1).to_csv(train_path, index=False, header=True)\\n    pd.concat([X_test, y_test], axis=1).to_csv(test_path, index=False, header=True)\\n\\n    train = h2o.import_file(train_path)\\n    test = h2o.import_file(test_path)\\n\\n    h2o.init(max_mem_size=\\\"36G\\\")\\n\\n    x = train.columns\\n    train[\\\"OFFER_STATUS\\\"] = train[\\\"OFFER_STATUS\\\"].asfactor()\\n    x.remove(\\\"OFFER_STATUS\\\")\\n    aml = H2OAutoML(\\n        max_models=max_models,\\n        balance_classes=balance_classes,\\n        max_runtime_secs=int(max_runtime_secs),\\n        seed=42,\\n    )\\n    if training_with_all:\\n        saved_model_path = f\\\"{results_dir_path}/model_withALL\\\"\\n        print(f'[INFO] Model will be saved here:\\\"{saved_model_path}\\\"')\\n\\n    # print(train)\\n    # import sys;sys.exit(\\\"dsds\\\")\\n\\n    aml.train(x=x, y=\\\"OFFER_STATUS\\\", training_frame=train)\\n\\n    print(\\\"[INFO] Timestamp:\\\", timestamp)\\n    print(\\\"[INFO] AML Leaderboard\\\", aml.leaderboard)\\n\\n    model_bac_scores = []\\n\\n    if training_with_all:\\n        saved_model_path = f\\\"{results_dir_path}/model_withALL\\\"\\n        h2o.save_model(model=aml.leader, path=saved_model_path, force=True)\\n    else:\\n        for i in range(int(aml.max_models * 1.5)):\\n            current_model = aml.leaderboard[i, 0]\\n            if current_model == \\\"NA\\\":\\n                print(f\\\"[INFO] Found {i} models in total.\\\")\\n                break\\n            current_model = h2o.get_model(current_model)\\n            new_pred = current_model.predict(test)\\n            new_pred = new_pred[0].as_data_frame().values.flatten()\\n            model_bac_score = balanced_accuracy_score(Y_test, new_pred)\\n            print(f\\\"[INFO] Model #{i}, BAC={model_bac_score}.\\\")\\n            model_bac_scores.append(model_bac_score)\\n\\n        index_max_bac_score = model_bac_scores.index(max(model_bac_scores))\\n\\n        saved_model_path = f'{results_dir_path}/model_{\\\"bac_%.3f\\\" % model_bac_scores[index_max_bac_score]}'\\n        h2o.save_model(\\n            model=h2o.get_model(aml.leaderboard[index_max_bac_score, 0]),\\n            path=saved_model_path,\\n            force=True,\\n        )\\n\\n    print(\\\"[INFO] RESULTS:\\\")\\n    print(f' > Train data saved to:\\\"{train_path}\\\".')\\n    print(f' > Test data saved to :\\\"{test_path}\\\".')\\n    print(f' > H20 Model saved to :\\\"{saved_model_path}\\\".')\\n\\n    return aml, model_bac_scores\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## Helper Functions\n",
    "\n",
    "def type_separator(df: pd.DataFrame, print_results=False):\n",
    "    dtype_names = [\"categorical\", \"binary\", \"continuous\", \"integer\", \"numerical\"]\n",
    "    type_to_cols = dict.fromkeys(dtype_names, [])\n",
    "\n",
    "    dtype_char_to_names = {\n",
    "        \"O\": \"categorical\",\n",
    "        \"i\": \"integer\",\n",
    "        \"u\": \"integer\",\n",
    "        \"f\": \"continuous\",\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        unique_vals = set(df[col].dropna().unique())\n",
    "\n",
    "        if len(unique_vals) < 2 or (\n",
    "            df[col].dtype == np.dtype(\"object\") and len(unique_vals) > 500\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"[ERROR] Something wrong with column:{col} cannot be this case, check conversions!\"\n",
    "            )\n",
    "\n",
    "        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\n",
    "            type_to_cols[\"binary\"] = type_to_cols[\"binary\"] + [col]\n",
    "        elif df[col].dtype.str[1] in dtype_char_to_names:\n",
    "            dtype_char = df[col].dtype.str[1]\n",
    "            dtype_name = dtype_char_to_names[dtype_char]\n",
    "            type_to_cols[dtype_name] = type_to_cols[dtype_name] + [col]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'[ERROR] Numpy data type:\"{df[col].dtype}\" of col:\"{col}\" not understood.'\n",
    "            )\n",
    "\n",
    "    type_to_cols[\"numerical\"] = type_to_cols[\"continuous\"] + type_to_cols[\"integer\"]\n",
    "    type_to_cols[\"nominal\"] = type_to_cols[\"categorical\"] + type_to_cols[\"binary\"]\n",
    "    for dtype_name, col_names in type_to_cols.items():\n",
    "        assert len(type_to_cols[dtype_name]) == len(\n",
    "            set(type_to_cols[dtype_name])\n",
    "        ), f'For type:\"{dtype_name}\", some columns are duplicate in: {col_names}.'\n",
    "\n",
    "    if print_results:\n",
    "        for key, val in type_to_cols.items():\n",
    "            print(\"type:\", key, \"columns:\")\n",
    "            for col in sorted(val):\n",
    "                print(\">\", col)\n",
    "            print(\"-\" * 32)\n",
    "    return type_to_cols\n",
    "\n",
    "\n",
    "def get_null_columns(df: pd.DataFrame) -> List:\n",
    "    return [col for col in df.columns if np.any(df[col].isna())]\n",
    "\n",
    "\n",
    "def get_non_null_columns(df: pd.DataFrame) -> List:\n",
    "    return [col for col in df.columns if not np.any(df[col].isna())]\n",
    "\n",
    "\n",
    "def get_inf_columns(df: pd.DataFrame) -> List:\n",
    "    return [col for col in df.columns if np.any(df[col] == np.inf)]\n",
    "\n",
    "\n",
    "def print_nan_and_inf_columns(df: pd.DataFrame):\n",
    "    print(\"NaN and Infinity Columns and Counts:\")\n",
    "    for col in sorted(get_null_columns(df)):\n",
    "        print(\"> [NaN     ]\", col, \"Null Count:\", np.sum(df[col].isna()))\n",
    "    for col in sorted(get_inf_columns(df)):\n",
    "        print(\"> [INFINITY]\", col, \"Inf Count:\", np.sum(df[col] == np.inf))\n",
    "\n",
    "\n",
    "def get_labeled_set(df: pd.DataFrame, target_col: str = \"OFFER_STATUS\"):\n",
    "    return df[~np.isnan(df[target_col])]\n",
    "\n",
    "\n",
    "def get_unlabeled_set(df: pd.DataFrame, target_col: str = \"OFFER_STATUS\"):\n",
    "    return df[np.isnan(df[target_col])]\n",
    "\n",
    "\n",
    "def MissingUniqueStatistics(df, show_unique_values=False):\n",
    "\n",
    "    total_entry_list = []\n",
    "    total_missing_value_list = []\n",
    "    missing_value_ratio_list = []\n",
    "    data_type_list = []\n",
    "    unique_values_list = []\n",
    "    number_of_unique_values_list = []\n",
    "    variable_name_list = []\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        variable_name_list.append(col)\n",
    "        missing_value_ratio = round((df[col].isna().sum() / len(df[col])), 4)\n",
    "        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\n",
    "        total_missing_value_list.append(df[col].isna().sum())\n",
    "        missing_value_ratio_list.append(missing_value_ratio)\n",
    "        data_type_list.append(df[col].dtype)\n",
    "        unique_values_list.append(list(df[col].unique()))\n",
    "        number_of_unique_values_list.append(len(df[col].unique()))\n",
    "\n",
    "    data_info_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Variable\": variable_name_list,\n",
    "            \"#_Total_Entry\": total_entry_list,\n",
    "            \"#_Missing_Value\": total_missing_value_list,\n",
    "            \"%_Missing_Value\": missing_value_ratio_list,\n",
    "            \"Data_Type\": data_type_list,\n",
    "            \"Unique_Values\": unique_values_list,\n",
    "            \"#_Uniques_Values\": number_of_unique_values_list,\n",
    "        }\n",
    "    )\n",
    "    if not show_unique_values:\n",
    "        data_info_df = data_info_df.drop(\"Unique_Values\", axis=1)\n",
    "\n",
    "    return data_info_df.sort_values(by=\"#_Missing_Value\", ascending=False).set_index(\n",
    "        \"Variable\"\n",
    "    )\n",
    "\n",
    "\n",
    "def histogram(df, feature):  # Histogram of the target categories\n",
    "    %matplotlib inline\n",
    "    ncount = len(df)\n",
    "    ax = sns.countplot(x=feature, data=df, palette=\"hls\")\n",
    "    sns.set(font_scale=1)\n",
    "    ax.set_xlabel(\"Target Segments\")\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel(\"Number of Observations\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 5)\n",
    "    # Make twin axis\n",
    "    ax2 = ax.twinx()\n",
    "    # Switch so count axis is on right, frequency on left\n",
    "    ax2.yaxis.tick_left()\n",
    "    ax.yaxis.tick_right()\n",
    "    # Also switch the labels over\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax2.yaxis.set_label_position(\"left\")\n",
    "    ax2.set_ylabel(\"Frequency [%]\")\n",
    "    for p in ax.patches:\n",
    "        x = p.get_bbox().get_points()[:, 0]\n",
    "        y = p.get_bbox().get_points()[1, 1]\n",
    "        ax.annotate(\n",
    "            \"{:.2f}%\".format(100.0 * y / ncount),\n",
    "            (x.mean(), y),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )  # set the alignment of the text\n",
    "    # Use a LinearLocator to ensure the correct number of ticks\n",
    "    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\n",
    "    # Fix the frequency range to 0-100\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax.set_ylim(0, ncount)\n",
    "    # And use a MultipleLocator to ensure a tick spacing of 10\n",
    "    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n",
    "    ax2.grid(None)\n",
    "    plt.title(\"Histogram of Binary Target Categories\", fontsize=20, y=1.08)\n",
    "    plt.show()\n",
    "    plt.savefig(\"target_histogram.png\")\n",
    "    del ncount, x, y\n",
    "\n",
    "    # USAGE: histogram(data, \"CLASS\")\n",
    "\n",
    "## From Submission 3, With H2O\n",
    "\n",
    "def apply_h2o(data, max_runtime_secs=10 * 60, max_models=20, balance_classes=True,\n",
    "              models_dir_path = './h2o_models_with_data',training_with_all=False):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    from h2o.automl import H2OAutoML\n",
    "    import h2o\n",
    "    h2o.init(max_mem_size=\"32G\")\n",
    "    \n",
    "    if not os.path.exists(models_dir_path):\n",
    "        os.makedirs(models_dir_path)\n",
    "        \n",
    "    results_dir_path =f'{models_dir_path}/results_ts_{timestamp}'\n",
    "    os.makedirs(results_dir_path)\n",
    "        \n",
    "    train_path =f\"{results_dir_path}/train.csv\"\n",
    "    test_path =f\"{results_dir_path}/test.csv\"\n",
    "\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(train_path, index=False, header=True)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(test_path, index=False, header=True)\n",
    "\n",
    "    train = h2o.import_file(train_path)\n",
    "    test = h2o.import_file(test_path)\n",
    "\n",
    "    h2o.init(max_mem_size=\"36G\")\n",
    "\n",
    "    x = train.columns\n",
    "    train[\"OFFER_STATUS\"] = train[\"OFFER_STATUS\"].asfactor()\n",
    "    x.remove(\"OFFER_STATUS\")\n",
    "    aml = H2OAutoML(\n",
    "        max_models=max_models,\n",
    "        balance_classes=balance_classes,\n",
    "        max_runtime_secs=int(max_runtime_secs),\n",
    "        seed=42,\n",
    "    )\n",
    "    if  training_with_all:\n",
    "        saved_model_path=f'{results_dir_path}/model_withALL'\n",
    "        print(f'[INFO] Model will be saved here:\"{saved_model_path}\"')\n",
    "    \n",
    "    #print(train)\n",
    "    #import sys;sys.exit(\"dsds\")\n",
    "    \n",
    "    aml.train(x=x, y=\"OFFER_STATUS\", training_frame=train)\n",
    "    \n",
    "    print(\"[INFO] Timestamp:\",timestamp)\n",
    "    print('[INFO] AML Leaderboard',aml.leaderboard)\n",
    "    \n",
    "    model_bac_scores = []\n",
    "    \n",
    "    if  training_with_all:\n",
    "        saved_model_path=f'{results_dir_path}/model_withALL'\n",
    "        h2o.save_model(model=aml.leader, path=saved_model_path, force=True)\n",
    "    else:\n",
    "        for i in range(int(aml.max_models * 1.5)):\n",
    "            current_model = aml.leaderboard[i, 0]\n",
    "            if current_model == \"NA\":\n",
    "                print(f\"[INFO] Found {i} models in total.\")\n",
    "                break\n",
    "            current_model = h2o.get_model(current_model)\n",
    "            new_pred = current_model.predict(test)\n",
    "            new_pred = new_pred[0].as_data_frame().values.flatten()\n",
    "            model_bac_score = balanced_accuracy_score(Y_test, new_pred)\n",
    "            print(f'[INFO] Model #{i}, BAC={model_bac_score}.')\n",
    "            model_bac_scores.append(model_bac_score)\n",
    "\n",
    "        index_max_bac_score = model_bac_scores.index(max(model_bac_scores))\n",
    "    \n",
    "        saved_model_path=f'{results_dir_path}/model_{\"bac_%.3f\" % model_bac_scores[index_max_bac_score]}'\n",
    "        h2o.save_model(model=h2o.get_model(aml.leaderboard[index_max_bac_score, 0]), \n",
    "                   path=saved_model_path, force=True)\n",
    "    \n",
    "    print(\"[INFO] RESULTS:\")\n",
    "    print(f' > Train data saved to:\"{train_path}\".')\n",
    "    print(f' > Test data saved to :\"{test_path}\".')\n",
    "    print(f' > H20 Model saved to :\"{saved_model_path}\".')\n",
    "\n",
    "    return aml, model_bac_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fd535aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 43;\n",
       "                var nbb_unformatted_code = \"def code_block(params:dict, verbose=False, calc_feature_importances=False):\\n    \\n    ####### Get Parameters - Start ################\\n    df = pd.read_csv(params['preprocessed_data_path']) # Read Data\\n    ###############################################\\n    \\n    ####### Block Specific Helper Functions #######\\n    def print_if_verbose(*args, **kwargs):\\n        if verbose:\\n            print(*args, **kwargs)\\n    ###############################################\\n\\n    ###############################################\\n    id_columns = [\\\"CUSTOMER\\\", \\\"TEST_SET_ID\\\", \\\"IDX_CUSTOMER\\\"]\\n    #unnecessary_reduced_cols = [\\n    #    \\\"OFFER_TYPE_REDUCED_1\\\",\\n    #    \\\"OFFER_TYPE_REDUCED_2\\\",\\n    #    \\\"SALES_OFFICE_REDUCED\\\",\\n    #]\\n\\n    #to_be_dropped_cols = id_columns + unnecessary_reduced_cols\\n    to_be_dropped_cols = id_columns\\n    df = df.drop(to_be_dropped_cols, axis=1)\\n\\n    # new columns\\n    df[\\\"ADDITIONAL_COST\\\"] = df[\\\"OFFER_PRICE\\\"] - df[\\\"MATERIAL_COST\\\"] - df[\\\"SERVICE_COST\\\"]\\n    df[\\\"TOTAL_COST\\\"] = df[\\\"MATERIAL_COST\\\"] + df[\\\"SERVICE_COST\\\"]\\n\\n    ###############################################\\n\\n    assert 'OFFER_STATUS' not in type_separator(df)[\\\"categorical\\\"]\\n    for col in type_separator(df)[\\\"categorical\\\"]:\\n        num_unq = len(df[col].unique())\\n        trimmed_col = col.strip().replace(\\\" \\\", \\\"_\\\")\\n        if num_unq < 5:\\n            print_if_verbose(f\\\"[INFO] Col:{col},num_of_unq:{num_unq}, applying 1-HOT encoding.\\\")\\n            onehot_df = pd.get_dummies(df[col])\\n            onehot_df = onehot_df.add_prefix(trimmed_col + \\\"_1HOTENC_\\\")\\n            df = pd.concat((df, onehot_df), axis=1)\\n        elif num_unq >= 5:\\n            print_if_verbose(f\\\"[INFO] Col:{col},num_of_unq:{num_unq}, applying BINARY encoding.\\\")\\n            encoder = ce.BinaryEncoder(cols=[col])\\n            binenc_df = encoder.fit_transform(df[[col]])\\n            binenc_df.columns = [\\n                f\\\"{trimmed_col}_BINENC_{i}\\\" for i in range(len(binenc_df.columns))\\n            ]\\n            df = pd.concat((df, binenc_df), axis=1)\\n\\n    for col in type_separator(df)[\\\"categorical\\\"]:\\n        if col != 'OFFER_STATUS':\\n            df[col] = pd.Categorical(df[col])\\n\\n    ##########################\\n\\n    ###### ADDITIONAL < FEATURES - START\\n    add_less_than_features = True\\n    print(\\\"[WARN] Adding Less Than Features:\\\", add_less_than_features)\\n\\n    if add_less_than_features:\\n        raw_num_cols = type_separator(df)[\\\"numerical\\\"]\\n        nonraw_strings_in_cols = (\\\"OFFER_STATUS\\\",\\\"IS_NA\\\",\\\"HAS_\\\",\\\"1HOTENC\\\",\\\"BINENC\\\",\\\"IS_\\\",\\\"_LOG\\\")\\n        raw_num_cols = [a for a in raw_num_cols if not any( x in a for x in nonraw_strings_in_cols)]\\n\\n        raw_numeric_cols_combinations = list(itertools.combinations(raw_num_cols, r=2))\\n        new_less_than_cols = []\\n        for col1, col2 in raw_numeric_cols_combinations:\\n            new_col = np.where(\\n                np.isnan(df[col1]) | np.isnan(df[col2]), np.nan, (df[col1] < df[col2])\\n            )\\n            name_of_new_col = col1 + \\\"_<_\\\" + col2\\n\\n            if (len(np.unique(new_col[~np.isnan(new_col)])) > 1):  # Not all values are 1 or 0\\n                new_less_than_cols.append(pd.Series(new_col, name=name_of_new_col))\\n                print_if_verbose(f'[INFO] Added new \\\"less than\\\" column: \\\"{name_of_new_col}\\\".')\\n            else:\\n                print_if_verbose(f'[INFO] NOT added \\\"Less Then\\\" column: \\\"{name_of_new_col}\\\".')\\n\\n        df = pd.concat([df, pd.concat(new_less_than_cols, axis=1)], axis=1)\\n\\n    ############# DROPPP -START\\n    drop_cols = [\\n    #    ############## Correlation are same or very similar\\n    #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Privately Owned/Publicly Traded\\\",\\n    #    \\\"OWNERSHIP_NA_AS_NO_INFO_REDUCED_1HOTENC_Privately Owned/Publicly Traded\\\",\\n    #    \\\"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Privately Owned/Publicly Traded\\\",\\n    #    \\\"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_Privately Owned/Publicly Traded\\\",\\n    #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Individual Person\\\",\\n    #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Governmental\\\",\\n    #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_No information\\\",\\n    #    \\\"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_NOT_GIVEN\\\",\\n    #    \\\"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_NOT_GIVEN\\\",\\n    #    \\\"OWNERSHIP_REDUCED_1HOTENC_NOT_GIVEN\\\",\\n    #    \\\"TECH_REDUCED_2_IS_F\\\",\\n    #    \\\"TECH_BINENC_0\\\",\\n    #    \\\"SINCE_CREATION_YEAR_<_REV_PERCENTAGE_INCREASE_NO_OUTLIER\\\",  # (34, False)\\n    #    ############## Experimentally\\n    #    \\\"TOTAL_COSTS_PRODUCT_LOG\\\",\\n    #    \\\"CURRENCY_BINENC_0\\\",  # (19, False)\\n    #    \\\"OWNERSHIP_BINENC_0\\\",  # (20, False)\\n    #    \\\"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Individual Person\\\",  # (21, False)\\n    #    \\\"IS_NA_SALES_LOCATION\\\",  # (22, False)\\n    #    \\\"IS_NA_SALES_OFFICE\\\",  # (23, False)\\n    #    ## From XGB Feature Importance\\n    #    \\\"SERVICE_COST_<_CREATION_YEAR\\\",  # 0.0\\n    #    \\\"REV_CURRENT_YEAR.1_<_REV_PERCENTAGE_INCREASE\\\",  # 0.0\\n    #    \\\"CREATION_YEAR_<_TOTAL_COST\\\",  # 0.0\\n    #    \\\"CREATION_YEAR_<_REV_PERCENTAGE_INCREASE\\\",  # 0.0\\n    #    \\\"CREATION_YEAR_<_ADDITIONAL_COST\\\",  # 0.0\\n    ]\\n\\n    for col in drop_cols:\\n        if col in list(df.columns):\\n            print_if_verbose(\\\"[INFO] Dropped:\\\", col)\\n            df = df.drop(col, axis=1)\\n\\n    if calc_feature_importances:\\n        def calculate_feature_importances():\\n            from boruta import BorutaPy\\n            from sklearn.ensemble import RandomForestClassifier\\n\\n            df_new = df[~np.isnan(df[\\\"OFFER_STATUS\\\"])].copy()\\n            columns_to_be_dropped = get_null_columns(df_new)\\n            if \\\"categorical\\\" in type_separator(df_new):\\n                columns_to_be_dropped += [\\n                    col for col in type_separator(df_new)[\\\"categorical\\\"]\\n                    if col in df_new.columns\\n                ]\\n            columns_to_be_dropped = set(columns_to_be_dropped)\\n            if \\\"OFFER_STATUS\\\" in columns_to_be_dropped:\\n                columns_to_be_dropped.remove(\\\"OFFER_STATUS\\\")\\n\\n            df_new = df_new.drop(columns_to_be_dropped, axis=1)\\n            X, y = df_new.drop(\\\"OFFER_STATUS\\\", axis=1), df_new[\\\"OFFER_STATUS\\\"]\\n\\n            forest = RandomForestClassifier(n_jobs=-1, class_weight=\\\"balanced\\\", max_depth=5)\\n            forest.fit(X, y)\\n            \\n            feat_selector = BorutaPy( # define Boruta feature selection method\\n                forest, n_estimators=\\\"auto\\\", verbose=2,\\n                random_state=42,\\n            )\\n            \\n            feat_selector.fit(X.to_numpy(), y.to_numpy()) # find all relevant features\\n            feature_importances = list(zip(feat_selector.ranking_,feat_selector.support_,X.columns))\\n            for item in sorted(feature_importances):\\n                print(item, \\\",\\\")\\n        calculate_feature_importances()\\n        \\n    if params['train_all_mode']:\\n        df_for_unlabeled_set = df[np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n        df_for_labeled_set = df[~np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n        \\n        X_train, y_train = df_for_labeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1), df_for_labeled_set[\\\"OFFER_STATUS\\\"]\\n        X_test = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n        y_test = df_for_unlabeled_set[\\\"OFFER_STATUS\\\"]\\n    else:\\n        df_for_test = df[np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n        df = df[~np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n\\n        X, y = df.drop(\\\"OFFER_STATUS\\\", axis=1), df[\\\"OFFER_STATUS\\\"] # Col Selection & Conversion\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\\n    \\n    if params['classifer'] == 'h2o':\\n        return apply_h2o(data=[X_train, X_test, y_train, y_test],max_runtime_secs=params['max_runtime_secs'],\\n                         training_with_all=params['train_all_mode'])\";\n",
       "                var nbb_formatted_code = \"def code_block(params: dict, verbose=False, calc_feature_importances=False):\\n\\n    ####### Get Parameters - Start ################\\n    df = pd.read_csv(params[\\\"preprocessed_data_path\\\"])  # Read Data\\n    ###############################################\\n\\n    ####### Block Specific Helper Functions #######\\n    def print_if_verbose(*args, **kwargs):\\n        if verbose:\\n            print(*args, **kwargs)\\n\\n    ###############################################\\n\\n    ###############################################\\n    id_columns = [\\\"CUSTOMER\\\", \\\"TEST_SET_ID\\\", \\\"IDX_CUSTOMER\\\"]\\n    # unnecessary_reduced_cols = [\\n    #    \\\"OFFER_TYPE_REDUCED_1\\\",\\n    #    \\\"OFFER_TYPE_REDUCED_2\\\",\\n    #    \\\"SALES_OFFICE_REDUCED\\\",\\n    # ]\\n\\n    # to_be_dropped_cols = id_columns + unnecessary_reduced_cols\\n    to_be_dropped_cols = id_columns\\n    df = df.drop(to_be_dropped_cols, axis=1)\\n\\n    # new columns\\n    df[\\\"ADDITIONAL_COST\\\"] = df[\\\"OFFER_PRICE\\\"] - df[\\\"MATERIAL_COST\\\"] - df[\\\"SERVICE_COST\\\"]\\n    df[\\\"TOTAL_COST\\\"] = df[\\\"MATERIAL_COST\\\"] + df[\\\"SERVICE_COST\\\"]\\n\\n    ###############################################\\n\\n    assert \\\"OFFER_STATUS\\\" not in type_separator(df)[\\\"categorical\\\"]\\n    for col in type_separator(df)[\\\"categorical\\\"]:\\n        num_unq = len(df[col].unique())\\n        trimmed_col = col.strip().replace(\\\" \\\", \\\"_\\\")\\n        if num_unq < 5:\\n            print_if_verbose(\\n                f\\\"[INFO] Col:{col},num_of_unq:{num_unq}, applying 1-HOT encoding.\\\"\\n            )\\n            onehot_df = pd.get_dummies(df[col])\\n            onehot_df = onehot_df.add_prefix(trimmed_col + \\\"_1HOTENC_\\\")\\n            df = pd.concat((df, onehot_df), axis=1)\\n        elif num_unq >= 5:\\n            print_if_verbose(\\n                f\\\"[INFO] Col:{col},num_of_unq:{num_unq}, applying BINARY encoding.\\\"\\n            )\\n            encoder = ce.BinaryEncoder(cols=[col])\\n            binenc_df = encoder.fit_transform(df[[col]])\\n            binenc_df.columns = [\\n                f\\\"{trimmed_col}_BINENC_{i}\\\" for i in range(len(binenc_df.columns))\\n            ]\\n            df = pd.concat((df, binenc_df), axis=1)\\n\\n    for col in type_separator(df)[\\\"categorical\\\"]:\\n        if col != \\\"OFFER_STATUS\\\":\\n            df[col] = pd.Categorical(df[col])\\n\\n    ##########################\\n\\n    ###### ADDITIONAL < FEATURES - START\\n    add_less_than_features = True\\n    print(\\\"[WARN] Adding Less Than Features:\\\", add_less_than_features)\\n\\n    if add_less_than_features:\\n        raw_num_cols = type_separator(df)[\\\"numerical\\\"]\\n        nonraw_strings_in_cols = (\\n            \\\"OFFER_STATUS\\\",\\n            \\\"IS_NA\\\",\\n            \\\"HAS_\\\",\\n            \\\"1HOTENC\\\",\\n            \\\"BINENC\\\",\\n            \\\"IS_\\\",\\n            \\\"_LOG\\\",\\n        )\\n        raw_num_cols = [\\n            a for a in raw_num_cols if not any(x in a for x in nonraw_strings_in_cols)\\n        ]\\n\\n        raw_numeric_cols_combinations = list(itertools.combinations(raw_num_cols, r=2))\\n        new_less_than_cols = []\\n        for col1, col2 in raw_numeric_cols_combinations:\\n            new_col = np.where(\\n                np.isnan(df[col1]) | np.isnan(df[col2]), np.nan, (df[col1] < df[col2])\\n            )\\n            name_of_new_col = col1 + \\\"_<_\\\" + col2\\n\\n            if (\\n                len(np.unique(new_col[~np.isnan(new_col)])) > 1\\n            ):  # Not all values are 1 or 0\\n                new_less_than_cols.append(pd.Series(new_col, name=name_of_new_col))\\n                print_if_verbose(\\n                    f'[INFO] Added new \\\"less than\\\" column: \\\"{name_of_new_col}\\\".'\\n                )\\n            else:\\n                print_if_verbose(\\n                    f'[INFO] NOT added \\\"Less Then\\\" column: \\\"{name_of_new_col}\\\".'\\n                )\\n\\n        df = pd.concat([df, pd.concat(new_less_than_cols, axis=1)], axis=1)\\n\\n    ############# DROPPP -START\\n    drop_cols = [\\n        #    ############## Correlation are same or very similar\\n        #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Privately Owned/Publicly Traded\\\",\\n        #    \\\"OWNERSHIP_NA_AS_NO_INFO_REDUCED_1HOTENC_Privately Owned/Publicly Traded\\\",\\n        #    \\\"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Privately Owned/Publicly Traded\\\",\\n        #    \\\"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_Privately Owned/Publicly Traded\\\",\\n        #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Individual Person\\\",\\n        #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Governmental\\\",\\n        #    \\\"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_No information\\\",\\n        #    \\\"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_NOT_GIVEN\\\",\\n        #    \\\"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_NOT_GIVEN\\\",\\n        #    \\\"OWNERSHIP_REDUCED_1HOTENC_NOT_GIVEN\\\",\\n        #    \\\"TECH_REDUCED_2_IS_F\\\",\\n        #    \\\"TECH_BINENC_0\\\",\\n        #    \\\"SINCE_CREATION_YEAR_<_REV_PERCENTAGE_INCREASE_NO_OUTLIER\\\",  # (34, False)\\n        #    ############## Experimentally\\n        #    \\\"TOTAL_COSTS_PRODUCT_LOG\\\",\\n        #    \\\"CURRENCY_BINENC_0\\\",  # (19, False)\\n        #    \\\"OWNERSHIP_BINENC_0\\\",  # (20, False)\\n        #    \\\"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Individual Person\\\",  # (21, False)\\n        #    \\\"IS_NA_SALES_LOCATION\\\",  # (22, False)\\n        #    \\\"IS_NA_SALES_OFFICE\\\",  # (23, False)\\n        #    ## From XGB Feature Importance\\n        #    \\\"SERVICE_COST_<_CREATION_YEAR\\\",  # 0.0\\n        #    \\\"REV_CURRENT_YEAR.1_<_REV_PERCENTAGE_INCREASE\\\",  # 0.0\\n        #    \\\"CREATION_YEAR_<_TOTAL_COST\\\",  # 0.0\\n        #    \\\"CREATION_YEAR_<_REV_PERCENTAGE_INCREASE\\\",  # 0.0\\n        #    \\\"CREATION_YEAR_<_ADDITIONAL_COST\\\",  # 0.0\\n    ]\\n\\n    for col in drop_cols:\\n        if col in list(df.columns):\\n            print_if_verbose(\\\"[INFO] Dropped:\\\", col)\\n            df = df.drop(col, axis=1)\\n\\n    if calc_feature_importances:\\n\\n        def calculate_feature_importances():\\n            from boruta import BorutaPy\\n            from sklearn.ensemble import RandomForestClassifier\\n\\n            df_new = df[~np.isnan(df[\\\"OFFER_STATUS\\\"])].copy()\\n            columns_to_be_dropped = get_null_columns(df_new)\\n            if \\\"categorical\\\" in type_separator(df_new):\\n                columns_to_be_dropped += [\\n                    col\\n                    for col in type_separator(df_new)[\\\"categorical\\\"]\\n                    if col in df_new.columns\\n                ]\\n            columns_to_be_dropped = set(columns_to_be_dropped)\\n            if \\\"OFFER_STATUS\\\" in columns_to_be_dropped:\\n                columns_to_be_dropped.remove(\\\"OFFER_STATUS\\\")\\n\\n            df_new = df_new.drop(columns_to_be_dropped, axis=1)\\n            X, y = df_new.drop(\\\"OFFER_STATUS\\\", axis=1), df_new[\\\"OFFER_STATUS\\\"]\\n\\n            forest = RandomForestClassifier(\\n                n_jobs=-1, class_weight=\\\"balanced\\\", max_depth=5\\n            )\\n            forest.fit(X, y)\\n\\n            feat_selector = BorutaPy(  # define Boruta feature selection method\\n                forest,\\n                n_estimators=\\\"auto\\\",\\n                verbose=2,\\n                random_state=42,\\n            )\\n\\n            feat_selector.fit(X.to_numpy(), y.to_numpy())  # find all relevant features\\n            feature_importances = list(\\n                zip(feat_selector.ranking_, feat_selector.support_, X.columns)\\n            )\\n            for item in sorted(feature_importances):\\n                print(item, \\\",\\\")\\n\\n        calculate_feature_importances()\\n\\n    if params[\\\"train_all_mode\\\"]:\\n        df_for_unlabeled_set = df[np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n        df_for_labeled_set = df[~np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n\\n        X_train, y_train = (\\n            df_for_labeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1),\\n            df_for_labeled_set[\\\"OFFER_STATUS\\\"],\\n        )\\n        X_test = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n        y_test = df_for_unlabeled_set[\\\"OFFER_STATUS\\\"]\\n    else:\\n        df_for_test = df[np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n        df = df[~np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n\\n        X, y = (\\n            df.drop(\\\"OFFER_STATUS\\\", axis=1),\\n            df[\\\"OFFER_STATUS\\\"],\\n        )  # Col Selection & Conversion\\n        X_train, X_test, y_train, y_test = train_test_split(\\n            X, y, stratify=y, test_size=0.33, random_state=42\\n        )\\n\\n    if params[\\\"classifer\\\"] == \\\"h2o\\\":\\n        return apply_h2o(\\n            data=[X_train, X_test, y_train, y_test],\\n            max_runtime_secs=params[\\\"max_runtime_secs\\\"],\\n            training_with_all=params[\\\"train_all_mode\\\"],\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def code_block(params:dict, verbose=False, calc_feature_importances=False):\n",
    "    \n",
    "    ####### Get Parameters - Start ################\n",
    "    df = pd.read_csv(params['preprocessed_data_path']) # Read Data\n",
    "    ###############################################\n",
    "    \n",
    "    ####### Block Specific Helper Functions #######\n",
    "    def print_if_verbose(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    ###############################################\n",
    "\n",
    "    ###############################################\n",
    "    id_columns = [\"CUSTOMER\", \"TEST_SET_ID\", \"IDX_CUSTOMER\"]\n",
    "    #unnecessary_reduced_cols = [\n",
    "    #    \"OFFER_TYPE_REDUCED_1\",\n",
    "    #    \"OFFER_TYPE_REDUCED_2\",\n",
    "    #    \"SALES_OFFICE_REDUCED\",\n",
    "    #]\n",
    "\n",
    "    #to_be_dropped_cols = id_columns + unnecessary_reduced_cols\n",
    "    to_be_dropped_cols = id_columns\n",
    "    df = df.drop(to_be_dropped_cols, axis=1)\n",
    "\n",
    "    # new columns\n",
    "    df[\"ADDITIONAL_COST\"] = df[\"OFFER_PRICE\"] - df[\"MATERIAL_COST\"] - df[\"SERVICE_COST\"]\n",
    "    df[\"TOTAL_COST\"] = df[\"MATERIAL_COST\"] + df[\"SERVICE_COST\"]\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    assert 'OFFER_STATUS' not in type_separator(df)[\"categorical\"]\n",
    "    for col in type_separator(df)[\"categorical\"]:\n",
    "        num_unq = len(df[col].unique())\n",
    "        trimmed_col = col.strip().replace(\" \", \"_\")\n",
    "        if num_unq < 5:\n",
    "            print_if_verbose(f\"[INFO] Col:{col},num_of_unq:{num_unq}, applying 1-HOT encoding.\")\n",
    "            onehot_df = pd.get_dummies(df[col])\n",
    "            onehot_df = onehot_df.add_prefix(trimmed_col + \"_1HOTENC_\")\n",
    "            df = pd.concat((df, onehot_df), axis=1)\n",
    "        elif num_unq >= 5:\n",
    "            print_if_verbose(f\"[INFO] Col:{col},num_of_unq:{num_unq}, applying BINARY encoding.\")\n",
    "            encoder = ce.BinaryEncoder(cols=[col])\n",
    "            binenc_df = encoder.fit_transform(df[[col]])\n",
    "            binenc_df.columns = [\n",
    "                f\"{trimmed_col}_BINENC_{i}\" for i in range(len(binenc_df.columns))\n",
    "            ]\n",
    "            df = pd.concat((df, binenc_df), axis=1)\n",
    "\n",
    "    for col in type_separator(df)[\"categorical\"]:\n",
    "        if col != 'OFFER_STATUS':\n",
    "            df[col] = pd.Categorical(df[col])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    ###### ADDITIONAL < FEATURES - START\n",
    "    add_less_than_features = True\n",
    "    print(\"[WARN] Adding Less Than Features:\", add_less_than_features)\n",
    "\n",
    "    if add_less_than_features:\n",
    "        raw_num_cols = type_separator(df)[\"numerical\"]\n",
    "        nonraw_strings_in_cols = (\"OFFER_STATUS\",\"IS_NA\",\"HAS_\",\"1HOTENC\",\"BINENC\",\"IS_\",\"_LOG\")\n",
    "        raw_num_cols = [a for a in raw_num_cols if not any( x in a for x in nonraw_strings_in_cols)]\n",
    "\n",
    "        raw_numeric_cols_combinations = list(itertools.combinations(raw_num_cols, r=2))\n",
    "        new_less_than_cols = []\n",
    "        for col1, col2 in raw_numeric_cols_combinations:\n",
    "            new_col = np.where(\n",
    "                np.isnan(df[col1]) | np.isnan(df[col2]), np.nan, (df[col1] < df[col2])\n",
    "            )\n",
    "            name_of_new_col = col1 + \"_<_\" + col2\n",
    "\n",
    "            if (len(np.unique(new_col[~np.isnan(new_col)])) > 1):  # Not all values are 1 or 0\n",
    "                new_less_than_cols.append(pd.Series(new_col, name=name_of_new_col))\n",
    "                print_if_verbose(f'[INFO] Added new \"less than\" column: \"{name_of_new_col}\".')\n",
    "            else:\n",
    "                print_if_verbose(f'[INFO] NOT added \"Less Then\" column: \"{name_of_new_col}\".')\n",
    "\n",
    "        df = pd.concat([df, pd.concat(new_less_than_cols, axis=1)], axis=1)\n",
    "\n",
    "    ############# DROPPP -START\n",
    "    drop_cols = [\n",
    "    #    ############## Correlation are same or very similar\n",
    "    #    \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "    #    \"OWNERSHIP_NA_AS_NO_INFO_REDUCED_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "    #    \"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "    #    \"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "    #    \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Individual Person\",\n",
    "    #    \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Governmental\",\n",
    "    #    \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_No information\",\n",
    "    #    \"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_NOT_GIVEN\",\n",
    "    #    \"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_NOT_GIVEN\",\n",
    "    #    \"OWNERSHIP_REDUCED_1HOTENC_NOT_GIVEN\",\n",
    "    #    \"TECH_REDUCED_2_IS_F\",\n",
    "    #    \"TECH_BINENC_0\",\n",
    "    #    \"SINCE_CREATION_YEAR_<_REV_PERCENTAGE_INCREASE_NO_OUTLIER\",  # (34, False)\n",
    "    #    ############## Experimentally\n",
    "    #    \"TOTAL_COSTS_PRODUCT_LOG\",\n",
    "    #    \"CURRENCY_BINENC_0\",  # (19, False)\n",
    "    #    \"OWNERSHIP_BINENC_0\",  # (20, False)\n",
    "    #    \"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Individual Person\",  # (21, False)\n",
    "    #    \"IS_NA_SALES_LOCATION\",  # (22, False)\n",
    "    #    \"IS_NA_SALES_OFFICE\",  # (23, False)\n",
    "    #    ## From XGB Feature Importance\n",
    "    #    \"SERVICE_COST_<_CREATION_YEAR\",  # 0.0\n",
    "    #    \"REV_CURRENT_YEAR.1_<_REV_PERCENTAGE_INCREASE\",  # 0.0\n",
    "    #    \"CREATION_YEAR_<_TOTAL_COST\",  # 0.0\n",
    "    #    \"CREATION_YEAR_<_REV_PERCENTAGE_INCREASE\",  # 0.0\n",
    "    #    \"CREATION_YEAR_<_ADDITIONAL_COST\",  # 0.0\n",
    "    ]\n",
    "\n",
    "    for col in drop_cols:\n",
    "        if col in list(df.columns):\n",
    "            print_if_verbose(\"[INFO] Dropped:\", col)\n",
    "            df = df.drop(col, axis=1)\n",
    "\n",
    "    if calc_feature_importances:\n",
    "        def calculate_feature_importances():\n",
    "            from boruta import BorutaPy\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "            df_new = df[~np.isnan(df[\"OFFER_STATUS\"])].copy()\n",
    "            columns_to_be_dropped = get_null_columns(df_new)\n",
    "            if \"categorical\" in type_separator(df_new):\n",
    "                columns_to_be_dropped += [\n",
    "                    col for col in type_separator(df_new)[\"categorical\"]\n",
    "                    if col in df_new.columns\n",
    "                ]\n",
    "            columns_to_be_dropped = set(columns_to_be_dropped)\n",
    "            if \"OFFER_STATUS\" in columns_to_be_dropped:\n",
    "                columns_to_be_dropped.remove(\"OFFER_STATUS\")\n",
    "\n",
    "            df_new = df_new.drop(columns_to_be_dropped, axis=1)\n",
    "            X, y = df_new.drop(\"OFFER_STATUS\", axis=1), df_new[\"OFFER_STATUS\"]\n",
    "\n",
    "            forest = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5)\n",
    "            forest.fit(X, y)\n",
    "            \n",
    "            feat_selector = BorutaPy( # define Boruta feature selection method\n",
    "                forest, n_estimators=\"auto\", verbose=2,\n",
    "                random_state=42,\n",
    "            )\n",
    "            \n",
    "            feat_selector.fit(X.to_numpy(), y.to_numpy()) # find all relevant features\n",
    "            feature_importances = list(zip(feat_selector.ranking_,feat_selector.support_,X.columns))\n",
    "            for item in sorted(feature_importances):\n",
    "                print(item, \",\")\n",
    "        calculate_feature_importances()\n",
    "        \n",
    "    if params['train_all_mode']:\n",
    "        df_for_unlabeled_set = df[np.isnan(df[\"OFFER_STATUS\"])]\n",
    "        df_for_labeled_set = df[~np.isnan(df[\"OFFER_STATUS\"])]\n",
    "        \n",
    "        X_train, y_train = df_for_labeled_set.drop([\"OFFER_STATUS\"], axis=1), df_for_labeled_set[\"OFFER_STATUS\"]\n",
    "        X_test = df_for_unlabeled_set.drop([\"OFFER_STATUS\"], axis=1)\n",
    "        y_test = df_for_unlabeled_set[\"OFFER_STATUS\"]\n",
    "    else:\n",
    "        df_for_test = df[np.isnan(df[\"OFFER_STATUS\"])]\n",
    "        df = df[~np.isnan(df[\"OFFER_STATUS\"])]\n",
    "\n",
    "        X, y = df.drop(\"OFFER_STATUS\", axis=1), df[\"OFFER_STATUS\"] # Col Selection & Conversion\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    if params['classifer'] == 'h2o':\n",
    "        return apply_h2o(data=[X_train, X_test, y_train, y_test],max_runtime_secs=params['max_runtime_secs'],\n",
    "                         training_with_all=params['train_all_mode'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ef5696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Adding Less Than Features: True\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>57 mins 32 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_iceking_xb133p</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>31.75 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         57 mins 32 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    3 days\n",
       "H2O_cluster_name:           H2O_from_python_iceking_xb133p\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    31.75 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ----------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>57 mins 34 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_iceking_xb133p</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>31.75 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         57 mins 34 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    3 days\n",
       "H2O_cluster_name:           H2O_from_python_iceking_xb133p\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    31.75 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ----------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model will be saved here:\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/model_withALL\"\n",
      "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n",
      "[INFO] Timestamp: 2022-01-29_18:20:43\n",
      "[INFO] AML Leaderboard "
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                                </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_7_AutoML_16_20220129_182045   </td><td style=\"text-align: right;\">0.829642</td><td style=\"text-align: right;\"> 0.368   </td><td style=\"text-align: right;\">0.945555</td><td style=\"text-align: right;\">              0.366578</td><td style=\"text-align: right;\">0.33743 </td><td style=\"text-align: right;\">0.113859</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_4_AutoML_16_20220129_182045   </td><td style=\"text-align: right;\">0.829569</td><td style=\"text-align: right;\"> 0.368071</td><td style=\"text-align: right;\">0.945466</td><td style=\"text-align: right;\">              0.348187</td><td style=\"text-align: right;\">0.337406</td><td style=\"text-align: right;\">0.113843</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_3_AutoML_16_20220129_182045   </td><td style=\"text-align: right;\">0.829469</td><td style=\"text-align: right;\"> 0.368117</td><td style=\"text-align: right;\">0.945469</td><td style=\"text-align: right;\">              0.36181 </td><td style=\"text-align: right;\">0.337401</td><td style=\"text-align: right;\">0.11384 </td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_2_AutoML_16_20220129_182045   </td><td style=\"text-align: right;\">0.829226</td><td style=\"text-align: right;\"> 0.368313</td><td style=\"text-align: right;\">0.945359</td><td style=\"text-align: right;\">              0.354662</td><td style=\"text-align: right;\">0.337464</td><td style=\"text-align: right;\">0.113882</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_1_AutoML_16_20220129_182045   </td><td style=\"text-align: right;\">0.828784</td><td style=\"text-align: right;\"> 0.368772</td><td style=\"text-align: right;\">0.94515 </td><td style=\"text-align: right;\">              0.364247</td><td style=\"text-align: right;\">0.337641</td><td style=\"text-align: right;\">0.114001</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_8_AutoML_16_20220129_182045</td><td style=\"text-align: right;\">0.827798</td><td style=\"text-align: right;\"> 0.369941</td><td style=\"text-align: right;\">0.944939</td><td style=\"text-align: right;\">              0.35658 </td><td style=\"text-align: right;\">0.3382  </td><td style=\"text-align: right;\">0.114379</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_3_AutoML_16_20220129_182045</td><td style=\"text-align: right;\">0.82774 </td><td style=\"text-align: right;\"> 0.369896</td><td style=\"text-align: right;\">0.944829</td><td style=\"text-align: right;\">              0.355575</td><td style=\"text-align: right;\">0.338073</td><td style=\"text-align: right;\">0.114293</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_5_AutoML_16_20220129_182045</td><td style=\"text-align: right;\">0.827692</td><td style=\"text-align: right;\"> 0.369959</td><td style=\"text-align: right;\">0.944864</td><td style=\"text-align: right;\">              0.354061</td><td style=\"text-align: right;\">0.338153</td><td style=\"text-align: right;\">0.114347</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_2_AutoML_16_20220129_182045</td><td style=\"text-align: right;\">0.827686</td><td style=\"text-align: right;\"> 0.369845</td><td style=\"text-align: right;\">0.944666</td><td style=\"text-align: right;\">              0.35111 </td><td style=\"text-align: right;\">0.337979</td><td style=\"text-align: right;\">0.11423 </td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_4_AutoML_16_20220129_182045</td><td style=\"text-align: right;\">0.827605</td><td style=\"text-align: right;\"> 0.370004</td><td style=\"text-align: right;\">0.944787</td><td style=\"text-align: right;\">              0.35899 </td><td style=\"text-align: right;\">0.338133</td><td style=\"text-align: right;\">0.114334</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] RESULTS:\n",
      " > Train data saved to:\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/train.csv\".\n",
      " > Test data saved to :\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/test.csv\".\n",
      " > H20 Model saved to :\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/model_withALL\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<h2o.automl._estimator.H2OAutoML at 0x7fa0de10a790>, [])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"result = code_block(\\n    params={\\n        \\\"max_runtime_secs\\\": 5*60*60,\\n        \\\"preprocessed_data_path\\\": \\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\",\\n        \\\"classifer\\\": \\\"h2o\\\",\\n        \\\"train_all_mode\\\": True,\\n    }\\n)\\nresult\";\n",
       "                var nbb_formatted_code = \"result = code_block(\\n    params={\\n        \\\"max_runtime_secs\\\": 5 * 60 * 60,\\n        \\\"preprocessed_data_path\\\": \\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\",\\n        \\\"classifer\\\": \\\"h2o\\\",\\n        \\\"train_all_mode\\\": True,\\n    }\\n)\\nresult\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = code_block(\n",
    "    params={\n",
    "        \"max_runtime_secs\": 5 * 60 * 60,\n",
    "        \"preprocessed_data_path\": \"interim_data/df_completed_1_2_3_with_mv_new.csv\",\n",
    "        \"classifer\": \"h2o\",\n",
    "        \"train_all_mode\": True,\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04a72578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>2 hours 24 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_iceking_xb133p</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>30.88 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         2 hours 24 mins\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    3 days\n",
       "H2O_cluster_name:           H2O_from_python_iceking_xb133p\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    30.88 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ----------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "stackedensemble prediction progress: |███████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">      p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.464275</td><td style=\"text-align: right;\">0.535725</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.467839</td><td style=\"text-align: right;\">0.532161</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.422071</td><td style=\"text-align: right;\">0.577929</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.464224</td><td style=\"text-align: right;\">0.535776</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.541066</td><td style=\"text-align: right;\">0.458934</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.53577 </td><td style=\"text-align: right;\">0.46423 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.459357</td><td style=\"text-align: right;\">0.540643</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.647896</td><td style=\"text-align: right;\">0.352104</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.385583</td><td style=\"text-align: right;\">0.614417</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.390319</td><td style=\"text-align: right;\">0.609681</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1    2318\n",
      "0    258 \n",
      "Name: prediction, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26018</th>\n",
       "      <td>1</td>\n",
       "      <td>26019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26032</th>\n",
       "      <td>0</td>\n",
       "      <td>26033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26049</th>\n",
       "      <td>1</td>\n",
       "      <td>26050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26057</th>\n",
       "      <td>1</td>\n",
       "      <td>26058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26085</th>\n",
       "      <td>0</td>\n",
       "      <td>26086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2576 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction     id\n",
       "5      1           6    \n",
       "8      1           9    \n",
       "13     1           14   \n",
       "34     1           35   \n",
       "35     0           36   \n",
       "...   ..           ..   \n",
       "26018  1           26019\n",
       "26032  0           26033\n",
       "26049  1           26050\n",
       "26057  1           26058\n",
       "26085  0           26086\n",
       "\n",
       "[2576 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"def real_predict_h2o(models_dir_path):\\n    from h2o.automl import H2OAutoML\\n    import h2o\\n\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n\\n    my_model = h2o.load_model(models_dir_path)\\n    df = pd.read_csv(\\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\")\\n\\n    df_for_unlabeled_set = df[np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n    ppppp = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n    x_realll = h2o.H2OFrame(ppppp)\\n\\n    ggl = my_model.predict(x_realll)\\n    gg = ggl[0].as_data_frame().values.flatten()\\n\\n    real_test = ppppp\\n    real_test[\\\"prediction\\\"] = gg\\n    test_set_id_col = df[np.isnan(df[\\\"OFFER_STATUS\\\"])][\\\"TEST_SET_ID\\\"]\\n    real_test = pd.concat([real_test[\\\"prediction\\\"], test_set_id_col], axis=1)\\n    real_test = real_test.rename(columns={\\\"TEST_SET_ID\\\": \\\"id\\\"})\\n\\n    real_test[\\\"prediction\\\"] = real_test[\\\"prediction\\\"].astype(int)\\n    real_test[\\\"id\\\"] = real_test[\\\"id\\\"].astype(int)\\n    print(ggl)\\n\\n    return real_test\\n\\n\\nlllresult = real_predict_h2o(\\n    # Submission 5\\n    # models_dir_path=\\\"h2o_models_with_data/results_ts_2022-01-29_13:09:41/model_bac_0.734/GBM_1_AutoML_31_20220129_130944\\\"\\n    # Submission 6\\n    models_dir_path=\\\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/model_withALL/StackedEnsemble_AllModels_7_AutoML_16_20220129_182045\\\"\\n)\\n\\nprint(lllresult.prediction.value_counts())\\n\\nlllresult.to_csv(\\\"predictions_versed_chimpanzee_6.csv\\\", header=True, index=False)\\n\\nlllresult\";\n",
       "                var nbb_formatted_code = \"def real_predict_h2o(models_dir_path):\\n    from h2o.automl import H2OAutoML\\n    import h2o\\n\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n\\n    my_model = h2o.load_model(models_dir_path)\\n    df = pd.read_csv(\\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\")\\n\\n    df_for_unlabeled_set = df[np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n    ppppp = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n    x_realll = h2o.H2OFrame(ppppp)\\n\\n    ggl = my_model.predict(x_realll)\\n    gg = ggl[0].as_data_frame().values.flatten()\\n\\n    real_test = ppppp\\n    real_test[\\\"prediction\\\"] = gg\\n    test_set_id_col = df[np.isnan(df[\\\"OFFER_STATUS\\\"])][\\\"TEST_SET_ID\\\"]\\n    real_test = pd.concat([real_test[\\\"prediction\\\"], test_set_id_col], axis=1)\\n    real_test = real_test.rename(columns={\\\"TEST_SET_ID\\\": \\\"id\\\"})\\n\\n    real_test[\\\"prediction\\\"] = real_test[\\\"prediction\\\"].astype(int)\\n    real_test[\\\"id\\\"] = real_test[\\\"id\\\"].astype(int)\\n    print(ggl)\\n\\n    return real_test\\n\\n\\nlllresult = real_predict_h2o(\\n    # Submission 5\\n    # models_dir_path=\\\"h2o_models_with_data/results_ts_2022-01-29_13:09:41/model_bac_0.734/GBM_1_AutoML_31_20220129_130944\\\"\\n    # Submission 6\\n    models_dir_path=\\\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/model_withALL/StackedEnsemble_AllModels_7_AutoML_16_20220129_182045\\\"\\n)\\n\\nprint(lllresult.prediction.value_counts())\\n\\nlllresult.to_csv(\\\"predictions_versed_chimpanzee_6.csv\\\", header=True, index=False)\\n\\nlllresult\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def real_predict_h2o(models_dir_path):\n",
    "    from h2o.automl import H2OAutoML\n",
    "    import h2o\n",
    "\n",
    "    h2o.init(max_mem_size=\"32G\")\n",
    "\n",
    "    my_model = h2o.load_model(models_dir_path)\n",
    "    df = pd.read_csv(\"interim_data/df_completed_1_2_3_with_mv_new.csv\")\n",
    "\n",
    "    df_for_unlabeled_set = df[np.isnan(df[\"OFFER_STATUS\"])]\n",
    "    ppppp = df_for_unlabeled_set.drop([\"OFFER_STATUS\"], axis=1)\n",
    "    x_realll = h2o.H2OFrame(ppppp)\n",
    "\n",
    "    ggl = my_model.predict(x_realll)\n",
    "    gg = ggl[0].as_data_frame().values.flatten()\n",
    "\n",
    "    real_test = ppppp\n",
    "    real_test[\"prediction\"] = gg\n",
    "    test_set_id_col = df[np.isnan(df[\"OFFER_STATUS\"])][\"TEST_SET_ID\"]\n",
    "    real_test = pd.concat([real_test[\"prediction\"], test_set_id_col], axis=1)\n",
    "    real_test = real_test.rename(columns={\"TEST_SET_ID\": \"id\"})\n",
    "\n",
    "    real_test[\"prediction\"] = real_test[\"prediction\"].astype(int)\n",
    "    real_test[\"id\"] = real_test[\"id\"].astype(int)\n",
    "    print(ggl)\n",
    "\n",
    "    return real_test\n",
    "\n",
    "\n",
    "lllresult = real_predict_h2o(\n",
    "    # Submission 5\n",
    "    # models_dir_path=\"h2o_models_with_data/results_ts_2022-01-29_13:09:41/model_bac_0.734/GBM_1_AutoML_31_20220129_130944\"\n",
    "    # Submission 6\n",
    "    models_dir_path=\"./h2o_models_with_data/results_ts_2022-01-29_18:20:43/model_withALL/StackedEnsemble_AllModels_7_AutoML_16_20220129_182045\"\n",
    ")\n",
    "\n",
    "print(lllresult.prediction.value_counts())\n",
    "\n",
    "lllresult.to_csv(\"predictions_versed_chimpanzee_6.csv\", header=True, index=False)\n",
    "\n",
    "lllresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f2482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Versed Chimpanzee.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
