{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaRsjuhwdGkL"
   },
   "source": [
    "## Common Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26823,
     "status": "ok",
     "timestamp": 1643399504606,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "MaWqZsOKUcSW",
    "outputId": "3613266f-824a-4e45-fcc4-4fef32c1b4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "'AC Use Cases Draft.pdf'\t     interim_data\n",
      " customers.csv\t\t\t     Plan.gdoc\n",
      "'data columns with colors.pdf'\t     submission_random.csv\n",
      "'Feature Importance Scores.gsheet'   transactions.csv\n",
      " geo.csv\t\t\t    'Versed Chimpanzee.ipynb'\n",
      "'Info - Analytics Cup 2022.pdf'\n"
     ]
    }
   ],
   "source": [
    "# Google Drive Operations - Only for Google Drive, Delete in Local Settings\n",
    "# Reference for using R in Colab: https://towardsdatascience.com/how-to-use-r-in-google-colab-b6e02d736497\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Versed Chimpanzee - AC Group\")\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50pscwa67S3q"
   },
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLlrPqha6VHF"
   },
   "source": [
    "### 1.1 Prepare Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2121,
     "status": "ok",
     "timestamp": 1643399506718,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "DW9le4Sy6plU",
    "outputId": "a0236eff-cead-46c6-bf8f-2182453f651b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "R[write to console]: ✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n",
      "✔ tibble  3.1.6     ✔ stringr 1.4.0\n",
      "✔ tidyr   1.1.4     ✔ forcats 0.5.1\n",
      "✔ readr   2.1.1     \n",
      "\n",
      "R[write to console]: ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(dplyr, warn.conflicts = F, quietly = T)\n",
    "library(tidyverse, warn.conflicts = F, quietly = T)\n",
    "library(lubridate, warn.conflicts = F, quietly = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxpfOCPV7Lap"
   },
   "source": [
    "### 1.2 Load and Inspect Transactions (transactions.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3041,
     "status": "ok",
     "timestamp": 1643399509751,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "kLCC7zAac_H6",
    "outputId": "71e62d53-0a83-4b47-847a-a988be5fc75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 26151 Columns: 23\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (12): MO_ID, SO_ID, CUSTOMER, END_CUSTOMER, PRICE_LIST, MO_CREATED_DATE,...\n",
      "dbl (11): OFFER_PRICE, SERVICE_LIST_PRICE, MATERIAL_COST, SERVICE_COST, ISIC...\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"MO_ID: 0/26151\"\n",
      "[1] \"SO_ID: 0/26151\"\n",
      "[1] \"CUSTOMER: 0/26151\"\n",
      "[1] \"END_CUSTOMER: 20114/26151\"\n",
      "[1] \"OFFER_PRICE: 0/26151\"\n",
      "[1] \"SERVICE_LIST_PRICE: 0/26151\"\n",
      "[1] \"MATERIAL_COST: 0/26151\"\n",
      "[1] \"SERVICE_COST: 0/26151\"\n",
      "[1] \"PRICE_LIST: 0/26151\"\n",
      "[1] \"ISIC: 1675/26151\"\n",
      "[1] \"MO_CREATED_DATE: 0/26151\"\n",
      "[1] \"SO_CREATED_DATE: 0/26151\"\n",
      "[1] \"TECH: 0/26151\"\n",
      "[1] \"OFFER_TYPE: 0/26151\"\n",
      "[1] \"BUSINESS_TYPE: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_A: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_B: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_C: 0/26151\"\n",
      "[1] \"OFFER_STATUS: 2576/26151\"\n",
      "[1] \"COSTS_PRODUCT_D: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_E: 0/26151\"\n",
      "[1] \"SALES_LOCATION: 37/26151\"\n",
      "[1] \"TEST_SET_ID: 23575/26151\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "trnsc_df = as_tibble(read_csv(\"transactions.csv\"))\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(trnsc_df)) {\n",
    "  print(paste0(names(trnsc_df)[i], \": \", sum(is.na(trnsc_df[, i])), \"/\", nrow(trnsc_df)))\n",
    "}\n",
    "\n",
    "trnsc_df$CUSTOMER = (substring(trnsc_df$CUSTOMER, 2, nchar(trnsc_df$CUSTOMER)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QriuLKgN783y"
   },
   "source": [
    "### 1.3 Load, Inspect and Merge Geographic Data (geo.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1643399510076,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "464Zi2qI6NRF",
    "outputId": "6e5035ea-30cc-40fa-9bb2-a528e6675049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 46 Columns: 4\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (4): COUNTRY, SALES_OFFICE, SALES_BRANCH, SALES_LOCATION\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"COUNTRY: 0/46\"\n",
      "[1] \"SALES_OFFICE: 2/46\"\n",
      "[1] \"SALES_BRANCH: 1/46\"\n",
      "[1] \"SALES_LOCATION: 1/46\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "geo_df = as_tibble(read_csv(\"geo.csv\"))\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(geo_df)) {\n",
    "  print(paste0(names(geo_df)[i], \": \", sum(is.na(geo_df[, i])), \"/\", nrow(geo_df)))\n",
    "}\n",
    "# Rename column COUNTRY COUNTRY_CODE, since it only contains codes like CH, FR\n",
    "geo_df = rename(geo_df, COUNTRY_CODE = COUNTRY)\n",
    "\n",
    "# Perform left join using dplyr\n",
    "trnsc_geo_df = left_join(trnsc_df, geo_df, by = 'SALES_LOCATION')\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = trnsc_geo_df, file = \"interim_data/trnsc_geo_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dj7zs3GH8ZJB"
   },
   "source": [
    "### 1.4 Load, Inspect and Merge Customers Data (customers.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1643399511504,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "ZyDvvzSp8jt7",
    "outputId": "f0e362fd-6418-4e85-8498-cffe20914adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 8452 Columns: 8\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr (5): REV_CURRENT_YEAR, CREATION_YEAR, OWNERSHIP, COUNTRY, CURRENCY\n",
      "dbl (3): CUSTOMER, REV_CURRENT_YEAR.1, REV_CURRENT_YEAR.2\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"CUSTOMER: 0/8452\"\n",
      "[1] \"REV_CURRENT_YEAR: 0/8452\"\n",
      "[1] \"REV_CURRENT_YEAR.1: 0/8452\"\n",
      "[1] \"REV_CURRENT_YEAR.2: 0/8452\"\n",
      "[1] \"CREATION_YEAR: 0/8452\"\n",
      "[1] \"OWNERSHIP: 0/8452\"\n",
      "[1] \"COUNTRY: 0/8452\"\n",
      "[1] \"CURRENCY: 0/8452\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "cst_df = as_tibble(read_csv(\"customers.csv\"))\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(cst_df)) {\n",
    "  print(paste0(names(cst_df)[i], \": \", sum(is.na(cst_df[, i])), \"/\", nrow(cst_df)))\n",
    "}\n",
    "\n",
    "##### NOTE FROM TEO: I THINK THE MERGE MAY BE WRONG, I'M MARKING THE SECTION ##########\n",
    "##### WHICH I THINK MAY NEED REPLACEMENT (SEE MY SECTION) #################\n",
    "##### REPLACE FROM HERE ###################\n",
    "# Change data type of CUSTOMER column in customers dataset\n",
    "cst_df$CUSTOMER <- as.character(cst_df$CUSTOMER)\n",
    "\n",
    "# Create IDX_CUSTOMER for trnsc_geo_df\n",
    "trnsc_geo_df = mutate(trnsc_geo_df,\n",
    "                      IDX_CUSTOMER = paste0(COUNTRY_CODE, \"_\", CUSTOMER))\n",
    "\n",
    "# Create IDX_CUSTOMER for customers\n",
    "cst_df = cst_df %>% mutate(COUNTRY_CODE = case_when(\n",
    "  COUNTRY == 'Switzerland' ~ \"CH\",\n",
    "  COUNTRY == 'France' ~ \"FR\"\n",
    "))\n",
    "\n",
    "cst_df = mutate(cst_df, IDX_CUSTOMER = paste0(COUNTRY_CODE, \"_\", CUSTOMER))\n",
    "\n",
    "# Perform left join using dplyr\n",
    "all_merged = left_join(trnsc_geo_df, cst_df, by = 'IDX_CUSTOMER')\n",
    "####### TO HERE ###############################\n",
    "\n",
    "# FIXME: Delete this before submisssion.\n",
    "write_csv(x = all_merged, file = \"interim_data/all_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcIb0u_B9FQ-"
   },
   "source": [
    "### 1.5 Fix Basic Problems in Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1643399511925,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "_2v-a-dD9cZJ",
    "outputId": "47bc2733-02b1-446e-c03b-02e26aef096f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] Number Of Cols: 36\"\n",
      "[1] \"[INFO] Number Of Rows: 26151\"\n",
      "[INFO] Names Of Columns:\n",
      " \"MO_ID\",  \"SO_ID\",  \"CUSTOMER.x\",  \"END_CUSTOMER\",  \"OFFER_PRICE\",  \"SERVICE_LIST_PRICE\",  \"MATERIAL_COST\",  \"SERVICE_COST\",  \"PRICE_LIST\",  \"ISIC\",  \"MO_CREATED_DATE\",  \"SO_CREATED_DATE\",  \"TECH\",  \"OFFER_TYPE\",  \"BUSINESS_TYPE\",  \"COSTS_PRODUCT_A\",  \"COSTS_PRODUCT_B\",  \"COSTS_PRODUCT_C\",  \"OFFER_STATUS\",  \"COSTS_PRODUCT_D\",  \"COSTS_PRODUCT_E\",  \"SALES_LOCATION\",  \"TEST_SET_ID\",  \"COUNTRY_CODE.x\",  \"SALES_OFFICE\",  \"SALES_BRANCH\",  \"IDX_CUSTOMER\",  \"CUSTOMER.y\",  \"REV_CURRENT_YEAR\",  \"REV_CURRENT_YEAR.1\",  \"REV_CURRENT_YEAR.2\",  \"CREATION_YEAR\",  \"OWNERSHIP\",  \"COUNTRY\",  \"CURRENCY\",  \"COUNTRY_CODE.y\", \n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "[1] \"[INFO] Glimpse:\"\n",
      "Rows: 26,151\n",
      "Columns: 36\n",
      "$ MO_ID              <chr> \"a050N000013fnfrQAA\", \"a050N000013fgL1QAI\", \"a050N0…\n",
      "$ SO_ID              <chr> \"a030N00001EochoQAB\", \"a030N00001EociNQAR\", \"a030N0…\n",
      "$ CUSTOMER.x         <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", …\n",
      "$ END_CUSTOMER       <chr> NA, NA, NA, \"4\", NA, NA, NA, NA, \"272\", NA, NA, \"No…\n",
      "$ OFFER_PRICE        <dbl> 1711.00, 26687.60, 6264.70, 4300.20, 13693.00, 2340…\n",
      "$ SERVICE_LIST_PRICE <dbl> 1395, 14651, 2296, 310, 5815, 5932, 930, 2310, 1207…\n",
      "$ MATERIAL_COST      <dbl> 1107, 9282, 1722, 246, 4674, 4674, 738, 1845, 7650,…\n",
      "$ SERVICE_COST       <dbl> 186.30, 7768.34, 2168.56, 2775.92, 4179.38, 15186.3…\n",
      "$ PRICE_LIST         <chr> \"SFT Standard\", \"CMT Installer\", \"SFT Standard\", \"S…\n",
      "$ ISIC               <dbl> 2100, 7110, 6820, 3821, 4719, 6419, 2710, 4742, 811…\n",
      "$ MO_CREATED_DATE    <chr> \"14.01.2019 08:43\", \"12.01.2019 16:36\", \"14.01.2019…\n",
      "$ SO_CREATED_DATE    <chr> \"14.01.2019 08:45\", \"14.01.2019 08:50\", \"14.01.2019…\n",
      "$ TECH               <chr> \"S\", \"C\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"C\", \"F\", \"…\n",
      "$ OFFER_TYPE         <chr> \"IN\", \"D\", \"FIR\", \"FIR\", \"FIR\", \"FIR\", \"FIR\", \"FIR\"…\n",
      "$ BUSINESS_TYPE      <chr> \"E\", \"N\", \"E\", \"M\", \"E\", \"N\", \"E\", \"E\", \"E\", \"E\", \"…\n",
      "$ COSTS_PRODUCT_A    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1000, …\n",
      "$ COSTS_PRODUCT_B    <dbl> 59.48, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.…\n",
      "$ COSTS_PRODUCT_C    <dbl> 0.00, 0.00, 0.00, 0.00, 1854.01, 0.00, 0.00, 0.00, …\n",
      "$ OFFER_STATUS       <chr> \"LOsT\", \"Lost\", \"WIN\", \"Win\", \"WIN\", NA, \"WIN\", \"Wo…\n",
      "$ COSTS_PRODUCT_D    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n",
      "$ COSTS_PRODUCT_E    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n",
      "$ SALES_LOCATION     <chr> \"Luzern Central\", \"Zürich East\", \"Luzern Central\", …\n",
      "$ TEST_SET_ID        <dbl> NA, NA, NA, NA, NA, 6, NA, NA, 9, NA, NA, NA, NA, 1…\n",
      "$ COUNTRY_CODE.x     <chr> \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH…\n",
      "$ SALES_OFFICE       <chr> \"Luzern\", \"Zürich\", \"Luzern\", \"Basel\", \"Geneva\", \"G…\n",
      "$ SALES_BRANCH       <chr> \"Branch Central\", \"Branch East\", \"Branch Central\", …\n",
      "$ IDX_CUSTOMER       <chr> \"CH_1\", \"CH_2\", \"CH_3\", \"CH_4\", \"CH_5\", \"CH_6\", \"CH…\n",
      "$ CUSTOMER.y         <chr> \"1\", \"2\", NA, \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"…\n",
      "$ REV_CURRENT_YEAR   <chr> \"\\\"81283.9230769231\\\"\", \"\\\"0\\\"\", NA, \"\\\"12668.84\\\"\"…\n",
      "$ REV_CURRENT_YEAR.1 <dbl> 81283.92, 0.00, NA, 12668.84, 7130.98, 518017.92, 2…\n",
      "$ REV_CURRENT_YEAR.2 <dbl> 32203.62, 0.00, NA, 32731.18, 10210.86, 736630.99, …\n",
      "$ CREATION_YEAR      <chr> \"01/01/2004\", \"01.01.2004\", NA, \"01.01.2003\", \"01/0…\n",
      "$ OWNERSHIP          <chr> \"Privately Owned/Publicly Traded\", \"Privately Owned…\n",
      "$ COUNTRY            <chr> \"Switzerland\", \"Switzerland\", NA, \"Switzerland\", \"S…\n",
      "$ CURRENCY           <chr> \"Chinese Yuan\", \"Chinese Yuan\", NA, \"Euro\", \"Euro\",…\n",
      "$ COUNTRY_CODE.y     <chr> \"CH\", \"CH\", NA, \"CH\", \"CH\", \"CH\", \"CH\", \"CH\", \"CH\",…\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "[1] \"[INFO] First 20 Rows:\"\n",
      "# A tibble: 20 × 36\n",
      "   MO_ID              SO_ID CUSTOMER.x END_CUSTOMER OFFER_PRICE SERVICE_LIST_PR…\n",
      "   <chr>              <chr> <chr>      <chr>              <dbl>            <dbl>\n",
      " 1 a050N000013fnfrQAA a030… 1          <NA>               1711              1395\n",
      " 2 a050N000013fgL1QAI a030… 2          <NA>              26688.            14651\n",
      " 3 a050N000013fnwdQAA a030… 3          <NA>               6265.             2296\n",
      " 4 a050N000013foAGQAY a030… 4          4                  4300.              310\n",
      " 5 a050N000013foKVQAY a030… 5          <NA>              13693              5815\n",
      " 6 a050N000013fpJ9QAI a030… 6          <NA>              23404.             5932\n",
      " 7 a050N000013fpYOQAY a030… 7          <NA>               1287               930\n",
      " 8 a050N000013fqA8QAI a030… 8          <NA>               8620.             2310\n",
      " 9 a050N00001B3D8RQAV a030… 9          272               52884.            12075\n",
      "10 a050N00001B3DXOQA3 a030… 10         <NA>               6485              2325\n",
      "11 a050N00001B3EpyQAF a030… 11         <NA>               1065.             1040\n",
      "12 a050N00001B3DePQAV a030… 12         No                 3935              3730\n",
      "13 a050N00001B3FICQA3 a030… 13         <NA>              29279              8050\n",
      "14 a050N00001B3FHTQA3 a030… 14         <NA>               6704              3275\n",
      "15 a050N00001B3FQBQA3 a030… 15         <NA>               2863.              805\n",
      "16 a050N000013fKrNQAU a030… 16         <NA>              30942.            14480\n",
      "17 a050N000013fLN9QAM a030… 17         <NA>               5605              2480\n",
      "18 a050N000013fN8EQAU a030… 18         <NA>               4325              1365\n",
      "19 a050N000013fNDYQA2 a030… 19         <NA>               1076.              775\n",
      "20 a050N000013fNTqQAM a030… 20         <NA>               1486.              915\n",
      "# … with 30 more variables: MATERIAL_COST <dbl>, SERVICE_COST <dbl>,\n",
      "#   PRICE_LIST <chr>, ISIC <dbl>, MO_CREATED_DATE <chr>, SO_CREATED_DATE <chr>,\n",
      "#   TECH <chr>, OFFER_TYPE <chr>, BUSINESS_TYPE <chr>, COSTS_PRODUCT_A <dbl>,\n",
      "#   COSTS_PRODUCT_B <dbl>, COSTS_PRODUCT_C <dbl>, OFFER_STATUS <chr>,\n",
      "#   COSTS_PRODUCT_D <dbl>, COSTS_PRODUCT_E <dbl>, SALES_LOCATION <chr>,\n",
      "#   TEST_SET_ID <dbl>, COUNTRY_CODE.x <chr>, SALES_OFFICE <chr>,\n",
      "#   SALES_BRANCH <chr>, IDX_CUSTOMER <chr>, CUSTOMER.y <chr>, …\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(paste(\"[INFO] Number Of Cols:\", ncol(all_merged)))\n",
    "print(paste(\"[INFO] Number Of Rows:\", nrow(all_merged)))\n",
    "cat(\"[INFO] Names Of Columns:\\n\", sprintf(\"\\\"%s\\\", \", names(all_merged)))\n",
    "\n",
    "cat(\"\\n\\n----------------------------------------------------\\n\\n\")\n",
    "\n",
    "print(\"[INFO] Glimpse:\")\n",
    "glimpse(all_merged)\n",
    "\n",
    "cat(\"\\n----------------------------------------------------\\n\\n\")\n",
    "\n",
    "print(\"[INFO] First 20 Rows:\")\n",
    "print(head(all_merged, n = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1643399511925,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "2x9cPHRc-qlR"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Feature Elimination & Fix\n",
    "all_merged = select(all_merged, -c(COUNTRY_CODE.y, CUSTOMER.y, COUNTRY, MO_ID, SO_ID))\n",
    "\n",
    "# Feature Renaming\n",
    "all_merged = rename(all_merged, COUNTRY_CODE = COUNTRY_CODE.x)\n",
    "all_merged = rename(all_merged, CUSTOMER = CUSTOMER.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1643399512471,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "TNHb5bxY9tfi",
    "outputId": "02f621c6-bdaf-472e-f471-7d36f32f2dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] Number of NA values in each column:\"\n",
      "[1] \"CUSTOMER: 0/26151\"\n",
      "[1] \"END_CUSTOMER: 20114/26151\"\n",
      "[1] \"OFFER_PRICE: 0/26151\"\n",
      "[1] \"SERVICE_LIST_PRICE: 0/26151\"\n",
      "[1] \"MATERIAL_COST: 0/26151\"\n",
      "[1] \"SERVICE_COST: 0/26151\"\n",
      "[1] \"PRICE_LIST: 0/26151\"\n",
      "[1] \"ISIC: 1675/26151\"\n",
      "[1] \"MO_CREATED_DATE: 0/26151\"\n",
      "[1] \"SO_CREATED_DATE: 0/26151\"\n",
      "[1] \"TECH: 0/26151\"\n",
      "[1] \"OFFER_TYPE: 0/26151\"\n",
      "[1] \"BUSINESS_TYPE: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_A: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_B: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_C: 0/26151\"\n",
      "[1] \"OFFER_STATUS: 2576/26151\"\n",
      "[1] \"COSTS_PRODUCT_D: 0/26151\"\n",
      "[1] \"COSTS_PRODUCT_E: 0/26151\"\n",
      "[1] \"SALES_LOCATION: 37/26151\"\n",
      "[1] \"TEST_SET_ID: 23575/26151\"\n",
      "[1] \"COUNTRY_CODE: 0/26151\"\n",
      "[1] \"SALES_OFFICE: 38/26151\"\n",
      "[1] \"SALES_BRANCH: 37/26151\"\n",
      "[1] \"IDX_CUSTOMER: 0/26151\"\n",
      "[1] \"REV_CURRENT_YEAR: 2885/26151\"\n",
      "[1] \"REV_CURRENT_YEAR.1: 2885/26151\"\n",
      "[1] \"REV_CURRENT_YEAR.2: 2885/26151\"\n",
      "[1] \"CREATION_YEAR: 2885/26151\"\n",
      "[1] \"OWNERSHIP: 2885/26151\"\n",
      "[1] \"CURRENCY: 2885/26151\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(all_merged)) { # for-loop over columns\n",
    "  print(paste0(names(all_merged)[i], \": \", sum(is.na(all_merged[, i])), \"/\", nrow(all_merged)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1643399513440,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "4EJQ1aUm-BQe",
    "outputId": "6245ded8-0d56-4a78-b581-43f64b66a95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] Unique values in each column:\"\n",
      "[1] \"PRICE_LIST: c(\\\"SFT Standard\\\", \\\"CMT Installer\\\", \\\"CMT End Customer\\\", \\\"Tarif public\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"TECH: c(\\\"S\\\", \\\"C\\\", \\\"F\\\", \\\"BP\\\", \\\"FP\\\", \\\"EPS\\\", \\\"E\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"BUSINESS_TYPE: c(\\\"E\\\", \\\"N\\\", \\\"M\\\", \\\"C\\\", \\\"T\\\", \\\"Exp\\\", \\\"New\\\", \\\"Mig\\\", \\\"S\\\", \\\"F\\\", \\\"R\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"OFFER_STATUS: c(\\\"LOsT\\\", \\\"Lost\\\", \\\"WIN\\\", \\\"Win\\\", NA, \\\"Won\\\", \\\"LOST\\\", \\\"Lose\\\", \\\"WON\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"COUNTRY_CODE: c(\\\"CH\\\", \\\"FR\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"SALES_BRANCH: c(\\\"Branch Central\\\", \\\"Branch East\\\", \\\"Branch West\\\", \\\"EPS CH\\\", NA, \\\"Grand Paris\\\", \\\"Sud Ouest\\\", \\\"Nord FR\\\", \\\"Ouest\\\", \\\"Centre-Est\\\", \\\"Grand Est\\\", \\\"Sud-Est\\\", \\\"Enterprise Business France\\\", \\\"SI\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"OWNERSHIP: c(\\\"Privately Owned/Publicly Traded\\\", NA, \\\"Governmental\\\", \\\"No information\\\", \\\"Individual Person\\\")\"\n",
      "[1] \"---------------\"\n",
      "[1] \"CURRENCY: c(\\\"Chinese Yuan\\\", NA, \\\"Euro\\\", \\\"US Dollar\\\", \\\"Pound Sterling\\\")\"\n",
      "[1] \"---------------\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(\"[INFO] Unique values in each column:\")\n",
    "for (i in 1:ncol(all_merged)) {\n",
    "  count_unq_vals = count(unique(all_merged[, i]))\n",
    "  if (count_unq_vals <20) {\n",
    "    print(paste0(names(all_merged)[i], \": \", unique(all_merged[, i])))\n",
    "    print(\"---------------\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = all_merged, file = \"interim_data/all_merged_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1643399513847,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "MT5oWavbcb6l"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "merged_df = all_merged\n",
    "write_csv(x = all_merged, file = \"interim_data/merged_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7hDf8Sr_-dW"
   },
   "source": [
    "### 1.6 Split Data into Labeled and Unlabeled (Test) Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1643399514667,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "5PopjRAoAI43",
    "outputId": "6952487e-f09d-497b-94b0-4704f690e187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2576\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "## TODO: YOU CAN SPLIT AFTERWARDS\n",
    "labeled_data = all_merged[is.na(all_merged$TEST_SET_ID),]\n",
    "test_data = all_merged[!is.na(all_merged$TEST_SET_ID),]\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = test_data, file = \"interim_data/labeled_set.csv\")\n",
    "write_csv(x = test_data, file = \"interim_data/test_set.csv\")\n",
    "\n",
    "labeled_data %>% nrow()\n",
    "test_data %>% nrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBsNJ7ktkzTe"
   },
   "source": [
    "## 2. Feature Elimination and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1643399514668,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "np2m7fQU_k7D"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df = data.frame(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbK8S6Wu304x"
   },
   "source": [
    "### 2.1 Delete Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1643399515101,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "vz6dt9cc361w",
    "outputId": "a123b6a0-e2e2-4b2c-848c-39e07791b89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"CUSTOMER\"           \"END_CUSTOMER\"       \"OFFER_PRICE\"       \n",
      " [4] \"SERVICE_LIST_PRICE\" \"MATERIAL_COST\"      \"SERVICE_COST\"      \n",
      " [7] \"PRICE_LIST\"         \"ISIC\"               \"SO_CREATED_DATE\"   \n",
      "[10] \"TECH\"               \"OFFER_TYPE\"         \"BUSINESS_TYPE\"     \n",
      "[13] \"COSTS_PRODUCT_A\"    \"COSTS_PRODUCT_B\"    \"COSTS_PRODUCT_C\"   \n",
      "[16] \"OFFER_STATUS\"       \"COSTS_PRODUCT_D\"    \"COSTS_PRODUCT_E\"   \n",
      "[19] \"SALES_LOCATION\"     \"TEST_SET_ID\"        \"COUNTRY_CODE\"      \n",
      "[22] \"SALES_OFFICE\"       \"IDX_CUSTOMER\"       \"REV_CURRENT_YEAR.1\"\n",
      "[25] \"REV_CURRENT_YEAR.2\" \"CREATION_YEAR\"      \"OWNERSHIP\"         \n",
      "[28] \"CURRENCY\"          \n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# Delete columns: MO_CREATED_DATE, SALES_BRANCH, REV_CURRENT_YEAR\n",
    "df = select (df,-c(MO_CREATED_DATE,SALES_BRANCH,REV_CURRENT_YEAR))\n",
    "\n",
    "names(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkBkY9ye9wlX"
   },
   "source": [
    "### 2.2 Process Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1643399515102,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "l3_PjxA9AIC3"
   },
   "outputs": [],
   "source": [
    "# Processed Features in 2.2:\n",
    "#  * OFFER_STATUS         [Modified]\n",
    "#  * END_CUSTOMER         [Deleted]\n",
    "#  * HAS_END_CUSTOMER     [Created]\n",
    "#  * ISIC                 [Deleted]\n",
    "#  * HAS_ISIC             [Created]\n",
    "#  * HAS_COSTS_PRODUCT_A  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_B  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_C  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_D  [Created]\n",
    "#  * HAS_COSTS_PRODUCT_E  [Created]\n",
    "#  * COUNTRY_CODE         [Deleted]\n",
    "#  * IS_COUNTRY_CODE_CH   [Created]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515102,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "oFpbqYdb_ZwH"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"OFFER_STATUS\" Feature: Replace string values with binary values\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    OFFER_STATUS = case_when(\n",
    "      OFFER_STATUS %in% c(\"WIN\",\"Win\",\"Won\",\"WON\") ~ \"1\",\n",
    "      OFFER_STATUS %in% c(\"LOsT\",\"Lost\",\"LOST\",\"Lose\") ~ \"0\",\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515103,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "EYyzZsuy-CO0"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"END_CUSTOMER\" Feature: Convert to HAS_END_CUSTOMER\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    HAS_END_CUSTOMER = case_when(\n",
    "      END_CUSTOMER %in% c(NA,\"No\") ~ 0,\n",
    "      TRUE ~ 1 # Includes numbers and \"Yes\" values\n",
    "      \n",
    "    )\n",
    "  )\n",
    "\n",
    "# Delete column: END_CUSTOMER\n",
    "df = select (df,-c(END_CUSTOMER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515104,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "PyYHc4Cf-ZqC"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"ISIC\" Feature: Convert to HAS_ISIC\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    HAS_ISIC = case_when(\n",
    "      ISIC %in% c(NA) ~ 0,\n",
    "      TRUE ~ 1 # Includes numbers\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Delete column: ISIC\n",
    "df = select (df,-c(ISIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515105,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "P4G6ov4t-qsf"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"COUNTRY_CODE\" Feature: Convert to IS_COUNTRY_CODE_CH\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    IS_COUNTRY_CODE_CH = case_when(\n",
    "      COUNTRY_CODE %in% c(\"CH\") ~ 1,\n",
    "      COUNTRY_CODE %in% c(\"FR\") ~ 0,\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Delete column: COUNTRY_CODE\n",
    "df = select (df,-c(COUNTRY_CODE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515106,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "bo42X4me-jtd"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"COSTS_PRODUCT_*\" Features: Convert to HAS_PRODUCT_*\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    HAS_COSTS_PRODUCT_A = case_when(\n",
    "      COSTS_PRODUCT_A %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_B = case_when(\n",
    "      COSTS_PRODUCT_B %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_C = case_when(\n",
    "      COSTS_PRODUCT_C %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_D = case_when(\n",
    "      COSTS_PRODUCT_D %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    ),\n",
    "    HAS_COSTS_PRODUCT_E = case_when(\n",
    "      COSTS_PRODUCT_E %in% c(0) ~ 0,\n",
    "      TRUE ~ 1 # Includes floating point numbers\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktMjLWJG96W7"
   },
   "source": [
    "### 2.3 Process Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515107,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "sMsW18q8Ba4H"
   },
   "outputs": [],
   "source": [
    "# Processed Features in 2.3:\n",
    "#  * TOTAL_COSTS_PRODUCT      [Created]\n",
    "#  * COSTS_PRODUCT_A          [Deleted]\n",
    "#  * COSTS_PRODUCT_B          [Deleted]\n",
    "#  * COSTS_PRODUCT_C          [Deleted]\n",
    "#  * COSTS_PRODUCT_D          [Deleted]\n",
    "#  * COSTS_PRODUCT_E          [Deleted]\n",
    "#  * CREATION_YEAR            [Modified]\n",
    "#  * SINCE_CREATION_YEAR      [Created]\n",
    "#  * REV_CURRENT_YEAR.1       [Modified]\n",
    "#  * REV_CURRENT_YEAR.2       [Modified]\n",
    "#  * REV_PERCENTAGE_INCREASE  [Created]\n",
    "#  * OWNERSHIP_NO_INFO_AS_NA  [Created]\n",
    "#  * OWNERSHIP_NA_AS_NO_INFO  [Created]\n",
    "#  * SO_CREATED_DATE_SCALED   [Created]\n",
    "#  * SO_CREATED_DATE          [Deleted]\n",
    "#  * SO_CREATED_DATE_INTEGER  [Deleted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399515107,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "0eJH5Emj-vgl"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Create \"TOTAL_COSTS_PRODUCT\" Feature: Sum of \"COSTS_PRODUCT_*\"\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    TOTAL_COSTS_PRODUCT = COSTS_PRODUCT_A\n",
    "    + COSTS_PRODUCT_B + \n",
    "      COSTS_PRODUCT_C + COSTS_PRODUCT_D + COSTS_PRODUCT_E\n",
    "  )\n",
    "\n",
    "# Delete columns: COSTS_PRODUCT_A to COSTS_PRODUCT_E\n",
    "df = select (df,-c(COSTS_PRODUCT_A,COSTS_PRODUCT_B,\n",
    "                                 COSTS_PRODUCT_C,COSTS_PRODUCT_D,\n",
    "                                 COSTS_PRODUCT_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399515108,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "VZsRAr2U_qsT"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"CREATION_YEAR\" Feature: Extract year\n",
    "df = df %>%\n",
    "  mutate(CREATION_YEAR = case_when(\n",
    "    is.character(CREATION_YEAR) ~ CREATION_YEAR %>%\n",
    "      substr(nchar(CREATION_YEAR) - 3, nchar(CREATION_YEAR)) %>%\n",
    "      as.numeric()\n",
    "  ))\n",
    "\n",
    "# Create \"SINCE_CREATION_YEAR\" Feature: 2021 - CREATION_YEAR\n",
    "df = df %>%\n",
    "  mutate(SINCE_CREATION_YEAR = case_when(!is.na(CREATION_YEAR) ~ as.double(2021 - CREATION_YEAR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1643399515109,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "bpclY2cGCv_B"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Manipulate \"REV_CURRENT_YEAR.1\" and \"REV_CURRENT_YEAR.2\" Feature: Convert to EUR\n",
    "\n",
    "# Convert negative values to zero\n",
    "# df$REV_CURRENT_YEAR.1 = ifelse(df$REV_CURRENT_YEAR.1 < 0,0,df$REV_CURRENT_YEAR.1)\n",
    "# df$REV_CURRENT_YEAR.2 = ifelse(df$REV_CURRENT_YEAR.2 < 0,0,df$REV_CURRENT_YEAR.2)\n",
    "\n",
    "# Create \"REV_PERCENTAGE_INCREASE\" Feature: REV_CURRENT_YEAR.2 to REV_CURRENT_YEAR.1\n",
    "# calculate_percentage_increase = function(new, old) {\n",
    "#   100 * (new - old) / old\n",
    "# }\n",
    "# df = df %>%\n",
    "#   mutate(REV_PERCENTAGE_INCREASE = case_when(\n",
    "#     !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2) ~\n",
    "#       calculate_percentage_increase(REV_CURRENT_YEAR.1, REV_CURRENT_YEAR.2)\n",
    "#   ))\n",
    "\n",
    "\n",
    "# df = df %>% mutate( # If REV_CURRENT_YEAR.1 is 0 or null, fill it from REV_CURRENT_YEAR.2\n",
    "#   REV_CURRENT_YEAR.1 = ifelse(\n",
    "#     ((REV_CURRENT_YEAR.1 <= 0) |\n",
    "#        is.na(REV_CURRENT_YEAR.1)) &\n",
    "#       !is.na(REV_CURRENT_YEAR.2),\n",
    "#     REV_CURRENT_YEAR.2,\n",
    "#     REV_CURRENT_YEAR.1\n",
    "#   ) %>% as.double()\n",
    "# )\n",
    "# \n",
    "# df = df %>% mutate( # If REV_CURRENT_YEAR.2 is 0 or null, fill it from REV_CURRENT_YEAR.1\n",
    "#   REV_CURRENT_YEAR.2 = ifelse(\n",
    "#     ((REV_CURRENT_YEAR.2 <= 0) |\n",
    "#        is.na(REV_CURRENT_YEAR.2)) &\n",
    "#       !is.na(REV_CURRENT_YEAR.1),\n",
    "#     REV_CURRENT_YEAR.1,\n",
    "#     REV_CURRENT_YEAR.2\n",
    "#   ) %>% as.double()\n",
    "# )\n",
    "\n",
    "\n",
    "# Note: 2020 and 2021 annual average exchange rates are used.\n",
    "# Source: https://www.x-rates.com/average/?from=USD&to=EUR&amount=1&year=2021\n",
    "df = df %>%\n",
    "  mutate(\n",
    "    REV_CURRENT_YEAR.1 = case_when(\n",
    "      CURRENCY ==  \"Pound Sterling\" ~ REV_CURRENT_YEAR.1 * 1.1438161149110808,\n",
    "      CURRENCY ==  \"Chinese Yuan\" ~ REV_CURRENT_YEAR.1 * 0.12906362243502054,\n",
    "      CURRENCY ==  \"US Dollar\" ~ REV_CURRENT_YEAR.1 * 0.8614249616963066,\n",
    "      CURRENCY ==  \"Euro\" ~ REV_CURRENT_YEAR.1\n",
    "    )\n",
    "  )\n",
    "\n",
    "\n",
    "#df = df %>%\n",
    "#  mutate(\n",
    "#    REV_CURRENT_YEAR.2 = case_when(\n",
    "#      CURRENCY ==  \"Pound Sterling\" ~ REV_CURRENT_YEAR.2 * 1.1438161149110808,\n",
    "#      CURRENCY ==  \"Chinese Yuan\" ~ REV_CURRENT_YEAR.2 * 0.12906362243502054,\n",
    "#      CURRENCY ==  \"US Dollar\" ~ REV_CURRENT_YEAR.2 * 0.8614249616963066,\n",
    "#      CURRENCY ==  \"Euro\" ~ REV_CURRENT_YEAR.2\n",
    "#    )\n",
    "#  )\n",
    "\n",
    "# Create \"REV_RATE\" Feature: REV_CURRENT_YEAR.2 to REV_CURRENT_YEAR.1\n",
    "df = df %>%\n",
    "  mutate(REV_RATE = case_when(\n",
    "    !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  REV_CURRENT_YEAR.1 !=0 & REV_CURRENT_YEAR.2 !=0 ~\n",
    "     REV_CURRENT_YEAR.1/REV_CURRENT_YEAR.2\n",
    "  ))\n",
    "\n",
    "# Create \"REV_AVG\" Feature: REV_CURRENT_YEAR.2 + REV_CURRENT_YEAR.1\n",
    "df = df %>%\n",
    "  mutate(REV_AVG = case_when(\n",
    "    !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  REV_CURRENT_YEAR.1 !=0 & REV_CURRENT_YEAR.2 !=0 ~\n",
    "     (REV_CURRENT_YEAR.1+REV_CURRENT_YEAR.2)/2,\n",
    "     !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  (REV_CURRENT_YEAR.1 ==0 | REV_CURRENT_YEAR.2 ==0) ~\n",
    "     (REV_CURRENT_YEAR.1+REV_CURRENT_YEAR.2),\n",
    "      !is.na(REV_CURRENT_YEAR.1) & !is.na(REV_CURRENT_YEAR.2)  &  REV_CURRENT_YEAR.1 ==0 & REV_CURRENT_YEAR.2 ==0 ~\n",
    "      0.00\n",
    "  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399515109,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "4XnCrUPEYO3n"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Create \"OWNERSHIP_NO_INFO_AS_NA\" Feature: Treat \"No information\" as NA value\n",
    "df$OWNERSHIP_NO_INFO_AS_NA = ifelse(df$OWNERSHIP ==  \"No information\", NA, df$OWNERSHIP)\n",
    "\n",
    "# Create \"OWNERSHIP_NA_AS_NO_INFO\" Feature: Treat NA values as \"No information\"\n",
    "df$OWNERSHIP_NA_AS_NO_INFO = ifelse(is.na(df$OWNERSHIP), \"No information\", df$OWNERSHIP)\n",
    "\n",
    "\n",
    "# Create \"SO_CREATED_DATE_SCALED\" Feature: Scale \"SO_CREATED_DATE\" x 100\n",
    "\n",
    "df$SO_CREATED_DATE_INTEGER = as_datetime(df$SO_CREATED_DATE, format = \"%d.%m.%Y %H:%M\") # Parse date-format 1\n",
    "date_format_2 = as_datetime(df$SO_CREATED_DATE, format = \"%Y-%m-%d %H:%M:%S\") # Parse date-format 1\n",
    "df$SO_CREATED_DATE_INTEGER[is.na(df$SO_CREATED_DATE_INTEGER)] = date_format_2[!is.na(date_format_2)]\n",
    "\n",
    "df$SO_CREATED_DATE_INTEGER = as.numeric(as.POSIXct(df$SO_CREATED_DATE_INTEGER))# Convert to Unix Time Stamp\n",
    "standart_scale = function (x) (x - mean(x, na.rm = T)) / sd(x, na.rm = T)\n",
    "\n",
    "df$SO_CREATED_DATE_SCALED = standart_scale(df$SO_CREATED_DATE_INTEGER) # Scale x 100\n",
    "\n",
    "# Delete column: \"SO_CREATED_DATE\"\n",
    "df = select (df,-c(SO_CREATED_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1643399516200,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "82eF6BV5ZYbE"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = df, file = \"interim_data/df_completed_1_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_HL0VLnb7o_"
   },
   "source": [
    "## 3. Dealing with Extreme Values, Filling Missing Values and Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFdincpk43N"
   },
   "source": [
    "### 3.1 Deal with Extreme Values in Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1643399516201,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "VyjZSA_ef6ab"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df$TOTAL_COSTS_PRODUCT = ifelse(df$TOTAL_COSTS_PRODUCT < 0,-df$TOTAL_COSTS_PRODUCT,df$TOTAL_COSTS_PRODUCT)\n",
    "df$TOTAL_COSTS_PRODUCT_LOG=log(df$TOTAL_COSTS_PRODUCT+1)\n",
    "\n",
    "df$SERVICE_COST = ifelse(df$SERVICE_COST < 0,-df$SERVICE_COST,df$SERVICE_COST)\n",
    "df$SERVICE_COST_LOG=log(df$SERVICE_COST+1)\n",
    "\n",
    "df$OFFER_PRICE_LOG=log(df$OFFER_PRICE)\n",
    "\n",
    "df$SERVICE_LIST_PRICE_LOG=log(df$SERVICE_LIST_PRICE+1)\n",
    "\n",
    "df$MATERIAL_COST_LOG=log(df$MATERIAL_COST+1)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.1=log(df$REV_CURRENT_YEAR.1+1)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.2=log(df$REV_CURRENT_YEAR.2+1)\n",
    "\n",
    "df$CREATION_YEAR_LOG=log(df$CREATION_YEAR)\n",
    "\n",
    "df$SINCE_CREATION_YEAR_LOG=log(df$SINCE_CREATION_YEAR+1)\n",
    "\n",
    "#For REV_PERCENTAGE_INCREASE\n",
    "#Q1 <- quantile(df$REV_PERCENTAGE_INCREASE, .25,na.rm=T)\n",
    "#Q3 <- quantile(df$REV_PERCENTAGE_INCREASE, .75,na.rm=T)\n",
    "#IQR <- IQR(df$REV_PERCENTAGE_INCREASE,na.rm=T)\n",
    "#df = df %>%  mutate(\n",
    "#  REV_PERCENTAGE_INCREASE_NO_OUTLIER = case_when(\n",
    "#    REV_PERCENTAGE_INCREASE < (Q1 - 3.0*IQR) ~ (Q1 - 3.0*IQR),\n",
    "#    REV_PERCENTAGE_INCREASE > (Q3 + 3.0*IQR) ~ (Q3 + 3.0*IQR),\n",
    "#    TRUE ~ REV_PERCENTAGE_INCREASE\n",
    "#  )\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSsIh60js_dH"
   },
   "source": [
    "### 3.2 Deal with Extreme Values in Character Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1643399516202,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "G8-H0hT9tTWs"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df$TECH_REDUCED_1 = ifelse(df$TECH %in% c(\"E\", \"EPS\", \"FP\", \"BP\"), \"E_EPS_FP_BP\", df$TECH)\n",
    "df$TECH_REDUCED_2_IS_F = ifelse(df$TECH == \"F\", 1, 0)\n",
    "\n",
    "df$OFFER_TYPE_REDUCED_1 = ifelse(\n",
    "  df$OFFER_TYPE %in% c(\n",
    "    \"FD\",\n",
    "    \"EH\",\n",
    "    \"FEI\",\n",
    "    \"MSYS\",\n",
    "    \"DCF\",\n",
    "    \"GAM\",\n",
    "    \"CP\",\n",
    "    \"CS\",\n",
    "    \"CI\",\n",
    "    \"EN\",\n",
    "    \"FIB\",\n",
    "    \"PAT\",\n",
    "    \"XCPS\"\n",
    "  ),\n",
    "  \"FD_EH_FEI_MSYS_DCF_GAM_CP_CS_CI_EN_FIB_PAT_XCPS\",\n",
    "  df$OFFER_TYPE\n",
    ")\n",
    "df$OFFER_TYPE_REDUCED_2 = ifelse(\n",
    "  df$OFFER_TYPE %in% c(\n",
    "    \"FED\",\n",
    "    \"CPP\",\n",
    "    \"ED\",\n",
    "    \"EV\",\n",
    "    \"FD\",\n",
    "    \"EH\",\n",
    "    \"FEI\",\n",
    "    \"MSYS\",\n",
    "    \"DCF\",\n",
    "    \"GAM\",\n",
    "    \"CP\",\n",
    "    \"CS\",\n",
    "    \"CI\",\n",
    "    \"EN\",\n",
    "    \"FIB\",\n",
    "    \"PAT\",\n",
    "    \"XCPS\"\n",
    "  ),\n",
    "  \"FED_CPP_ED_EV_FD_EH_FEI_MSYS_DCF_GAM_CP_CS_CI_EN_FIB_PAT_XCPS\",\n",
    "  df$OFFER_TYPE\n",
    ")\n",
    "\n",
    "df$OWNERSHIP_NA_AS_NO_INFO_REDUCED = ifelse(\n",
    "  df$OWNERSHIP_NA_AS_NO_INFO %in% c(\"Governmental\", \"Individual Person\", \"No information\"),\n",
    "  \"Governmental_IndividualPerson_Noinformation\",\n",
    "  df$OWNERSHIP_NA_AS_NO_INFO\n",
    ")\n",
    "\n",
    "df$OWNERSHIP_NO_INFO_AS_NA_REDUCED = ifelse(\n",
    "  df$OWNERSHIP_NO_INFO_AS_NA %in% c(\"Governmental\", \"Individual Person\"),\n",
    "  \"Governmental_IndividualPerson\",\n",
    "  df$OWNERSHIP_NO_INFO_AS_NA\n",
    ")\n",
    "\n",
    "df$OWNERSHIP_REDUCED = ifelse(\n",
    "  df$OWNERSHIP %in% c(\"Governmental\", \"Individual Person\", \"No information\"),\n",
    "  \"Governmental_IndividualPerson_Noinformation\",\n",
    "  df$OWNERSHIP\n",
    ")\n",
    "\n",
    "df$SALES_OFFICE_REDUCED = ifelse(\n",
    "  df$SALES_OFFICE %in% c(\n",
    "    \"Montpellier\",\n",
    "    \"Monaco\",\n",
    "    \"Limoges\",\n",
    "    \"Vertical Market\",\n",
    "    \"Others Functions\"\n",
    "  ),\n",
    "  \"Montpellier_Monaco_Limoges_Vertical Market_OthersFunctions\",\n",
    "  df$SALES_OFFICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4QqnZXbXx0R"
   },
   "source": [
    "### 3.3 Create IS_NA Columns and Remove the Duplicate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643399516203,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "Z8QeYxnZX2aR"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# For numeric columns\n",
    "#df$IS_NA_REV_PERCENTAGE_INCREASE = ifelse(is.na(df$REV_PERCENTAGE_INCREASE) == T,1,0)\n",
    "df$IS_NA_REV_RATE = ifelse(is.na(df$REV_RATE) == T,1,0)\n",
    "df$IS_NA_REV_AVG = ifelse(is.na(df$REV_AVG) == T,1,0)\n",
    "df$IS_NA_REV_CURRENT_YEAR = ifelse(is.na(df$REV_CURRENT_YEAR.1) == T,1,0)\n",
    "\n",
    "\n",
    "# For Categoric Columns\n",
    "df$IS_NA_SALES_LOCATION = ifelse(is.na(df$SALES_LOCATION) == T,1,0)\n",
    "df$IS_NA_SALES_OFFICE = ifelse(is.na(df$SALES_OFFICE) == T,1,0)\n",
    "df$IS_NA_CURRENCY = ifelse(is.na(df$CURRENCY) == T,1,0)\n",
    "df$IS_NA_OWNERSHIP_NO_INFO_AS_NA = ifelse(is.na(df$OWNERSHIP_NO_INFO_AS_NA) == T,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643399516204,
     "user": {
      "displayName": "Berk Sudan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhK0rH2nh-MrT8eLjlwllxnopFOxPdK95LJySKN9w=s64",
      "userId": "00865719443568149227"
     },
     "user_tz": -60
    },
    "id": "4NxPAaClAK5m"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = df, file = \"interim_data/df_completed_1_2_3_with_mv_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8YsFnPltjql"
   },
   "source": [
    "### 3.4 Fill Missing Values in Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7G8FcnoXgXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJpoIQv47k8h"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Replace with mean\n",
    "\n",
    "df$REV_CURRENT_YEAR.1[is.na(df$REV_CURRENT_YEAR.1)] = mean(df$REV_CURRENT_YEAR.1, na.rm=TRUE)\n",
    "\n",
    "df$REV_CURRENT_YEAR.2[is.na(df$REV_CURRENT_YEAR.2)] = mean(df$REV_CURRENT_YEAR.2, na.rm=TRUE)\n",
    "\n",
    "df$CREATION_YEAR[is.na(df$CREATION_YEAR)] = mean(df$CREATION_YEAR, na.rm=TRUE)\n",
    "\n",
    "df$SINCE_CREATION_YEAR[is.na(df$SINCE_CREATION_YEAR)] = mean(df$SINCE_CREATION_YEAR, na.rm=TRUE)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.1[is.na(df$REV_CURRENT_YEAR_LOG.1)] = mean(df$REV_CURRENT_YEAR_LOG.1, na.rm=TRUE)\n",
    "\n",
    "df$REV_CURRENT_YEAR_LOG.2[is.na(df$REV_CURRENT_YEAR_LOG.2)] = mean(df$REV_CURRENT_YEAR_LOG.2, na.rm=TRUE)\n",
    "\n",
    "df$CREATION_YEAR_LOG[is.na(df$CREATION_YEAR_LOG)] = mean(df$CREATION_YEAR_LOG, na.rm=TRUE)\n",
    "\n",
    "df$SINCE_CREATION_YEAR_LOG[is.na(df$SINCE_CREATION_YEAR_LOG)] = mean(df$SINCE_CREATION_YEAR_LOG, na.rm=TRUE)\n",
    "\n",
    "df$REV_PERCENTAGE_INCREASE_NO_OUTLIER[is.na(df$REV_PERCENTAGE_INCREASE_NO_OUTLIER)] = mean(df$REV_PERCENTAGE_INCREASE_NO_OUTLIER, na.rm=TRUE)\n",
    "\n",
    "df$REV_PERCENTAGE_INCREASE[is.na(df$REV_PERCENTAGE_INCREASE)] = mean(df$REV_PERCENTAGE_INCREASE, na.rm=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXb9iITP8_fl"
   },
   "source": [
    "### 3.5 Fill Missing Values in Character Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuLsQL2zKHnS"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df$SALES_LOCATION[is.na(df$SALES_LOCATION)] = \"Geneva West\"\n",
    "\n",
    "df$SALES_OFFICE[is.na(df$SALES_OFFICE)] = \"Geneva\"\n",
    "\n",
    "df$SALES_OFFICE_REDUCED[is.na(df$SALES_OFFICE_REDUCED)] = \"Geneva\"\n",
    "\n",
    "df$CURRENCY[is.na(df$CURRENCY)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP[is.na(df$OWNERSHIP)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP_NO_INFO_AS_NA[is.na(df$OWNERSHIP_NO_INFO_AS_NA)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP_NO_INFO_AS_NA_REDUCED[is.na(df$OWNERSHIP_NO_INFO_AS_NA_REDUCED)] = \"NOT_GIVEN\"\n",
    "\n",
    "df$OWNERSHIP_REDUCED[is.na(df$OWNERSHIP_REDUCED)] = \"NOT_GIVEN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRBBpQSzMPpo"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# FIXME: Delete this before submission.\n",
    "write_csv(x = df, file = \"interim_data/df_completed_1_2_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QNZP03OgL-R"
   },
   "source": [
    "## 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jhwiJS2CXSpg"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages(\"party\")\n",
    "library(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32gAGeFFXM9j"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "x = df\n",
    "\n",
    "x = x[is.na(x$OFFER_STATUS) == F,]\n",
    "\n",
    "x = x %>% select(-c(TEST_SET_ID,IDX_CUSTOMER,CUSTOMER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtthyQNlYLlU"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "x$OFFER_STATUS = as.numeric(x$OFFER_STATUS)\n",
    "\n",
    "a <- select_if(x, is.numeric)  \n",
    "colnames(a)\n",
    "cf1 <- cforest(a$OFFER_STATUS ~ . , data= a, control=cforest_unbiased(mtry=2,ntree=50)) # fit the random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rElTbYKze-0F"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "varimp(cf1) # get variable importance, based on mean decrease in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot_dQRpboO6e"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"earth\")\n",
    "library(earth)\n",
    "\n",
    "marsModel <- earth(x$OFFER_STATUS ~ ., data=x) # build model\n",
    "ev <- evimp (marsModel) # estimate variable importance\n",
    "\n",
    "plot(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gi9UHXa_pZE6"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"relaimpo\")\n",
    "library(relaimpo)\n",
    "lmMod <- lm(OFFER_STATUS ~ . , data = x)  # fit lm() model\n",
    "relImportance <- calc.relimp(lmMod, type = \"lmg\", rela = TRUE)  # calculate relative importance scaled to 100\n",
    "sort(relImportance$lmg, decreasing=TRUE)  # relative importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1n7WgWfYVmr"
   },
   "outputs": [],
   "source": [
    "\n",
    "names(boruta_output)\n",
    "\n",
    "# Get significant variables including tentatives\n",
    "boruta_signif <-\n",
    "  getSelectedAttributes(boruta_output, withTentative = TRUE)\n",
    "print(boruta_signif)\n",
    "\n",
    "# Do a tentative rough fix\n",
    "roughFixMod <- TentativeRoughFix(boruta_output)\n",
    "boruta_signif <- getSelectedAttributes(roughFixMod)\n",
    "print(boruta_signif)\n",
    "\n",
    "# Variable Importance Scores\n",
    "imps <- attStats(roughFixMod)\n",
    "imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]\n",
    "head(imps2[order(-imps2$meanImp),])  # descending sort\n",
    "\n",
    "x = summary(boruta_output$ImpHistory)\n",
    "# Plot variable importance\n",
    "plot(\n",
    "  boruta_output,\n",
    "  cex.axis = .7,\n",
    "  las = 2,\n",
    "  xlab = \"\",\n",
    "  main = \"Variable Importance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0iqq88ByJkxt"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "print(\"[INFO] Number of NA values in each column:\")\n",
    "for (i in 1:ncol(df)) {\n",
    "  print(paste0(names(df)[i], \": \", sum(is.na(df[, i])), \"/\", nrow(df)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdfOLo3kabdu"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# fill in missing values\n",
    "customers = customers %>% mutate(REV_CURRENT_YEAR.1 = ifelse(REV_CURRENT_YEAR.1 == 0,REV_CURRENT_YEAR.2, REV_CURRENT_YEAR.1))\n",
    "customers = customers %>% mutate(REV_CURRENT_YEAR = ifelse(REV_CURRENT_YEAR == 0,REV_CURRENT_YEAR.2, REV_CURRENT_YEAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s489vmTWc1xm"
   },
   "source": [
    "## Li - Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1I7UglBc_jo"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "## check NA values in the labeled_data\n",
    "#print(\"[INFO] Number of NA values in each column:\")\n",
    "#for (i in 1:ncol(labeled_data)) { # for-loop over columns\n",
    "#  print(paste0(names(labeled_data)[i], \": \", sum(is.na(labeled_data[, i])), \"/\", nrow(labeled_data)))\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xvm8oF40t0Ig"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#remove REV_CURRENT_YEAR column as it's the same as REV_CURRENT_YEAR.1\n",
    "all_merged = subset(all_merged, select = -c(REV_CURRENT_YEAR))\n",
    "#remove MO_CREATED_DATE column\n",
    "all_merged = subset(all_merged, select = -c(MO_CREATED_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVWDm8ubJfu1"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#nice function from stackoverflow \n",
    "one_hot_encoding = function(df, columns=\"PRICE_LIST\"){\n",
    "  # create a copy of the original data.frame for not modifying the original\n",
    "  df = cbind(df)\n",
    "  # convert the columns to vector in case it is a string\n",
    "  columns = c(columns)\n",
    "  # for each variable perform the One hot encoding\n",
    "  for (column in columns){\n",
    "    unique_values = sort(unique(df[column])[,column])\n",
    "    non_reference_values  = unique_values[c(-1)] # the first element is going \n",
    "                                                 # to be the reference by default\n",
    "    for (value in non_reference_values){\n",
    "      # the new dummy column name\n",
    "      new_col_name = paste0(column,'_',value)\n",
    "      # create new dummy column for each value of the non_reference_values\n",
    "      df[new_col_name] <- with(df, ifelse(df[,column] == value, 1, 0))\n",
    "    }\n",
    "    # delete the one hot encoded column\n",
    "    df[column] = NULL\n",
    "\n",
    "  }\n",
    "  return(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zUYgjAEJlZk"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#one_hot_encoding column PRICE_LIST, TECH, BUSINESS_TYPE, OFFER_TYPE\n",
    "all_merged = one_hot_encoding(all_merged, c(\"PRICE_LIST\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"TECH\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"BUSINESS_TYPE\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"OFFER_TYPE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMgzSR5FJ4UN"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "#Convert COUNTRY_CODE to binary\n",
    "all_merged = all_merged %>%\n",
    "  mutate(\n",
    "    COUNTRY_CODE = case_when(\n",
    "      COUNTRY_CODE %in% c(\"CH\") ~ 0,\n",
    "      COUNTRY_CODE %in% c(\"FR\") ~ 1,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLM6OH4HJ-q7"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "glimpse(all_merged)\n",
    "write_csv(x = all_merged, \"interim_data/all_merged_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTlddBKIvCS7"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#install.packages(\"Hmisc\")\n",
    "#library(Hmisc)\n",
    "#complete REV_CURRENT_YEAR.1 and REV_CURRENT_YEAR.2 by mean using impute method\n",
    "#labeled_data$REV_CURRENT_YEAR.1 = impute(labeled_data$REV_CURRENT_YEAR.1, mean)\n",
    "#labeled_data$REV_CURRENT_YEAR.2 = impute(labeled_data$REV_CURRENT_YEAR.2, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FBsSGOLv2-G"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#print(\"[INFO] Number of NA values in each column:\")\n",
    "#for (i in 1:ncol(labeled_data)) { # for-loop over columns\n",
    "#  print(paste0(names(labeled_data)[i], \": \", sum(is.na(labeled_data[, i])), \"/\", nrow(labeled_data)))\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nN_4XsPJ4Fiy"
   },
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0F2DyTrc2AU"
   },
   "source": [
    "## Saqib - Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3Lp4TS6jZeB"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "all_merged = one_hot_encoding(all_merged, c(\"SALES_OFFICE\"))\n",
    "all_merged = one_hot_encoding(all_merged, c(\"SALES_BRANCH\"))\n",
    "\n",
    "glimpse(all_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXeH5TXvc2Nf"
   },
   "source": [
    "## Teofil - Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGjCR3gxYNiy"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "old_all_merged = all_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZB0DVG4cJj0"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "customers = cst_df\n",
    "\n",
    "\n",
    "cst_df = cst_df %>% mutate(REV_CURRENT_YEAR.1 = ifelse(REV_CURRENT_YEAR.1 == 0,REV_CURRENT_YEAR.2, REV_CURRENT_YEAR.1))\n",
    "cst_df = cst_df %>% mutate(PREV_YEAR_PERCENTAGE_INCREASE.1 = ((REV_CURRENT_YEAR.1 - REV_CURRENT_YEAR.2)/REV_CURRENT_YEAR.2)*100)\n",
    "\n",
    "\n",
    "################# REPLACEMENT BLOC FOR FINAL MERGE ########################\n",
    "cst_df = cst_df %>% mutate(COUNTRY_CODE = case_when(\n",
    "  COUNTRY == 'Switzerland' ~ \"CH\",\n",
    "  COUNTRY == 'France' ~ \"FR\"\n",
    "))\n",
    "# Transform customer to integer\n",
    "trnsc_geo_df$CUSTOMER <- as.numeric(trnsc_geo_df$CUSTOMER)\n",
    "all_merged = left_join(trnsc_geo_df, cst_df, by = c(\"CUSTOMER\", \"COUNTRY_CODE\"))\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "########## SPLIT LABELED DATA INTO TRAIN AND VALIDATION BASED ON UNIQUE CUSTOMERS ##############\n",
    "unique_customers = unique(labeled_data$CUSTOMER)\n",
    "train_ids = sample(unique_customers, size= floor(0.8 * length(unique_customers)), replace=FALSE)\n",
    "\n",
    "train_set = labeled_data %>% filter(CUSTOMER %in% train_ids)\n",
    "validation_set = labeled_data %>%  filter(!CUSTOMER %in% train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vRtDs2li9zG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install missingno > /dev/null\n",
    "!pip install category_encoders > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVTFW6Wy5wBD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iceking/Desktop/TUM Lecture Docs/3. Business Analytics and Machine Learning (IN2028)/Analytics Cup/Project Files/venv/lib/python3.8/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n",
      "/home/iceking/Desktop/TUM Lecture Docs/3. Business Analytics and Machine Learning (IN2028)/Analytics Cup/Project Files/venv/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"#Import Libraries\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport matplotlib\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport matplotlib.ticker as ticker\\nimport missingno as msno\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import base\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\\nfrom category_encoders import BinaryEncoder\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\\nfrom IPython.display import Image\\nimport warnings\\nimport pandas as pd\\nimport numpy as np\\nimport collections as c\\nimport sklearn\\nimport os\\n\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nimport category_encoders as ce\\nfrom sklearn import preprocessing\\n\\n# use feature importance for feature selection\\nfrom numpy import loadtxt\\nfrom numpy import sort\\nfrom xgboost import XGBClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import balanced_accuracy_score\\nfrom sklearn.feature_selection import SelectFromModel\\n\\n# example of auto-sklearn for a classification dataset\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom autosklearn.classification import AutoSklearnClassifier\\nimport autosklearn\\n\\nimport copy\\nimport time\\nimport pickle\\nimport itertools\\nfrom typing import List\\nimport datetime\\n\\n%load_ext nb_black\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.max_columns\\\", 1000)  # or None\\npd.set_option(\\\"display.max_rows\\\", 1000)  # or None\\npd.set_option(\\\"display.max_colwidth\\\", -1)  # or -1\";\n",
       "                var nbb_formatted_code = \"# Import Libraries\\nimport pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport matplotlib\\n\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport matplotlib.ticker as ticker\\nimport missingno as msno\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import base\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\\nfrom category_encoders import BinaryEncoder\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import (\\n    accuracy_score,\\n    roc_auc_score,\\n    classification_report,\\n    confusion_matrix,\\n)\\nfrom IPython.display import Image\\nimport warnings\\nimport pandas as pd\\nimport numpy as np\\nimport collections as c\\nimport sklearn\\nimport os\\n\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nimport category_encoders as ce\\nfrom sklearn import preprocessing\\n\\n# use feature importance for feature selection\\nfrom numpy import loadtxt\\nfrom numpy import sort\\nfrom xgboost import XGBClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import balanced_accuracy_score\\nfrom sklearn.feature_selection import SelectFromModel\\n\\n# example of auto-sklearn for a classification dataset\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom autosklearn.classification import AutoSklearnClassifier\\nimport autosklearn\\n\\nimport copy\\nimport time\\nimport pickle\\nimport itertools\\nfrom typing import List\\nimport datetime\\n\\n%load_ext nb_black\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.max_columns\\\", 1000)  # or None\\npd.set_option(\\\"display.max_rows\\\", 1000)  # or None\\npd.set_option(\\\"display.max_colwidth\\\", -1)  # or -1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import base\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,classification_report,confusion_matrix\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections as c\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# use feature importance for feature selection\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# example of auto-sklearn for a classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "import autosklearn\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "from typing import List\n",
    "import datetime\n",
    "\n",
    "%load_ext nb_black\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 1000)  # or None\n",
    "pd.set_option(\"display.max_rows\", 1000)  # or None\n",
    "pd.set_option(\"display.max_colwidth\", -1)  # or -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def type_separator(df: pd.DataFrame, print_results=False):\\n    dtype_names = [\\\"categorical\\\", \\\"binary\\\", \\\"continuous\\\", \\\"integer\\\", \\\"numerical\\\"]\\n    type_to_cols = dict.fromkeys(dtype_names, [])\\n\\n    dtype_char_to_names = {\\n        \\\"O\\\": \\\"categorical\\\",\\n        \\\"i\\\": \\\"integer\\\",\\n        \\\"u\\\": \\\"integer\\\",\\n        \\\"f\\\": \\\"continuous\\\",\\n    }\\n\\n    for col in df.columns:\\n        unique_vals = set(df[col].dropna().unique())\\n\\n        if len(unique_vals) < 2 or (\\n            df[col].dtype == np.dtype(\\\"object\\\") and len(unique_vals) > 500\\n        ):\\n            raise ValueError(\\n                f\\\"[ERROR] Something wrong with column:{col} cannot be this case, check conversions!\\\"\\n            )\\n\\n        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\\n            type_to_cols[\\\"binary\\\"] = type_to_cols[\\\"binary\\\"] + [col]\\n        elif df[col].dtype.str[1] in dtype_char_to_names:\\n            dtype_char = df[col].dtype.str[1]\\n            dtype_name = dtype_char_to_names[dtype_char]\\n            type_to_cols[dtype_name] = type_to_cols[dtype_name] + [col]\\n        else:\\n            raise ValueError(\\n                f'[ERROR] Numpy data type:\\\"{df[col].dtype}\\\" of col:\\\"{col}\\\" not understood.'\\n            )\\n\\n    type_to_cols[\\\"numerical\\\"] = type_to_cols[\\\"continuous\\\"] + type_to_cols[\\\"integer\\\"]\\n    type_to_cols[\\\"nominal\\\"] = type_to_cols[\\\"categorical\\\"] + type_to_cols[\\\"binary\\\"]\\n    for dtype_name, col_names in type_to_cols.items():\\n        assert len(type_to_cols[dtype_name]) == len(\\n            set(type_to_cols[dtype_name])\\n        ), f'For type:\\\"{dtype_name}\\\", some columns are duplicate in: {col_names}.'\\n\\n    if print_results:\\n        for key, val in type_to_cols.items():\\n            print(\\\"type:\\\", key, \\\"columns:\\\")\\n            for col in sorted(val):\\n                print(\\\">\\\", col)\\n            print(\\\"-\\\" * 32)\\n    return type_to_cols\\n\\n\\ndef get_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col].isna())]\\n\\n\\ndef get_non_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if not np.any(df[col].isna())]\\n\\n\\ndef get_inf_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col] == np.inf)]\\n\\n\\ndef print_nan_and_inf_columns(df: pd.DataFrame):\\n    print(\\\"NaN and Infinity Columns and Counts:\\\")\\n    for col in sorted(get_null_columns(df)):\\n        print(\\\"> [NaN     ]\\\", col, \\\"Null Count:\\\", np.sum(df[col].isna()))\\n    for col in sorted(get_inf_columns(df)):\\n        print(\\\"> [INFINITY]\\\", col, \\\"Inf Count:\\\", np.sum(df[col] == np.inf))\\n\\n\\ndef get_labeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[~np.isnan(df[target_col])]\\n\\n\\ndef get_unlabeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[np.isnan(df[target_col])]\\n\\n\\ndef MissingUniqueStatistics(df, show_unique_values=False):\\n\\n    total_entry_list = []\\n    total_missing_value_list = []\\n    missing_value_ratio_list = []\\n    data_type_list = []\\n    unique_values_list = []\\n    number_of_unique_values_list = []\\n    variable_name_list = []\\n\\n    for col in df.columns:\\n\\n        variable_name_list.append(col)\\n        missing_value_ratio = round((df[col].isna().sum() / len(df[col])), 4)\\n        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\\n        total_missing_value_list.append(df[col].isna().sum())\\n        missing_value_ratio_list.append(missing_value_ratio)\\n        data_type_list.append(df[col].dtype)\\n        unique_values_list.append(list(df[col].unique()))\\n        number_of_unique_values_list.append(len(df[col].unique()))\\n\\n    data_info_df = pd.DataFrame(\\n        {\\n            \\\"Variable\\\": variable_name_list,\\n            \\\"#_Total_Entry\\\": total_entry_list,\\n            \\\"#_Missing_Value\\\": total_missing_value_list,\\n            \\\"%_Missing_Value\\\": missing_value_ratio_list,\\n            \\\"Data_Type\\\": data_type_list,\\n            \\\"Unique_Values\\\": unique_values_list,\\n            \\\"#_Uniques_Values\\\": number_of_unique_values_list,\\n        }\\n    )\\n    if not show_unique_values:\\n        data_info_df = data_info_df.drop(\\\"Unique_Values\\\", axis=1)\\n\\n    return data_info_df.sort_values(by=\\\"#_Missing_Value\\\", ascending=False).set_index(\\n        \\\"Variable\\\"\\n    )\\n\\n\\ndef histogram(df, feature):  # Histogram of the target categories\\n    %matplotlib inline\\n    ncount = len(df)\\n    ax = sns.countplot(x=feature, data=df, palette=\\\"hls\\\")\\n    sns.set(font_scale=1)\\n    ax.set_xlabel(\\\"Target Segments\\\")\\n    plt.xticks(rotation=90)\\n    ax.set_ylabel(\\\"Number of Observations\\\")\\n    fig = plt.gcf()\\n    fig.set_size_inches(12, 5)\\n    # Make twin axis\\n    ax2 = ax.twinx()\\n    # Switch so count axis is on right, frequency on left\\n    ax2.yaxis.tick_left()\\n    ax.yaxis.tick_right()\\n    # Also switch the labels over\\n    ax.yaxis.set_label_position(\\\"right\\\")\\n    ax2.yaxis.set_label_position(\\\"left\\\")\\n    ax2.set_ylabel(\\\"Frequency [%]\\\")\\n    for p in ax.patches:\\n        x = p.get_bbox().get_points()[:, 0]\\n        y = p.get_bbox().get_points()[1, 1]\\n        ax.annotate(\\n            \\\"{:.2f}%\\\".format(100.0 * y / ncount),\\n            (x.mean(), y),\\n            ha=\\\"center\\\",\\n            va=\\\"bottom\\\",\\n        )  # set the alignment of the text\\n    # Use a LinearLocator to ensure the correct number of ticks\\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\\n    # Fix the frequency range to 0-100\\n    ax2.set_ylim(0, 100)\\n    ax.set_ylim(0, ncount)\\n    # And use a MultipleLocator to ensure a tick spacing of 10\\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\\n    ax2.grid(None)\\n    plt.title(\\\"Histogram of Binary Target Categories\\\", fontsize=20, y=1.08)\\n    plt.show()\\n    plt.savefig(\\\"target_histogram.png\\\")\\n    del ncount, x, y\\n\\n    # USAGE: histogram(data, \\\"CLASS\\\")\";\n",
       "                var nbb_formatted_code = \"def type_separator(df: pd.DataFrame, print_results=False):\\n    dtype_names = [\\\"categorical\\\", \\\"binary\\\", \\\"continuous\\\", \\\"integer\\\", \\\"numerical\\\"]\\n    type_to_cols = dict.fromkeys(dtype_names, [])\\n\\n    dtype_char_to_names = {\\n        \\\"O\\\": \\\"categorical\\\",\\n        \\\"i\\\": \\\"integer\\\",\\n        \\\"u\\\": \\\"integer\\\",\\n        \\\"f\\\": \\\"continuous\\\",\\n    }\\n\\n    for col in df.columns:\\n        unique_vals = set(df[col].dropna().unique())\\n\\n        if len(unique_vals) < 2 or (\\n            df[col].dtype == np.dtype(\\\"object\\\") and len(unique_vals) > 500\\n        ):\\n            raise ValueError(\\n                f\\\"[ERROR] Something wrong with column:{col} cannot be this case, check conversions!\\\"\\n            )\\n\\n        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\\n            type_to_cols[\\\"binary\\\"] = type_to_cols[\\\"binary\\\"] + [col]\\n        elif df[col].dtype.str[1] in dtype_char_to_names:\\n            dtype_char = df[col].dtype.str[1]\\n            dtype_name = dtype_char_to_names[dtype_char]\\n            type_to_cols[dtype_name] = type_to_cols[dtype_name] + [col]\\n        else:\\n            raise ValueError(\\n                f'[ERROR] Numpy data type:\\\"{df[col].dtype}\\\" of col:\\\"{col}\\\" not understood.'\\n            )\\n\\n    type_to_cols[\\\"numerical\\\"] = type_to_cols[\\\"continuous\\\"] + type_to_cols[\\\"integer\\\"]\\n    type_to_cols[\\\"nominal\\\"] = type_to_cols[\\\"categorical\\\"] + type_to_cols[\\\"binary\\\"]\\n    for dtype_name, col_names in type_to_cols.items():\\n        assert len(type_to_cols[dtype_name]) == len(\\n            set(type_to_cols[dtype_name])\\n        ), f'For type:\\\"{dtype_name}\\\", some columns are duplicate in: {col_names}.'\\n\\n    if print_results:\\n        for key, val in type_to_cols.items():\\n            print(\\\"type:\\\", key, \\\"columns:\\\")\\n            for col in sorted(val):\\n                print(\\\">\\\", col)\\n            print(\\\"-\\\" * 32)\\n    return type_to_cols\\n\\n\\ndef get_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col].isna())]\\n\\n\\ndef get_non_null_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if not np.any(df[col].isna())]\\n\\n\\ndef get_inf_columns(df: pd.DataFrame) -> List:\\n    return [col for col in df.columns if np.any(df[col] == np.inf)]\\n\\n\\ndef print_nan_and_inf_columns(df: pd.DataFrame):\\n    print(\\\"NaN and Infinity Columns and Counts:\\\")\\n    for col in sorted(get_null_columns(df)):\\n        print(\\\"> [NaN     ]\\\", col, \\\"Null Count:\\\", np.sum(df[col].isna()))\\n    for col in sorted(get_inf_columns(df)):\\n        print(\\\"> [INFINITY]\\\", col, \\\"Inf Count:\\\", np.sum(df[col] == np.inf))\\n\\n\\ndef get_labeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[~np.isnan(df[target_col])]\\n\\n\\ndef get_unlabeled_set(df: pd.DataFrame, target_col: str = \\\"OFFER_STATUS\\\"):\\n    return df[np.isnan(df[target_col])]\\n\\n\\ndef MissingUniqueStatistics(df, show_unique_values=False):\\n\\n    total_entry_list = []\\n    total_missing_value_list = []\\n    missing_value_ratio_list = []\\n    data_type_list = []\\n    unique_values_list = []\\n    number_of_unique_values_list = []\\n    variable_name_list = []\\n\\n    for col in df.columns:\\n\\n        variable_name_list.append(col)\\n        missing_value_ratio = round((df[col].isna().sum() / len(df[col])), 4)\\n        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\\n        total_missing_value_list.append(df[col].isna().sum())\\n        missing_value_ratio_list.append(missing_value_ratio)\\n        data_type_list.append(df[col].dtype)\\n        unique_values_list.append(list(df[col].unique()))\\n        number_of_unique_values_list.append(len(df[col].unique()))\\n\\n    data_info_df = pd.DataFrame(\\n        {\\n            \\\"Variable\\\": variable_name_list,\\n            \\\"#_Total_Entry\\\": total_entry_list,\\n            \\\"#_Missing_Value\\\": total_missing_value_list,\\n            \\\"%_Missing_Value\\\": missing_value_ratio_list,\\n            \\\"Data_Type\\\": data_type_list,\\n            \\\"Unique_Values\\\": unique_values_list,\\n            \\\"#_Uniques_Values\\\": number_of_unique_values_list,\\n        }\\n    )\\n    if not show_unique_values:\\n        data_info_df = data_info_df.drop(\\\"Unique_Values\\\", axis=1)\\n\\n    return data_info_df.sort_values(by=\\\"#_Missing_Value\\\", ascending=False).set_index(\\n        \\\"Variable\\\"\\n    )\\n\\n\\ndef histogram(df, feature):  # Histogram of the target categories\\n    %matplotlib inline\\n    ncount = len(df)\\n    ax = sns.countplot(x=feature, data=df, palette=\\\"hls\\\")\\n    sns.set(font_scale=1)\\n    ax.set_xlabel(\\\"Target Segments\\\")\\n    plt.xticks(rotation=90)\\n    ax.set_ylabel(\\\"Number of Observations\\\")\\n    fig = plt.gcf()\\n    fig.set_size_inches(12, 5)\\n    # Make twin axis\\n    ax2 = ax.twinx()\\n    # Switch so count axis is on right, frequency on left\\n    ax2.yaxis.tick_left()\\n    ax.yaxis.tick_right()\\n    # Also switch the labels over\\n    ax.yaxis.set_label_position(\\\"right\\\")\\n    ax2.yaxis.set_label_position(\\\"left\\\")\\n    ax2.set_ylabel(\\\"Frequency [%]\\\")\\n    for p in ax.patches:\\n        x = p.get_bbox().get_points()[:, 0]\\n        y = p.get_bbox().get_points()[1, 1]\\n        ax.annotate(\\n            \\\"{:.2f}%\\\".format(100.0 * y / ncount),\\n            (x.mean(), y),\\n            ha=\\\"center\\\",\\n            va=\\\"bottom\\\",\\n        )  # set the alignment of the text\\n    # Use a LinearLocator to ensure the correct number of ticks\\n    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\\n    # Fix the frequency range to 0-100\\n    ax2.set_ylim(0, 100)\\n    ax.set_ylim(0, ncount)\\n    # And use a MultipleLocator to ensure a tick spacing of 10\\n    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\\n    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\\n    ax2.grid(None)\\n    plt.title(\\\"Histogram of Binary Target Categories\\\", fontsize=20, y=1.08)\\n    plt.show()\\n    plt.savefig(\\\"target_histogram.png\\\")\\n    del ncount, x, y\\n\\n    # USAGE: histogram(data, \\\"CLASS\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def type_separator(df: pd.DataFrame, print_results=False):\n",
    "    dtype_names = [\"categorical\", \"binary\", \"continuous\", \"integer\", \"numerical\"]\n",
    "    type_to_cols = dict.fromkeys(dtype_names, [])\n",
    "\n",
    "    dtype_char_to_names = {\n",
    "        \"O\": \"categorical\",\n",
    "        \"i\": \"integer\",\n",
    "        \"u\": \"integer\",\n",
    "        \"f\": \"continuous\",\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        unique_vals = set(df[col].dropna().unique())\n",
    "\n",
    "        if len(unique_vals) < 2 or (\n",
    "            df[col].dtype == np.dtype(\"object\") and len(unique_vals) > 500\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"[ERROR] Something wrong with column:{col} cannot be this case, check conversions!\"\n",
    "            )\n",
    "\n",
    "        if unique_vals == {0, 1} or unique_vals == {0.0, 1.0}:\n",
    "            type_to_cols[\"binary\"] = type_to_cols[\"binary\"] + [col]\n",
    "        elif df[col].dtype.str[1] in dtype_char_to_names:\n",
    "            dtype_char = df[col].dtype.str[1]\n",
    "            dtype_name = dtype_char_to_names[dtype_char]\n",
    "            type_to_cols[dtype_name] = type_to_cols[dtype_name] + [col]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'[ERROR] Numpy data type:\"{df[col].dtype}\" of col:\"{col}\" not understood.'\n",
    "            )\n",
    "\n",
    "    type_to_cols[\"numerical\"] = type_to_cols[\"continuous\"] + type_to_cols[\"integer\"]\n",
    "    type_to_cols[\"nominal\"] = type_to_cols[\"categorical\"] + type_to_cols[\"binary\"]\n",
    "    for dtype_name, col_names in type_to_cols.items():\n",
    "        assert len(type_to_cols[dtype_name]) == len(\n",
    "            set(type_to_cols[dtype_name])\n",
    "        ), f'For type:\"{dtype_name}\", some columns are duplicate in: {col_names}.'\n",
    "\n",
    "    if print_results:\n",
    "        for key, val in type_to_cols.items():\n",
    "            print(\"type:\", key, \"columns:\")\n",
    "            for col in sorted(val):\n",
    "                print(\">\", col)\n",
    "            print(\"-\" * 32)\n",
    "    return type_to_cols\n",
    "\n",
    "\n",
    "def get_null_columns(df: pd.DataFrame) -> List:\n",
    "    return [col for col in df.columns if np.any(df[col].isna())]\n",
    "\n",
    "\n",
    "def get_non_null_columns(df: pd.DataFrame) -> List:\n",
    "    return [col for col in df.columns if not np.any(df[col].isna())]\n",
    "\n",
    "\n",
    "def get_inf_columns(df: pd.DataFrame) -> List:\n",
    "    return [col for col in df.columns if np.any(df[col] == np.inf)]\n",
    "\n",
    "\n",
    "def print_nan_and_inf_columns(df: pd.DataFrame):\n",
    "    print(\"NaN and Infinity Columns and Counts:\")\n",
    "    for col in sorted(get_null_columns(df)):\n",
    "        print(\"> [NaN     ]\", col, \"Null Count:\", np.sum(df[col].isna()))\n",
    "    for col in sorted(get_inf_columns(df)):\n",
    "        print(\"> [INFINITY]\", col, \"Inf Count:\", np.sum(df[col] == np.inf))\n",
    "\n",
    "\n",
    "def get_labeled_set(df: pd.DataFrame, target_col: str = \"OFFER_STATUS\"):\n",
    "    return df[~np.isnan(df[target_col])]\n",
    "\n",
    "\n",
    "def get_unlabeled_set(df: pd.DataFrame, target_col: str = \"OFFER_STATUS\"):\n",
    "    return df[np.isnan(df[target_col])]\n",
    "\n",
    "\n",
    "def MissingUniqueStatistics(df, show_unique_values=False):\n",
    "\n",
    "    total_entry_list = []\n",
    "    total_missing_value_list = []\n",
    "    missing_value_ratio_list = []\n",
    "    data_type_list = []\n",
    "    unique_values_list = []\n",
    "    number_of_unique_values_list = []\n",
    "    variable_name_list = []\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        variable_name_list.append(col)\n",
    "        missing_value_ratio = round((df[col].isna().sum() / len(df[col])), 4)\n",
    "        total_entry_list.append(df[col].shape[0] - df[col].isna().sum())\n",
    "        total_missing_value_list.append(df[col].isna().sum())\n",
    "        missing_value_ratio_list.append(missing_value_ratio)\n",
    "        data_type_list.append(df[col].dtype)\n",
    "        unique_values_list.append(list(df[col].unique()))\n",
    "        number_of_unique_values_list.append(len(df[col].unique()))\n",
    "\n",
    "    data_info_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Variable\": variable_name_list,\n",
    "            \"#_Total_Entry\": total_entry_list,\n",
    "            \"#_Missing_Value\": total_missing_value_list,\n",
    "            \"%_Missing_Value\": missing_value_ratio_list,\n",
    "            \"Data_Type\": data_type_list,\n",
    "            \"Unique_Values\": unique_values_list,\n",
    "            \"#_Uniques_Values\": number_of_unique_values_list,\n",
    "        }\n",
    "    )\n",
    "    if not show_unique_values:\n",
    "        data_info_df = data_info_df.drop(\"Unique_Values\", axis=1)\n",
    "\n",
    "    return data_info_df.sort_values(by=\"#_Missing_Value\", ascending=False).set_index(\n",
    "        \"Variable\"\n",
    "    )\n",
    "\n",
    "\n",
    "def histogram(df, feature):  # Histogram of the target categories\n",
    "    %matplotlib inline\n",
    "    ncount = len(df)\n",
    "    ax = sns.countplot(x=feature, data=df, palette=\"hls\")\n",
    "    sns.set(font_scale=1)\n",
    "    ax.set_xlabel(\"Target Segments\")\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel(\"Number of Observations\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 5)\n",
    "    # Make twin axis\n",
    "    ax2 = ax.twinx()\n",
    "    # Switch so count axis is on right, frequency on left\n",
    "    ax2.yaxis.tick_left()\n",
    "    ax.yaxis.tick_right()\n",
    "    # Also switch the labels over\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax2.yaxis.set_label_position(\"left\")\n",
    "    ax2.set_ylabel(\"Frequency [%]\")\n",
    "    for p in ax.patches:\n",
    "        x = p.get_bbox().get_points()[:, 0]\n",
    "        y = p.get_bbox().get_points()[1, 1]\n",
    "        ax.annotate(\n",
    "            \"{:.2f}%\".format(100.0 * y / ncount),\n",
    "            (x.mean(), y),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )  # set the alignment of the text\n",
    "    # Use a LinearLocator to ensure the correct number of ticks\n",
    "    ax.yaxis.set_major_locator(ticker.LinearLocator(11))\n",
    "    # Fix the frequency range to 0-100\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax.set_ylim(0, ncount)\n",
    "    # And use a MultipleLocator to ensure a tick spacing of 10\n",
    "    ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    # Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\n",
    "    ax2.grid(None)\n",
    "    plt.title(\"Histogram of Binary Target Categories\", fontsize=20, y=1.08)\n",
    "    plt.show()\n",
    "    plt.savefig(\"target_histogram.png\")\n",
    "    del ncount, x, y\n",
    "\n",
    "    # USAGE: histogram(data, \"CLASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Submission 3, With H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"def apply_h2o(data, max_runtime_secs=10 * 60, max_models=20, balance_classes=True,\\n              models_dir_path = './h2o_models_with_data',training_with_all=False):\\n    timestamp = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H:%M:%S\\\")\\n    X_train, X_test, y_train, y_test = data\\n    \\n    from h2o.automl import H2OAutoML\\n    import h2o\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n    \\n    if not os.path.exists(models_dir_path):\\n        os.makedirs(models_dir_path)\\n        \\n    results_dir_path =f'{models_dir_path}/results_ts_{timestamp}'\\n    os.makedirs(results_dir_path)\\n        \\n    train_path =f\\\"{results_dir_path}/train.csv\\\"\\n    test_path =f\\\"{results_dir_path}/test.csv\\\"\\n\\n    pd.concat([X_train, Y_train], axis=1).to_csv(train_path, index=False, header=True)\\n    pd.concat([X_test, Y_test], axis=1).to_csv(test_path, index=False, header=True)\\n\\n    train = h2o.import_file(train_path)\\n    test = h2o.import_file(test_path)\\n\\n    h2o.init(max_mem_size=\\\"36G\\\")\\n\\n    x = train.columns\\n    train[\\\"OFFER_STATUS\\\"] = train[\\\"OFFER_STATUS\\\"].asfactor()\\n    x.remove(\\\"OFFER_STATUS\\\")\\n    aml = H2OAutoML(\\n        max_models=max_models,\\n        balance_classes=balance_classes,\\n        max_runtime_secs=int(max_runtime_secs),\\n        seed=42,\\n    )\\n    if  training_with_all:\\n        saved_model_path=f'{results_dir_path}/model_withALL'\\n        print(f'[INFO] Model will be saved here:\\\"{saved_model_path}\\\"',)\\n    aml.train(x=x, y=\\\"OFFER_STATUS\\\", training_frame=train)\\n    \\n    print(\\\"[INFO] Timestamp:\\\",timestamp)\\n    print('[INFO] AML Leaderboard',aml.leaderboard)\\n    \\n    model_bac_scores = []\\n    \\n    if  training_with_all:\\n        saved_model_path=f'{results_dir_path}/model_withALL'\\n        h2o.save_model(model=aml.leader, path=saved_model_path, force=True)\\n    else:\\n        for i in range(int(aml.max_models * 1.5)):\\n            current_model = aml.leaderboard[i, 0]\\n            if current_model == \\\"NA\\\":\\n                print(f\\\"[INFO] Found {i} models in total.\\\")\\n                break\\n            current_model = h2o.get_model(current_model)\\n            new_pred = current_model.predict(test)\\n            new_pred = new_pred[0].as_data_frame().values.flatten()\\n            model_bac_score = balanced_accuracy_score(Y_test, new_pred)\\n            print(f'[INFO] Model #{i}, BAC={model_bac_score}.')\\n            model_bac_scores.append(model_bac_score)\\n\\n        index_max_bac_score = model_bac_scores.index(max(model_bac_scores))\\n    \\n        saved_model_path=f'{results_dir_path}/model_{\\\"bac_%.3f\\\" % model_bac_scores[index_max_bac_score]}'\\n        h2o.save_model(model=h2o.get_model(aml.leaderboard[index_max_bac_score, 0]), \\n                   path=saved_model_path, force=True)\\n    \\n    print(\\\"[INFO] RESULTS:\\\")\\n    print(f' > Train data saved to:\\\"{train_path}\\\".')\\n    print(f' > Test data saved to :\\\"{test_path}\\\".')\\n    print(f' > H20 Model saved to :\\\"{saved_model_path}\\\".')\\n\\n    return aml, model_bac_scores\";\n",
       "                var nbb_formatted_code = \"def apply_h2o(\\n    data,\\n    max_runtime_secs=10 * 60,\\n    max_models=20,\\n    balance_classes=True,\\n    models_dir_path=\\\"./h2o_models_with_data\\\",\\n    training_with_all=False,\\n):\\n    timestamp = datetime.datetime.now().strftime(\\\"%Y-%m-%d_%H:%M:%S\\\")\\n    X_train, X_test, y_train, y_test = data\\n\\n    from h2o.automl import H2OAutoML\\n    import h2o\\n\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n\\n    if not os.path.exists(models_dir_path):\\n        os.makedirs(models_dir_path)\\n\\n    results_dir_path = f\\\"{models_dir_path}/results_ts_{timestamp}\\\"\\n    os.makedirs(results_dir_path)\\n\\n    train_path = f\\\"{results_dir_path}/train.csv\\\"\\n    test_path = f\\\"{results_dir_path}/test.csv\\\"\\n\\n    pd.concat([X_train, Y_train], axis=1).to_csv(train_path, index=False, header=True)\\n    pd.concat([X_test, Y_test], axis=1).to_csv(test_path, index=False, header=True)\\n\\n    train = h2o.import_file(train_path)\\n    test = h2o.import_file(test_path)\\n\\n    h2o.init(max_mem_size=\\\"36G\\\")\\n\\n    x = train.columns\\n    train[\\\"OFFER_STATUS\\\"] = train[\\\"OFFER_STATUS\\\"].asfactor()\\n    x.remove(\\\"OFFER_STATUS\\\")\\n    aml = H2OAutoML(\\n        max_models=max_models,\\n        balance_classes=balance_classes,\\n        max_runtime_secs=int(max_runtime_secs),\\n        seed=42,\\n    )\\n    if training_with_all:\\n        saved_model_path = f\\\"{results_dir_path}/model_withALL\\\"\\n        print(\\n            f'[INFO] Model will be saved here:\\\"{saved_model_path}\\\"',\\n        )\\n    aml.train(x=x, y=\\\"OFFER_STATUS\\\", training_frame=train)\\n\\n    print(\\\"[INFO] Timestamp:\\\", timestamp)\\n    print(\\\"[INFO] AML Leaderboard\\\", aml.leaderboard)\\n\\n    model_bac_scores = []\\n\\n    if training_with_all:\\n        saved_model_path = f\\\"{results_dir_path}/model_withALL\\\"\\n        h2o.save_model(model=aml.leader, path=saved_model_path, force=True)\\n    else:\\n        for i in range(int(aml.max_models * 1.5)):\\n            current_model = aml.leaderboard[i, 0]\\n            if current_model == \\\"NA\\\":\\n                print(f\\\"[INFO] Found {i} models in total.\\\")\\n                break\\n            current_model = h2o.get_model(current_model)\\n            new_pred = current_model.predict(test)\\n            new_pred = new_pred[0].as_data_frame().values.flatten()\\n            model_bac_score = balanced_accuracy_score(Y_test, new_pred)\\n            print(f\\\"[INFO] Model #{i}, BAC={model_bac_score}.\\\")\\n            model_bac_scores.append(model_bac_score)\\n\\n        index_max_bac_score = model_bac_scores.index(max(model_bac_scores))\\n\\n        saved_model_path = f'{results_dir_path}/model_{\\\"bac_%.3f\\\" % model_bac_scores[index_max_bac_score]}'\\n        h2o.save_model(\\n            model=h2o.get_model(aml.leaderboard[index_max_bac_score, 0]),\\n            path=saved_model_path,\\n            force=True,\\n        )\\n\\n    print(\\\"[INFO] RESULTS:\\\")\\n    print(f' > Train data saved to:\\\"{train_path}\\\".')\\n    print(f' > Test data saved to :\\\"{test_path}\\\".')\\n    print(f' > H20 Model saved to :\\\"{saved_model_path}\\\".')\\n\\n    return aml, model_bac_scores\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_h2o(data, max_runtime_secs=10 * 60, max_models=20, balance_classes=True,\n",
    "              models_dir_path = './h2o_models_with_data',training_with_all=False):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    from h2o.automl import H2OAutoML\n",
    "    import h2o\n",
    "    h2o.init(max_mem_size=\"32G\")\n",
    "    \n",
    "    if not os.path.exists(models_dir_path):\n",
    "        os.makedirs(models_dir_path)\n",
    "        \n",
    "    results_dir_path =f'{models_dir_path}/results_ts_{timestamp}'\n",
    "    os.makedirs(results_dir_path)\n",
    "        \n",
    "    train_path =f\"{results_dir_path}/train.csv\"\n",
    "    test_path =f\"{results_dir_path}/test.csv\"\n",
    "\n",
    "    pd.concat([X_train, Y_train], axis=1).to_csv(train_path, index=False, header=True)\n",
    "    pd.concat([X_test, Y_test], axis=1).to_csv(test_path, index=False, header=True)\n",
    "\n",
    "    train = h2o.import_file(train_path)\n",
    "    test = h2o.import_file(test_path)\n",
    "\n",
    "    h2o.init(max_mem_size=\"36G\")\n",
    "\n",
    "    x = train.columns\n",
    "    train[\"OFFER_STATUS\"] = train[\"OFFER_STATUS\"].asfactor()\n",
    "    x.remove(\"OFFER_STATUS\")\n",
    "    aml = H2OAutoML(\n",
    "        max_models=max_models,\n",
    "        balance_classes=balance_classes,\n",
    "        max_runtime_secs=int(max_runtime_secs),\n",
    "        seed=42,\n",
    "    )\n",
    "    if  training_with_all:\n",
    "        saved_model_path=f'{results_dir_path}/model_withALL'\n",
    "        print(f'[INFO] Model will be saved here:\"{saved_model_path}\"',)\n",
    "    aml.train(x=x, y=\"OFFER_STATUS\", training_frame=train)\n",
    "    \n",
    "    print(\"[INFO] Timestamp:\",timestamp)\n",
    "    print('[INFO] AML Leaderboard',aml.leaderboard)\n",
    "    \n",
    "    model_bac_scores = []\n",
    "    \n",
    "    if  training_with_all:\n",
    "        saved_model_path=f'{results_dir_path}/model_withALL'\n",
    "        h2o.save_model(model=aml.leader, path=saved_model_path, force=True)\n",
    "    else:\n",
    "        for i in range(int(aml.max_models * 1.5)):\n",
    "            current_model = aml.leaderboard[i, 0]\n",
    "            if current_model == \"NA\":\n",
    "                print(f\"[INFO] Found {i} models in total.\")\n",
    "                break\n",
    "            current_model = h2o.get_model(current_model)\n",
    "            new_pred = current_model.predict(test)\n",
    "            new_pred = new_pred[0].as_data_frame().values.flatten()\n",
    "            model_bac_score = balanced_accuracy_score(Y_test, new_pred)\n",
    "            print(f'[INFO] Model #{i}, BAC={model_bac_score}.')\n",
    "            model_bac_scores.append(model_bac_score)\n",
    "\n",
    "        index_max_bac_score = model_bac_scores.index(max(model_bac_scores))\n",
    "    \n",
    "        saved_model_path=f'{results_dir_path}/model_{\"bac_%.3f\" % model_bac_scores[index_max_bac_score]}'\n",
    "        h2o.save_model(model=h2o.get_model(aml.leaderboard[index_max_bac_score, 0]), \n",
    "                   path=saved_model_path, force=True)\n",
    "    \n",
    "    print(\"[INFO] RESULTS:\")\n",
    "    print(f' > Train data saved to:\"{train_path}\".')\n",
    "    print(f' > Test data saved to :\"{test_path}\".')\n",
    "    print(f' > H20 Model saved to :\"{saved_model_path}\".')\n",
    "\n",
    "    return aml, model_bac_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_block(params:dict, verbose=False, calc_feature_importances=False):\n",
    "    \n",
    "    ####### Get Parameters - Start ################\n",
    "    df = pd.read_csv(params['preprocessed_data_path']) # Read Data\n",
    "    ###############################################\n",
    "    \n",
    "    ####### Block Specific Helper Functions #######\n",
    "    def print_if_verbose(*args, **kwargs):\n",
    "        if verbose:\n",
    "            print(*args, **kwargs)\n",
    "    ###############################################\n",
    "\n",
    "    ###############################################\n",
    "    id_columns = [\"CUSTOMER\", \"TEST_SET_ID\", \"IDX_CUSTOMER\"]\n",
    "    unnecessary_reduced_cols = [\n",
    "        \"OFFER_TYPE_REDUCED_1\",\n",
    "        \"OFFER_TYPE_REDUCED_2\",\n",
    "        \"SALES_OFFICE_REDUCED\",\n",
    "    ]\n",
    "\n",
    "    to_be_dropped_cols = id_columns + unnecessary_reduced_cols\n",
    "    df = df.drop(to_be_dropped_cols, axis=1)\n",
    "\n",
    "    # new columns\n",
    "    df[\"ADDITIONAL_COST\"] = df[\"OFFER_PRICE\"] - df[\"MATERIAL_COST\"] - df[\"SERVICE_COST\"]\n",
    "    df[\"TOTAL_COST\"] = df[\"MATERIAL_COST\"] + df[\"SERVICE_COST\"]\n",
    "\n",
    "    ###############################################\n",
    "\n",
    "    assert 'OFFER_STATUS' not in type_separator(df)[\"categorical\"]\n",
    "    for col in type_separator(df)[\"categorical\"]:\n",
    "        num_unq = len(df[col].unique())\n",
    "        trimmed_col = col.strip().replace(\" \", \"_\")\n",
    "        if num_unq < 5:\n",
    "            print_if_verbose(f\"[INFO] Col:{col},num_of_unq:{num_unq}, applying 1-HOT encoding.\")\n",
    "            onehot_df = pd.get_dummies(df[col])\n",
    "            onehot_df = onehot_df.add_prefix(trimmed_col + \"_1HOTENC_\")\n",
    "            df = pd.concat((df, onehot_df), axis=1)\n",
    "        elif num_unq >= 5:\n",
    "            print_if_verbose(f\"[INFO] Col:{col},num_of_unq:{num_unq}, applying BINARY encoding.\")\n",
    "            encoder = ce.BinaryEncoder(cols=[col])\n",
    "            binenc_df = encoder.fit_transform(df[[col]])\n",
    "            binenc_df.columns = [\n",
    "                f\"{trimmed_col}_BINENC_{i}\" for i in range(len(binenc_df.columns))\n",
    "            ]\n",
    "            df = pd.concat((df, binenc_df), axis=1)\n",
    "\n",
    "    for col in type_separator(df)[\"categorical\"]:\n",
    "        if col != 'OFFER_STATUS':\n",
    "            df[col] = pd.Categorical(df[col])\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    ###### ADDITIONAL < FEATURES - START\n",
    "    add_less_than_features = True\n",
    "    print(\"[WARN] Adding Less Than Features:\", add_less_than_features)\n",
    "\n",
    "    if add_less_than_features:\n",
    "        raw_num_cols = type_separator(df)[\"numerical\"]\n",
    "        nonraw_strings_in_cols = (\"OFFER_STATUS\",\"IS_NA\",\"HAS_\",\"1HOTENC\",\"BINENC\",\"IS_\",\"_LOG\")\n",
    "        raw_num_cols = [a for a in raw_num_cols if not any( x in a for x in nonraw_strings_in_cols)]\n",
    "\n",
    "        raw_numeric_cols_combinations = list(itertools.combinations(raw_num_cols, r=2))\n",
    "        new_less_than_cols = []\n",
    "        for col1, col2 in raw_numeric_cols_combinations:\n",
    "            new_col = np.where(\n",
    "                np.isnan(df[col1]) | np.isnan(df[col2]), np.nan, (df[col1] < df[col2])\n",
    "            )\n",
    "            name_of_new_col = col1 + \"_<_\" + col2\n",
    "\n",
    "            if (len(np.unique(new_col[~np.isnan(new_col)])) > 1):  # Not all values are 1 or 0\n",
    "                new_less_than_cols.append(pd.Series(new_col, name=name_of_new_col))\n",
    "                print_if_verbose(f'[INFO] Added new \"less than\" column: \"{name_of_new_col}\".')\n",
    "            else:\n",
    "                print_if_verbose(f'[INFO] NOT added \"Less Then\" column: \"{name_of_new_col}\".')\n",
    "\n",
    "        df = pd.concat([df, pd.concat(new_less_than_cols, axis=1)], axis=1)\n",
    "\n",
    "    ############# DROPPP -START\n",
    "    drop_cols = [\n",
    "        ############## Correlation are same or very similar\n",
    "        \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "        \"OWNERSHIP_NA_AS_NO_INFO_REDUCED_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "        \"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "        \"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_Privately Owned/Publicly Traded\",\n",
    "        \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Individual Person\",\n",
    "        \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_Governmental\",\n",
    "        \"OWNERSHIP_NA_AS_NO_INFO_1HOTENC_No information\",\n",
    "        \"OWNERSHIP_NO_INFO_AS_NA_REDUCED_1HOTENC_NOT_GIVEN\",\n",
    "        \"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_NOT_GIVEN\",\n",
    "        \"OWNERSHIP_REDUCED_1HOTENC_NOT_GIVEN\",\n",
    "        \"TECH_REDUCED_2_IS_F\",\n",
    "        \"TECH_BINENC_0\",\n",
    "        \"SINCE_CREATION_YEAR_<_REV_PERCENTAGE_INCREASE_NO_OUTLIER\",  # (34, False)\n",
    "        ############## Experimentally\n",
    "        \"TOTAL_COSTS_PRODUCT_LOG\",\n",
    "        \"CURRENCY_BINENC_0\",  # (19, False)\n",
    "        \"OWNERSHIP_BINENC_0\",  # (20, False)\n",
    "        \"OWNERSHIP_NO_INFO_AS_NA_1HOTENC_Individual Person\",  # (21, False)\n",
    "        \"IS_NA_SALES_LOCATION\",  # (22, False)\n",
    "        \"IS_NA_SALES_OFFICE\",  # (23, False)\n",
    "        ## From XGB Feature Importance\n",
    "        \"SERVICE_COST_<_CREATION_YEAR\",  # 0.0\n",
    "        \"REV_CURRENT_YEAR.1_<_REV_PERCENTAGE_INCREASE\",  # 0.0\n",
    "        \"CREATION_YEAR_<_TOTAL_COST\",  # 0.0\n",
    "        \"CREATION_YEAR_<_REV_PERCENTAGE_INCREASE\",  # 0.0\n",
    "        \"CREATION_YEAR_<_ADDITIONAL_COST\",  # 0.0\n",
    "    ]\n",
    "\n",
    "    for col in drop_cols:\n",
    "        if col in list(df.columns):\n",
    "            print_if_verbose(\"[INFO] Dropped:\", col)\n",
    "            df = df.drop(col, axis=1)\n",
    "\n",
    "    if calc_feature_importances:\n",
    "        def calculate_feature_importances():\n",
    "            from boruta import BorutaPy\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "            df_new = df[~np.isnan(df[\"OFFER_STATUS\"])].copy()\n",
    "            columns_to_be_dropped = get_null_columns(df_new)\n",
    "            if \"categorical\" in type_separator(df_new):\n",
    "                columns_to_be_dropped += [\n",
    "                    col for col in type_separator(df_new)[\"categorical\"]\n",
    "                    if col in df_new.columns\n",
    "                ]\n",
    "            columns_to_be_dropped = set(columns_to_be_dropped)\n",
    "            if \"OFFER_STATUS\" in columns_to_be_dropped:\n",
    "                columns_to_be_dropped.remove(\"OFFER_STATUS\")\n",
    "\n",
    "            df_new = df_new.drop(columns_to_be_dropped, axis=1)\n",
    "            X, y = df_new.drop(\"OFFER_STATUS\", axis=1), df_new[\"OFFER_STATUS\"]\n",
    "\n",
    "            forest = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\", max_depth=5)\n",
    "            forest.fit(X, y)\n",
    "            \n",
    "            feat_selector = BorutaPy( # define Boruta feature selection method\n",
    "                forest, n_estimators=\"auto\", verbose=2,\n",
    "                random_state=42,\n",
    "            )\n",
    "            \n",
    "            feat_selector.fit(X.to_numpy(), y.to_numpy()) # find all relevant features\n",
    "            feature_importances = list(zip(feat_selector.ranking_,feat_selector.support_,X.columns))\n",
    "            for item in sorted(feature_importances):\n",
    "                print(item, \",\")\n",
    "        calculate_feature_importances()\n",
    "\n",
    "    df_for_test = df[np.isnan(df[\"OFFER_STATUS\"])]\n",
    "    df = df[~np.isnan(df[\"OFFER_STATUS\"])]\n",
    "    \n",
    "    X, y = df.drop(\"OFFER_STATUS\", axis=1), df[\"OFFER_STATUS\"] # Col Selection & Conversion\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    if params['classifer'] == 'h2o':\n",
    "        return apply_h2o(data=[X_train, X_test, y_train, y_test],max_runtime_secs=params['max_runtime_secs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = code_block(\n",
    "    params={\n",
    "        \"max_runtime_secs\": 3 * 60,\n",
    "        \"preprocessed_data_path\": \"interim_data/df_completed_1_2_3_with_mv_new.csv\",\n",
    "        # \"preprocessed_data_path\": \"interim_data/df_completed_1_2_3_with_mv.csv\",\n",
    "        \"classifer\": \"h2o\",\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Submission 4, With H2O & AutoSklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# def code_block(params:dict, verbose=False, calc_feature_importances=False):\\n# \\\"max_runtime_secs\\\": 3 * 60,\\nverbose = False\\ntraining_mode = False  # Make this false when predicting the real test set\\nis_data_analysis_and_visualization_part = False\\npreprocessed_data_path = \\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\"\\ntest_size_in_split = 0.33\";\n",
       "                var nbb_formatted_code = \"# def code_block(params:dict, verbose=False, calc_feature_importances=False):\\n# \\\"max_runtime_secs\\\": 3 * 60,\\nverbose = False\\ntraining_mode = False  # Make this false when predicting the real test set\\nis_data_analysis_and_visualization_part = False\\npreprocessed_data_path = \\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\"\\ntest_size_in_split = 0.33\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def code_block(params:dict, verbose=False, calc_feature_importances=False):\n",
    "# \"max_runtime_secs\": 3 * 60,\n",
    "verbose = False\n",
    "training_mode = False  # Make this false when predicting the real test set\n",
    "is_data_analysis_and_visualization_part = False\n",
    "preprocessed_data_path = \"interim_data/df_completed_1_2_3_with_mv_new.csv\"\n",
    "test_size_in_split = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Outlier Detection has begun.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"####### Block Specific Helper Functions #######\\ndef print_if_verbose(*args, **kwargs):\\n    if verbose:\\n        print(*args, **kwargs)\\n\\n\\n###############################################\\n\\n############# Data Postprocessing #############\\ndf = pd.read_csv(preprocessed_data_path).drop(  # Read Data\\n    [\\\"CUSTOMER\\\", \\\"TEST_SET_ID\\\", \\\"IDX_CUSTOMER\\\"], axis=1\\n)\\n\\n# Add some new columns\\ndf[\\\"ADDITIONAL_COST\\\"] = df[\\\"OFFER_PRICE\\\"] - df[\\\"MATERIAL_COST\\\"] - df[\\\"SERVICE_COST\\\"]\\ndf[\\\"TOTAL_COST\\\"] = df[\\\"MATERIAL_COST\\\"] + df[\\\"SERVICE_COST\\\"]\\ndf[\\\"TOTAL_COST_LOG\\\"] = df[\\\"MATERIAL_COST\\\"] + df[\\\"SERVICE_COST\\\"]\\ndf[\\\"TOTAL_COST_RATIO\\\"] = df[\\\"MATERIAL_COST\\\"] / df[\\\"TOTAL_COST\\\"]\\n\\n# HIGH ABSOLUTE CORRELATIONS BETWEEN INDEPENDENT VALUES:\\n# > abs(Corr)=1.00 with: \\\"SO_CREATED_DATE_SCALED\\\"    <->   \\\"SO_CREATED_DATE_INTEGER\\\"\\n# > abs(Corr)=1.00 with: \\\"SINCE_CREATION_YEAR\\\"       <->   \\\"CREATION_YEAR_LOG\\\"\\n# > abs(Corr)=1.00 with: \\\"SINCE_CREATION_YEAR\\\"       <->   \\\"CREATION_YEAR\\\"\\n# > abs(Corr)=1.00 with: \\\"IS_NA_REV_CURRENT_YEAR\\\"    <->   \\\"IS_NA_CURRENCY\\\"\\n# > abs(Corr)=1.00 with: \\\"MATERIAL_COST_LOG\\\"         <->   \\\"SERVICE_LIST_PRICE_LOG\\\"\\n# > abs(Corr)=1.00 with: \\\"TOTAL_COST\\\"                <->   \\\"TOTAL_COST_LOG\\\"\\n# > abs(Corr)=0.99 with: \\\"TOTAL_COST\\\"                <->   \\\"IS_NA_REV_AVG\\\"\\n# > abs(Corr)=0.99 with: \\\"SERVICE_LIST_PRICE\\\"        <->   \\\"MATERIAL_COST\\\"\\n# > abs(Corr)=0.99 with: \\\"REV_AVG\\\"                   <->   \\\"REV_CURRENT_YEAR.2\\\"\\n# > abs(Corr)=0.99 with: \\\"REV_AVG\\\"                   <->   \\\"REV_CURRENT_YEAR.1\\\"\\n# > abs(Corr)=0.99 with: \\\"IS_NA_SALES_OFFICE\\\"        <->   \\\"IS_NA_SALES_LOCATION\\\"\\n# > abs(Corr)=0.99 with: \\\"IS_NA_REV_CURRENT_YEAR\\\"    <->   \\\"IS_NA_OWNERSHIP_NO_INFO_AS_NA\\\"\\n# > abs(Corr)=0.96 with: \\\"TOTAL_COST\\\"                <->   \\\"OFFER_PRICE\\\"\\n# > abs(Corr)=0.96 with: \\\"SINCE_CREATION_YEAR_LOG\\\"   <->   \\\"SINCE_CREATION_YEAR\\\"\\n\\nselected_cols_by_boruta = [\\n    \\\"OFFER_STATUS\\\",  ################ Target Column\\n    \\\"ADDITIONAL_COST\\\",  # , # 1, # True',\\n    \\\"BUSINESS_TYPE\\\",  # , # 1, # True',\\n    \\\"OFFER_TYPE\\\",  # , # 1, # True',\\n    \\\"OFFER_TYPE_REDUCED_1\\\",  # , # 1, # True',\\n    \\\"OFFER_TYPE_REDUCED_2\\\",  # , # 1, # True',\\n    \\\"REV_AVG\\\",  # , # 1, # True',\\n    \\\"REV_CURRENT_YEAR.1\\\",  # , # 1, # True',\\n    \\\"REV_CURRENT_YEAR_LOG.1\\\",  # , # 1, # True',\\n    \\\"REV_CURRENT_YEAR_LOG.2\\\",  # , # 1, # True',\\n    \\\"REV_RATE\\\",  # , # 1, # True',\\n    \\\"SALES_LOCATION\\\",  # , # 1, # True',\\n    \\\"SALES_OFFICE\\\",  # , # 1, # True',\\n    \\\"SALES_OFFICE_REDUCED\\\",  # , # 1, # True',\\n    \\\"SERVICE_COST\\\",  # , # 1, # True',\\n    \\\"SERVICE_COST_LOG\\\",  # , # 1, # True',\\n    \\\"SO_CREATED_DATE_INTEGER\\\",  # , # 1, # True',\\n    # \\\"SO_CREATED_DATE_SCALED\\\",  # , # 1, # True',\\n    # \\\"TOTAL_COST_RATIO\\\",  # , # 1, # True',\\n    \\\"OFFER_PRICE_LOG\\\",  # , # 2, # False',\\n    \\\"OFFER_PRICE\\\",  # , # 3, # False',\\n    \\\"TECH_REDUCED_1\\\",  # , # 4, # False',\\n    # \\\"REV_CURRENT_YEAR.2\\\",  # , # 5, # False',\\n    # \\\"TOTAL_COST\\\",  # , # 5, # False',\\n    \\\"TECH\\\",  # , # 7, # False',\\n    \\\"TOTAL_COST_LOG\\\",  # , # 7, # False',\\n    # \\\"PRICE_LIST\\\",  # , # 9, # False',\\n    \\\"SERVICE_LIST_PRICE\\\",  # , # 9, # False',\\n    \\\"SERVICE_LIST_PRICE_LOG\\\",  # , # 11, # False',\\n    # \\\"MATERIAL_COST_LOG\\\",  # , # 12, # False',\\n    \\\"MATERIAL_COST\\\",  # , # 13, # False',\\n    \\\"CURRENCY\\\",  # , # 14, # False',\\n    # \\\"SINCE_CREATION_YEAR\\\",  # , # 15, # False',\\n    # \\\"CREATION_YEAR\\\",  # , # 16, # False',\\n    # \\\"CREATION_YEAR_LOG\\\",  # , # 16, # False',\\n    # \\\"SINCE_CREATION_YEAR_LOG\\\",  # , # 18, # False',\\n    # 'TECH_REDUCED_2_IS_F', # , # 19, # False',\\n    # 'OWNERSHIP_NA_AS_NO_INFO', # , # 21, # False',\\n    # 'TOTAL_COSTS_PRODUCT', # , # 21, # False',\\n    # 'TOTAL_COSTS_PRODUCT_LOG', # , # 21, # False',\\n    # 'OWNERSHIP_NA_AS_NO_INFO_REDUCED', # , # 23, # False',\\n    # 'OWNERSHIP', # , # 24, # False',\\n    # 'OWNERSHIP_REDUCED', # , # 24, # False',\\n    # 'OWNERSHIP_NO_INFO_AS_NA_REDUCED', # , # 26, # False',\\n    # 'OWNERSHIP_NO_INFO_AS_NA', # , # 27, # False',\\n    # 'IS_COUNTRY_CODE_CH', # , # 28, # False',\\n    # 'HAS_END_CUSTOMER', # , # 29, # False',\\n    # 'IS_NA_REV_RATE', # , # 30, # False',\\n    # 'IS_NA_REV_AVG', # , # 31, # False',\\n    # 'HAS_COSTS_PRODUCT_D', # , # 32, # False',\\n    # 'HAS_COSTS_PRODUCT_B', # , # 33, # False',\\n    # 'IS_NA_REV_CURRENT_YEAR', # , # 33, # False',\\n    # 'IS_NA_CURRENCY', # , # 35, # False',\\n    # 'IS_NA_OWNERSHIP_NO_INFO_AS_NA', # , # 35, # False',\\n    # 'HAS_ISIC', # , # 37, # False',\\n    # 'HAS_COSTS_PRODUCT_A', # , # 38, # False',\\n    # 'HAS_COSTS_PRODUCT_E', # , # 39, # False',\\n    # 'HAS_COSTS_PRODUCT_C', # , # 40, # False',\\n    # 'IS_NA_SALES_OFFICE', # , # 41, # False',\\n    # 'IS_NA_SALES_LOCATION', # , # 42, # False'\\n]\\nassert \\\"OFFER_STATUS\\\" in selected_cols_by_boruta\\ndf = df[selected_cols_by_boruta]\\n###############################################\\n\\n######## Data Analysis and Visualization ######\\nif is_data_analysis_and_visualization_part:\\n\\n    def data_analysis_and_visualization_part():\\n        print(\\\"[INFO] Correlation Matrix:\\\")\\n        f, ax = plt.subplots(figsize=(30, 13))\\n        sns.heatmap(abs(df.corr()), annot=True)\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Values:\\\")\\n        msno.bar(df, color=\\\"#79ccb3\\\", sort=\\\"descending\\\")\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Value Correlations - 1:\\\")\\n        msno.matrix(\\n            df[get_null_columns(df)],\\n            color=(0.45, 0.45, 0.64),\\n            figsize=(27, 10),\\n            width_ratios=(10, 0),\\n        )\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Value Correlations - 2:\\\")\\n        msno.dendrogram(df[get_null_columns(df)])\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Value Correlations - 3:\\\")\\n        msno.heatmap(df[get_null_columns(df)])\\n        plt.show()\\n\\n    data_analysis_and_visualization_part()\\n###############################################\\n\\n######## Outlier Detection #############\\nprint(\\\"[INFO] Outlier Detection has begun.\\\")\\n\\ncol_to_type = [\\n    ######################### Nominal\\n    (\\\"BUSINESS_TYPE\\\", \\\"Nominal\\\"),\\n    (\\\"CURRENCY\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_A\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_B\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_C\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_D\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_E\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_END_CUSTOMER\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_ISIC\\\", \\\"Nominal\\\"),\\n    (\\\"IS_COUNTRY_CODE_CH\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_CURRENCY\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_OWNERSHIP_NO_INFO_AS_NA\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_REV_CURRENT_YEAR\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_REV_AVG\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_REV_RATE\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_SALES_OFFICE\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_SALES_LOCATION\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_STATUS\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_TYPE\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_TYPE_REDUCED_1\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_TYPE_REDUCED_2\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NA_AS_NO_INFO\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NA_AS_NO_INFO_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NO_INFO_AS_NA\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NO_INFO_AS_NA_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"PRICE_LIST\\\", \\\"Nominal\\\"),\\n    (\\\"SALES_LOCATION\\\", \\\"Nominal\\\"),\\n    (\\\"SALES_OFFICE\\\", \\\"Nominal\\\"),\\n    (\\\"SALES_OFFICE_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"TECH\\\", \\\"Nominal\\\"),\\n    (\\\"TECH_REDUCED_1\\\", \\\"Nominal\\\"),\\n    (\\\"TECH_REDUCED_2_IS_F\\\", \\\"Nominal\\\"),\\n    ######################### Continuous\\n    (\\\"CREATION_YEAR\\\", \\\"Continuous\\\"),\\n    (\\\"CREATION_YEAR_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"MATERIAL_COST\\\", \\\"Continuous\\\"),\\n    (\\\"MATERIAL_COST_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"OFFER_PRICE\\\", \\\"Continuous\\\"),\\n    (\\\"OFFER_PRICE_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"REV_AVG\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR.1\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR_LOG.1\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR.2\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR_LOG.2\\\", \\\"Continuous\\\"),\\n    (\\\"REV_RATE\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_COST\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_COST_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_LIST_PRICE\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_LIST_PRICE_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"SINCE_CREATION_YEAR\\\", \\\"Continuous\\\"),\\n    (\\\"SINCE_CREATION_YEAR_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"SO_CREATED_DATE_INTEGER\\\", \\\"Continuous\\\"),\\n    (\\\"SO_CREATED_DATE_SCALED\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COST\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COST_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COST_RATIO\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COSTS_PRODUCT\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COSTS_PRODUCT_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"ADDITIONAL_COST\\\", \\\"Continuous\\\"),\\n]\\ndf_info = MissingUniqueStatistics(df)\\ndf_col_to_type = pd.DataFrame(col_to_type, columns=[\\\"Variable\\\", \\\"Variable_Type\\\"])\\ndf_info = df_info.merge(df_col_to_type, on=\\\"Variable\\\").set_index(\\\"Variable\\\")\\nprint_if_verbose(df_info)\\n\\nnumerical_columns = list(\\n    df_info.loc[\\n        (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Cardinal\\\")\\n        | (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Continuous\\\")\\n    ].index\\n)\\nassert len(numerical_columns) > 0\\n\\ncategorical_columns = list(\\n    df_info.loc[\\n        (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Nominal\\\")\\n        | (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Ordinal\\\")\\n    ].index\\n)\\nassert len(categorical_columns) > 0\\n\\n\\nassert len({x[0] for x in col_to_type}) == len(\\n    [x[0] for x in col_to_type]\\n), \\\"There are some duplicate columns in col_to_type list!\\\"\\nassert not np.any(df_info.isna()), \\\"Some variable types are null!\\\"\\nassert len(df_info.index) == df_info.shape[0], \\\"Some Rows are not unique!\\\"\\n\\ndf_all_processed = df.copy()\\ndf_for_unlabeled_set = df_all_processed[np.isnan(df_all_processed[\\\"OFFER_STATUS\\\"])]\\ndf = df_all_processed[~np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n\\nif training_mode:\\n    x, y = df.drop([\\\"OFFER_STATUS\\\"], axis=1), df[\\\"OFFER_STATUS\\\"]\\n    X_train, X_test, Y_train, Y_test = train_test_split(\\n        x, y, test_size=test_size_in_split, stratify=y, random_state=42\\n    )\\n    if verbose:\\n        histogram(X_train, Y_train)\\n        histogram(X_test, Y_test)\\nelse:\\n    X_train, Y_train = df.drop([\\\"OFFER_STATUS\\\"], axis=1), df[\\\"OFFER_STATUS\\\"]\\n\\n    X_test = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n    Y_test = df_for_unlabeled_set[\\\"OFFER_STATUS\\\"]\\n\\n\\nprint_if_verbose(\\n    f\\\"df_all_processed:{df_all_processed.shape}\\\",\\n    f\\\"df_for_unlabeled_set:{df_for_unlabeled_set.shape}\\\",\\n    f\\\"X_train:{X_train.shape}\\\",\\n    f\\\"Y_train:{Y_train.shape}\\\",\\n    f\\\"X_test:{X_test.shape}\\\",\\n    f\\\"Y_test:{Y_test.shape}\\\",\\n    sep=\\\"\\\\n\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"####### Block Specific Helper Functions #######\\ndef print_if_verbose(*args, **kwargs):\\n    if verbose:\\n        print(*args, **kwargs)\\n\\n\\n###############################################\\n\\n############# Data Postprocessing #############\\ndf = pd.read_csv(preprocessed_data_path).drop(  # Read Data\\n    [\\\"CUSTOMER\\\", \\\"TEST_SET_ID\\\", \\\"IDX_CUSTOMER\\\"], axis=1\\n)\\n\\n# Add some new columns\\ndf[\\\"ADDITIONAL_COST\\\"] = df[\\\"OFFER_PRICE\\\"] - df[\\\"MATERIAL_COST\\\"] - df[\\\"SERVICE_COST\\\"]\\ndf[\\\"TOTAL_COST\\\"] = df[\\\"MATERIAL_COST\\\"] + df[\\\"SERVICE_COST\\\"]\\ndf[\\\"TOTAL_COST_LOG\\\"] = df[\\\"MATERIAL_COST\\\"] + df[\\\"SERVICE_COST\\\"]\\ndf[\\\"TOTAL_COST_RATIO\\\"] = df[\\\"MATERIAL_COST\\\"] / df[\\\"TOTAL_COST\\\"]\\n\\n# HIGH ABSOLUTE CORRELATIONS BETWEEN INDEPENDENT VALUES:\\n# > abs(Corr)=1.00 with: \\\"SO_CREATED_DATE_SCALED\\\"    <->   \\\"SO_CREATED_DATE_INTEGER\\\"\\n# > abs(Corr)=1.00 with: \\\"SINCE_CREATION_YEAR\\\"       <->   \\\"CREATION_YEAR_LOG\\\"\\n# > abs(Corr)=1.00 with: \\\"SINCE_CREATION_YEAR\\\"       <->   \\\"CREATION_YEAR\\\"\\n# > abs(Corr)=1.00 with: \\\"IS_NA_REV_CURRENT_YEAR\\\"    <->   \\\"IS_NA_CURRENCY\\\"\\n# > abs(Corr)=1.00 with: \\\"MATERIAL_COST_LOG\\\"         <->   \\\"SERVICE_LIST_PRICE_LOG\\\"\\n# > abs(Corr)=1.00 with: \\\"TOTAL_COST\\\"                <->   \\\"TOTAL_COST_LOG\\\"\\n# > abs(Corr)=0.99 with: \\\"TOTAL_COST\\\"                <->   \\\"IS_NA_REV_AVG\\\"\\n# > abs(Corr)=0.99 with: \\\"SERVICE_LIST_PRICE\\\"        <->   \\\"MATERIAL_COST\\\"\\n# > abs(Corr)=0.99 with: \\\"REV_AVG\\\"                   <->   \\\"REV_CURRENT_YEAR.2\\\"\\n# > abs(Corr)=0.99 with: \\\"REV_AVG\\\"                   <->   \\\"REV_CURRENT_YEAR.1\\\"\\n# > abs(Corr)=0.99 with: \\\"IS_NA_SALES_OFFICE\\\"        <->   \\\"IS_NA_SALES_LOCATION\\\"\\n# > abs(Corr)=0.99 with: \\\"IS_NA_REV_CURRENT_YEAR\\\"    <->   \\\"IS_NA_OWNERSHIP_NO_INFO_AS_NA\\\"\\n# > abs(Corr)=0.96 with: \\\"TOTAL_COST\\\"                <->   \\\"OFFER_PRICE\\\"\\n# > abs(Corr)=0.96 with: \\\"SINCE_CREATION_YEAR_LOG\\\"   <->   \\\"SINCE_CREATION_YEAR\\\"\\n\\nselected_cols_by_boruta = [\\n    \\\"OFFER_STATUS\\\",  ################ Target Column\\n    \\\"ADDITIONAL_COST\\\",  # , # 1, # True',\\n    \\\"BUSINESS_TYPE\\\",  # , # 1, # True',\\n    \\\"OFFER_TYPE\\\",  # , # 1, # True',\\n    \\\"OFFER_TYPE_REDUCED_1\\\",  # , # 1, # True',\\n    \\\"OFFER_TYPE_REDUCED_2\\\",  # , # 1, # True',\\n    \\\"REV_AVG\\\",  # , # 1, # True',\\n    \\\"REV_CURRENT_YEAR.1\\\",  # , # 1, # True',\\n    \\\"REV_CURRENT_YEAR_LOG.1\\\",  # , # 1, # True',\\n    \\\"REV_CURRENT_YEAR_LOG.2\\\",  # , # 1, # True',\\n    \\\"REV_RATE\\\",  # , # 1, # True',\\n    \\\"SALES_LOCATION\\\",  # , # 1, # True',\\n    \\\"SALES_OFFICE\\\",  # , # 1, # True',\\n    \\\"SALES_OFFICE_REDUCED\\\",  # , # 1, # True',\\n    \\\"SERVICE_COST\\\",  # , # 1, # True',\\n    \\\"SERVICE_COST_LOG\\\",  # , # 1, # True',\\n    \\\"SO_CREATED_DATE_INTEGER\\\",  # , # 1, # True',\\n    # \\\"SO_CREATED_DATE_SCALED\\\",  # , # 1, # True',\\n    # \\\"TOTAL_COST_RATIO\\\",  # , # 1, # True',\\n    \\\"OFFER_PRICE_LOG\\\",  # , # 2, # False',\\n    \\\"OFFER_PRICE\\\",  # , # 3, # False',\\n    \\\"TECH_REDUCED_1\\\",  # , # 4, # False',\\n    # \\\"REV_CURRENT_YEAR.2\\\",  # , # 5, # False',\\n    # \\\"TOTAL_COST\\\",  # , # 5, # False',\\n    \\\"TECH\\\",  # , # 7, # False',\\n    \\\"TOTAL_COST_LOG\\\",  # , # 7, # False',\\n    # \\\"PRICE_LIST\\\",  # , # 9, # False',\\n    \\\"SERVICE_LIST_PRICE\\\",  # , # 9, # False',\\n    \\\"SERVICE_LIST_PRICE_LOG\\\",  # , # 11, # False',\\n    # \\\"MATERIAL_COST_LOG\\\",  # , # 12, # False',\\n    \\\"MATERIAL_COST\\\",  # , # 13, # False',\\n    \\\"CURRENCY\\\",  # , # 14, # False',\\n    # \\\"SINCE_CREATION_YEAR\\\",  # , # 15, # False',\\n    # \\\"CREATION_YEAR\\\",  # , # 16, # False',\\n    # \\\"CREATION_YEAR_LOG\\\",  # , # 16, # False',\\n    # \\\"SINCE_CREATION_YEAR_LOG\\\",  # , # 18, # False',\\n    # 'TECH_REDUCED_2_IS_F', # , # 19, # False',\\n    # 'OWNERSHIP_NA_AS_NO_INFO', # , # 21, # False',\\n    # 'TOTAL_COSTS_PRODUCT', # , # 21, # False',\\n    # 'TOTAL_COSTS_PRODUCT_LOG', # , # 21, # False',\\n    # 'OWNERSHIP_NA_AS_NO_INFO_REDUCED', # , # 23, # False',\\n    # 'OWNERSHIP', # , # 24, # False',\\n    # 'OWNERSHIP_REDUCED', # , # 24, # False',\\n    # 'OWNERSHIP_NO_INFO_AS_NA_REDUCED', # , # 26, # False',\\n    # 'OWNERSHIP_NO_INFO_AS_NA', # , # 27, # False',\\n    # 'IS_COUNTRY_CODE_CH', # , # 28, # False',\\n    # 'HAS_END_CUSTOMER', # , # 29, # False',\\n    # 'IS_NA_REV_RATE', # , # 30, # False',\\n    # 'IS_NA_REV_AVG', # , # 31, # False',\\n    # 'HAS_COSTS_PRODUCT_D', # , # 32, # False',\\n    # 'HAS_COSTS_PRODUCT_B', # , # 33, # False',\\n    # 'IS_NA_REV_CURRENT_YEAR', # , # 33, # False',\\n    # 'IS_NA_CURRENCY', # , # 35, # False',\\n    # 'IS_NA_OWNERSHIP_NO_INFO_AS_NA', # , # 35, # False',\\n    # 'HAS_ISIC', # , # 37, # False',\\n    # 'HAS_COSTS_PRODUCT_A', # , # 38, # False',\\n    # 'HAS_COSTS_PRODUCT_E', # , # 39, # False',\\n    # 'HAS_COSTS_PRODUCT_C', # , # 40, # False',\\n    # 'IS_NA_SALES_OFFICE', # , # 41, # False',\\n    # 'IS_NA_SALES_LOCATION', # , # 42, # False'\\n]\\nassert \\\"OFFER_STATUS\\\" in selected_cols_by_boruta\\ndf = df[selected_cols_by_boruta]\\n###############################################\\n\\n######## Data Analysis and Visualization ######\\nif is_data_analysis_and_visualization_part:\\n\\n    def data_analysis_and_visualization_part():\\n        print(\\\"[INFO] Correlation Matrix:\\\")\\n        f, ax = plt.subplots(figsize=(30, 13))\\n        sns.heatmap(abs(df.corr()), annot=True)\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Values:\\\")\\n        msno.bar(df, color=\\\"#79ccb3\\\", sort=\\\"descending\\\")\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Value Correlations - 1:\\\")\\n        msno.matrix(\\n            df[get_null_columns(df)],\\n            color=(0.45, 0.45, 0.64),\\n            figsize=(27, 10),\\n            width_ratios=(10, 0),\\n        )\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Value Correlations - 2:\\\")\\n        msno.dendrogram(df[get_null_columns(df)])\\n        plt.show()\\n\\n        print(\\\"[INFO] Missing Value Correlations - 3:\\\")\\n        msno.heatmap(df[get_null_columns(df)])\\n        plt.show()\\n\\n    data_analysis_and_visualization_part()\\n###############################################\\n\\n######## Outlier Detection #############\\nprint(\\\"[INFO] Outlier Detection has begun.\\\")\\n\\ncol_to_type = [\\n    ######################### Nominal\\n    (\\\"BUSINESS_TYPE\\\", \\\"Nominal\\\"),\\n    (\\\"CURRENCY\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_A\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_B\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_C\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_D\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_COSTS_PRODUCT_E\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_END_CUSTOMER\\\", \\\"Nominal\\\"),\\n    (\\\"HAS_ISIC\\\", \\\"Nominal\\\"),\\n    (\\\"IS_COUNTRY_CODE_CH\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_CURRENCY\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_OWNERSHIP_NO_INFO_AS_NA\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_REV_CURRENT_YEAR\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_REV_AVG\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_REV_RATE\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_SALES_OFFICE\\\", \\\"Nominal\\\"),\\n    (\\\"IS_NA_SALES_LOCATION\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_STATUS\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_TYPE\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_TYPE_REDUCED_1\\\", \\\"Nominal\\\"),\\n    (\\\"OFFER_TYPE_REDUCED_2\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NA_AS_NO_INFO\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NA_AS_NO_INFO_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NO_INFO_AS_NA\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_NO_INFO_AS_NA_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"OWNERSHIP_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"PRICE_LIST\\\", \\\"Nominal\\\"),\\n    (\\\"SALES_LOCATION\\\", \\\"Nominal\\\"),\\n    (\\\"SALES_OFFICE\\\", \\\"Nominal\\\"),\\n    (\\\"SALES_OFFICE_REDUCED\\\", \\\"Nominal\\\"),\\n    (\\\"TECH\\\", \\\"Nominal\\\"),\\n    (\\\"TECH_REDUCED_1\\\", \\\"Nominal\\\"),\\n    (\\\"TECH_REDUCED_2_IS_F\\\", \\\"Nominal\\\"),\\n    ######################### Continuous\\n    (\\\"CREATION_YEAR\\\", \\\"Continuous\\\"),\\n    (\\\"CREATION_YEAR_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"MATERIAL_COST\\\", \\\"Continuous\\\"),\\n    (\\\"MATERIAL_COST_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"OFFER_PRICE\\\", \\\"Continuous\\\"),\\n    (\\\"OFFER_PRICE_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"REV_AVG\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR.1\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR_LOG.1\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR.2\\\", \\\"Continuous\\\"),\\n    (\\\"REV_CURRENT_YEAR_LOG.2\\\", \\\"Continuous\\\"),\\n    (\\\"REV_RATE\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_COST\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_COST_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_LIST_PRICE\\\", \\\"Continuous\\\"),\\n    (\\\"SERVICE_LIST_PRICE_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"SINCE_CREATION_YEAR\\\", \\\"Continuous\\\"),\\n    (\\\"SINCE_CREATION_YEAR_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"SO_CREATED_DATE_INTEGER\\\", \\\"Continuous\\\"),\\n    (\\\"SO_CREATED_DATE_SCALED\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COST\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COST_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COST_RATIO\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COSTS_PRODUCT\\\", \\\"Continuous\\\"),\\n    (\\\"TOTAL_COSTS_PRODUCT_LOG\\\", \\\"Continuous\\\"),\\n    (\\\"ADDITIONAL_COST\\\", \\\"Continuous\\\"),\\n]\\ndf_info = MissingUniqueStatistics(df)\\ndf_col_to_type = pd.DataFrame(col_to_type, columns=[\\\"Variable\\\", \\\"Variable_Type\\\"])\\ndf_info = df_info.merge(df_col_to_type, on=\\\"Variable\\\").set_index(\\\"Variable\\\")\\nprint_if_verbose(df_info)\\n\\nnumerical_columns = list(\\n    df_info.loc[\\n        (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Cardinal\\\")\\n        | (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Continuous\\\")\\n    ].index\\n)\\nassert len(numerical_columns) > 0\\n\\ncategorical_columns = list(\\n    df_info.loc[\\n        (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Nominal\\\")\\n        | (df_info.loc[:, \\\"Variable_Type\\\"] == \\\"Ordinal\\\")\\n    ].index\\n)\\nassert len(categorical_columns) > 0\\n\\n\\nassert len({x[0] for x in col_to_type}) == len(\\n    [x[0] for x in col_to_type]\\n), \\\"There are some duplicate columns in col_to_type list!\\\"\\nassert not np.any(df_info.isna()), \\\"Some variable types are null!\\\"\\nassert len(df_info.index) == df_info.shape[0], \\\"Some Rows are not unique!\\\"\\n\\ndf_all_processed = df.copy()\\ndf_for_unlabeled_set = df_all_processed[np.isnan(df_all_processed[\\\"OFFER_STATUS\\\"])]\\ndf = df_all_processed[~np.isnan(df[\\\"OFFER_STATUS\\\"])]\\n\\nif training_mode:\\n    x, y = df.drop([\\\"OFFER_STATUS\\\"], axis=1), df[\\\"OFFER_STATUS\\\"]\\n    X_train, X_test, Y_train, Y_test = train_test_split(\\n        x, y, test_size=test_size_in_split, stratify=y, random_state=42\\n    )\\n    if verbose:\\n        histogram(X_train, Y_train)\\n        histogram(X_test, Y_test)\\nelse:\\n    X_train, Y_train = df.drop([\\\"OFFER_STATUS\\\"], axis=1), df[\\\"OFFER_STATUS\\\"]\\n\\n    X_test = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n    Y_test = df_for_unlabeled_set[\\\"OFFER_STATUS\\\"]\\n\\n\\nprint_if_verbose(\\n    f\\\"df_all_processed:{df_all_processed.shape}\\\",\\n    f\\\"df_for_unlabeled_set:{df_for_unlabeled_set.shape}\\\",\\n    f\\\"X_train:{X_train.shape}\\\",\\n    f\\\"Y_train:{Y_train.shape}\\\",\\n    f\\\"X_test:{X_test.shape}\\\",\\n    f\\\"Y_test:{Y_test.shape}\\\",\\n    sep=\\\"\\\\n\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Block Specific Helper Functions #######\n",
    "def print_if_verbose(*args, **kwargs):\n",
    "    if verbose:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "############# Data Postprocessing #############\n",
    "df = pd.read_csv(preprocessed_data_path).drop(  # Read Data\n",
    "    [\"CUSTOMER\", \"TEST_SET_ID\", \"IDX_CUSTOMER\"], axis=1\n",
    ")\n",
    "\n",
    "# Add some new columns\n",
    "df[\"ADDITIONAL_COST\"] = df[\"OFFER_PRICE\"] - df[\"MATERIAL_COST\"] - df[\"SERVICE_COST\"]\n",
    "df[\"TOTAL_COST\"] = df[\"MATERIAL_COST\"] + df[\"SERVICE_COST\"]\n",
    "df[\"TOTAL_COST_LOG\"] = df[\"MATERIAL_COST\"] + df[\"SERVICE_COST\"]\n",
    "df[\"TOTAL_COST_RATIO\"] = df[\"MATERIAL_COST\"] / df[\"TOTAL_COST\"]\n",
    "\n",
    "# HIGH ABSOLUTE CORRELATIONS BETWEEN INDEPENDENT VALUES:\n",
    "# > abs(Corr)=1.00 with: \"SO_CREATED_DATE_SCALED\"    <->   \"SO_CREATED_DATE_INTEGER\"\n",
    "# > abs(Corr)=1.00 with: \"SINCE_CREATION_YEAR\"       <->   \"CREATION_YEAR_LOG\"\n",
    "# > abs(Corr)=1.00 with: \"SINCE_CREATION_YEAR\"       <->   \"CREATION_YEAR\"\n",
    "# > abs(Corr)=1.00 with: \"IS_NA_REV_CURRENT_YEAR\"    <->   \"IS_NA_CURRENCY\"\n",
    "# > abs(Corr)=1.00 with: \"MATERIAL_COST_LOG\"         <->   \"SERVICE_LIST_PRICE_LOG\"\n",
    "# > abs(Corr)=1.00 with: \"TOTAL_COST\"                <->   \"TOTAL_COST_LOG\"\n",
    "# > abs(Corr)=0.99 with: \"TOTAL_COST\"                <->   \"IS_NA_REV_AVG\"\n",
    "# > abs(Corr)=0.99 with: \"SERVICE_LIST_PRICE\"        <->   \"MATERIAL_COST\"\n",
    "# > abs(Corr)=0.99 with: \"REV_AVG\"                   <->   \"REV_CURRENT_YEAR.2\"\n",
    "# > abs(Corr)=0.99 with: \"REV_AVG\"                   <->   \"REV_CURRENT_YEAR.1\"\n",
    "# > abs(Corr)=0.99 with: \"IS_NA_SALES_OFFICE\"        <->   \"IS_NA_SALES_LOCATION\"\n",
    "# > abs(Corr)=0.99 with: \"IS_NA_REV_CURRENT_YEAR\"    <->   \"IS_NA_OWNERSHIP_NO_INFO_AS_NA\"\n",
    "# > abs(Corr)=0.96 with: \"TOTAL_COST\"                <->   \"OFFER_PRICE\"\n",
    "# > abs(Corr)=0.96 with: \"SINCE_CREATION_YEAR_LOG\"   <->   \"SINCE_CREATION_YEAR\"\n",
    "\n",
    "selected_cols_by_boruta = [\n",
    "    \"OFFER_STATUS\",  ################ Target Column\n",
    "    \"ADDITIONAL_COST\",  # , # 1, # True',\n",
    "    \"BUSINESS_TYPE\",  # , # 1, # True',\n",
    "    \"OFFER_TYPE\",  # , # 1, # True',\n",
    "    \"OFFER_TYPE_REDUCED_1\",  # , # 1, # True',\n",
    "    \"OFFER_TYPE_REDUCED_2\",  # , # 1, # True',\n",
    "    \"REV_AVG\",  # , # 1, # True',\n",
    "    \"REV_CURRENT_YEAR.1\",  # , # 1, # True',\n",
    "    \"REV_CURRENT_YEAR_LOG.1\",  # , # 1, # True',\n",
    "    \"REV_CURRENT_YEAR_LOG.2\",  # , # 1, # True',\n",
    "    \"REV_RATE\",  # , # 1, # True',\n",
    "    \"SALES_LOCATION\",  # , # 1, # True',\n",
    "    \"SALES_OFFICE\",  # , # 1, # True',\n",
    "    \"SALES_OFFICE_REDUCED\",  # , # 1, # True',\n",
    "    \"SERVICE_COST\",  # , # 1, # True',\n",
    "    \"SERVICE_COST_LOG\",  # , # 1, # True',\n",
    "    \"SO_CREATED_DATE_INTEGER\",  # , # 1, # True',\n",
    "    # \"SO_CREATED_DATE_SCALED\",  # , # 1, # True',\n",
    "    # \"TOTAL_COST_RATIO\",  # , # 1, # True',\n",
    "    \"OFFER_PRICE_LOG\",  # , # 2, # False',\n",
    "    \"OFFER_PRICE\",  # , # 3, # False',\n",
    "    \"TECH_REDUCED_1\",  # , # 4, # False',\n",
    "    # \"REV_CURRENT_YEAR.2\",  # , # 5, # False',\n",
    "    # \"TOTAL_COST\",  # , # 5, # False',\n",
    "    \"TECH\",  # , # 7, # False',\n",
    "    \"TOTAL_COST_LOG\",  # , # 7, # False',\n",
    "    # \"PRICE_LIST\",  # , # 9, # False',\n",
    "    \"SERVICE_LIST_PRICE\",  # , # 9, # False',\n",
    "    \"SERVICE_LIST_PRICE_LOG\",  # , # 11, # False',\n",
    "    # \"MATERIAL_COST_LOG\",  # , # 12, # False',\n",
    "    \"MATERIAL_COST\",  # , # 13, # False',\n",
    "    \"CURRENCY\",  # , # 14, # False',\n",
    "    # \"SINCE_CREATION_YEAR\",  # , # 15, # False',\n",
    "    # \"CREATION_YEAR\",  # , # 16, # False',\n",
    "    # \"CREATION_YEAR_LOG\",  # , # 16, # False',\n",
    "    # \"SINCE_CREATION_YEAR_LOG\",  # , # 18, # False',\n",
    "    # 'TECH_REDUCED_2_IS_F', # , # 19, # False',\n",
    "    # 'OWNERSHIP_NA_AS_NO_INFO', # , # 21, # False',\n",
    "    # 'TOTAL_COSTS_PRODUCT', # , # 21, # False',\n",
    "    # 'TOTAL_COSTS_PRODUCT_LOG', # , # 21, # False',\n",
    "    # 'OWNERSHIP_NA_AS_NO_INFO_REDUCED', # , # 23, # False',\n",
    "    # 'OWNERSHIP', # , # 24, # False',\n",
    "    # 'OWNERSHIP_REDUCED', # , # 24, # False',\n",
    "    # 'OWNERSHIP_NO_INFO_AS_NA_REDUCED', # , # 26, # False',\n",
    "    # 'OWNERSHIP_NO_INFO_AS_NA', # , # 27, # False',\n",
    "    # 'IS_COUNTRY_CODE_CH', # , # 28, # False',\n",
    "    # 'HAS_END_CUSTOMER', # , # 29, # False',\n",
    "    # 'IS_NA_REV_RATE', # , # 30, # False',\n",
    "    # 'IS_NA_REV_AVG', # , # 31, # False',\n",
    "    # 'HAS_COSTS_PRODUCT_D', # , # 32, # False',\n",
    "    # 'HAS_COSTS_PRODUCT_B', # , # 33, # False',\n",
    "    # 'IS_NA_REV_CURRENT_YEAR', # , # 33, # False',\n",
    "    # 'IS_NA_CURRENCY', # , # 35, # False',\n",
    "    # 'IS_NA_OWNERSHIP_NO_INFO_AS_NA', # , # 35, # False',\n",
    "    # 'HAS_ISIC', # , # 37, # False',\n",
    "    # 'HAS_COSTS_PRODUCT_A', # , # 38, # False',\n",
    "    # 'HAS_COSTS_PRODUCT_E', # , # 39, # False',\n",
    "    # 'HAS_COSTS_PRODUCT_C', # , # 40, # False',\n",
    "    # 'IS_NA_SALES_OFFICE', # , # 41, # False',\n",
    "    # 'IS_NA_SALES_LOCATION', # , # 42, # False'\n",
    "]\n",
    "assert \"OFFER_STATUS\" in selected_cols_by_boruta\n",
    "df = df[selected_cols_by_boruta]\n",
    "###############################################\n",
    "\n",
    "######## Data Analysis and Visualization ######\n",
    "if is_data_analysis_and_visualization_part:\n",
    "\n",
    "    def data_analysis_and_visualization_part():\n",
    "        print(\"[INFO] Correlation Matrix:\")\n",
    "        f, ax = plt.subplots(figsize=(30, 13))\n",
    "        sns.heatmap(abs(df.corr()), annot=True)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"[INFO] Missing Values:\")\n",
    "        msno.bar(df, color=\"#79ccb3\", sort=\"descending\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"[INFO] Missing Value Correlations - 1:\")\n",
    "        msno.matrix(\n",
    "            df[get_null_columns(df)],\n",
    "            color=(0.45, 0.45, 0.64),\n",
    "            figsize=(27, 10),\n",
    "            width_ratios=(10, 0),\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        print(\"[INFO] Missing Value Correlations - 2:\")\n",
    "        msno.dendrogram(df[get_null_columns(df)])\n",
    "        plt.show()\n",
    "\n",
    "        print(\"[INFO] Missing Value Correlations - 3:\")\n",
    "        msno.heatmap(df[get_null_columns(df)])\n",
    "        plt.show()\n",
    "\n",
    "    data_analysis_and_visualization_part()\n",
    "###############################################\n",
    "\n",
    "######## Outlier Detection #############\n",
    "print(\"[INFO] Outlier Detection has begun.\")\n",
    "\n",
    "\n",
    "col_to_type = [\n",
    "    ######################### Nominal\n",
    "    (\"BUSINESS_TYPE\", \"Nominal\"),\n",
    "    (\"CURRENCY\", \"Nominal\"),\n",
    "    (\"HAS_COSTS_PRODUCT_A\", \"Nominal\"),\n",
    "    (\"HAS_COSTS_PRODUCT_B\", \"Nominal\"),\n",
    "    (\"HAS_COSTS_PRODUCT_C\", \"Nominal\"),\n",
    "    (\"HAS_COSTS_PRODUCT_D\", \"Nominal\"),\n",
    "    (\"HAS_COSTS_PRODUCT_E\", \"Nominal\"),\n",
    "    (\"HAS_END_CUSTOMER\", \"Nominal\"),\n",
    "    (\"HAS_ISIC\", \"Nominal\"),\n",
    "    (\"IS_COUNTRY_CODE_CH\", \"Nominal\"),\n",
    "    (\"IS_NA_CURRENCY\", \"Nominal\"),\n",
    "    (\"IS_NA_OWNERSHIP_NO_INFO_AS_NA\", \"Nominal\"),\n",
    "    (\"IS_NA_REV_CURRENT_YEAR\", \"Nominal\"),\n",
    "    (\"IS_NA_REV_AVG\", \"Nominal\"),\n",
    "    (\"IS_NA_REV_RATE\", \"Nominal\"),\n",
    "    (\"IS_NA_SALES_OFFICE\", \"Nominal\"),\n",
    "    (\"IS_NA_SALES_LOCATION\", \"Nominal\"),\n",
    "    (\"OFFER_STATUS\", \"Nominal\"),\n",
    "    (\"OFFER_TYPE\", \"Nominal\"),\n",
    "    (\"OFFER_TYPE_REDUCED_1\", \"Nominal\"),\n",
    "    (\"OFFER_TYPE_REDUCED_2\", \"Nominal\"),\n",
    "    (\"OWNERSHIP\", \"Nominal\"),\n",
    "    (\"OWNERSHIP_NA_AS_NO_INFO\", \"Nominal\"),\n",
    "    (\"OWNERSHIP_NA_AS_NO_INFO_REDUCED\", \"Nominal\"),\n",
    "    (\"OWNERSHIP_NO_INFO_AS_NA\", \"Nominal\"),\n",
    "    (\"OWNERSHIP_NO_INFO_AS_NA_REDUCED\", \"Nominal\"),\n",
    "    (\"OWNERSHIP_REDUCED\", \"Nominal\"),\n",
    "    (\"PRICE_LIST\", \"Nominal\"),\n",
    "    (\"SALES_LOCATION\", \"Nominal\"),\n",
    "    (\"SALES_OFFICE\", \"Nominal\"),\n",
    "    (\"SALES_OFFICE_REDUCED\", \"Nominal\"),\n",
    "    (\"TECH\", \"Nominal\"),\n",
    "    (\"TECH_REDUCED_1\", \"Nominal\"),\n",
    "    (\"TECH_REDUCED_2_IS_F\", \"Nominal\"),\n",
    "    ######################### Continuous\n",
    "    (\"CREATION_YEAR\", \"Continuous\"),\n",
    "    (\"CREATION_YEAR_LOG\", \"Continuous\"),\n",
    "    (\"MATERIAL_COST\", \"Continuous\"),\n",
    "    (\"MATERIAL_COST_LOG\", \"Continuous\"),\n",
    "    (\"OFFER_PRICE\", \"Continuous\"),\n",
    "    (\"OFFER_PRICE_LOG\", \"Continuous\"),\n",
    "    (\"REV_AVG\", \"Continuous\"),\n",
    "    (\"REV_CURRENT_YEAR.1\", \"Continuous\"),\n",
    "    (\"REV_CURRENT_YEAR_LOG.1\", \"Continuous\"),\n",
    "    (\"REV_CURRENT_YEAR.2\", \"Continuous\"),\n",
    "    (\"REV_CURRENT_YEAR_LOG.2\", \"Continuous\"),\n",
    "    (\"REV_RATE\", \"Continuous\"),\n",
    "    (\"SERVICE_COST\", \"Continuous\"),\n",
    "    (\"SERVICE_COST_LOG\", \"Continuous\"),\n",
    "    (\"SERVICE_LIST_PRICE\", \"Continuous\"),\n",
    "    (\"SERVICE_LIST_PRICE_LOG\", \"Continuous\"),\n",
    "    (\"SINCE_CREATION_YEAR\", \"Continuous\"),\n",
    "    (\"SINCE_CREATION_YEAR_LOG\", \"Continuous\"),\n",
    "    (\"SO_CREATED_DATE_INTEGER\", \"Continuous\"),\n",
    "    (\"SO_CREATED_DATE_SCALED\", \"Continuous\"),\n",
    "    (\"TOTAL_COST\", \"Continuous\"),\n",
    "    (\"TOTAL_COST_LOG\", \"Continuous\"),\n",
    "    (\"TOTAL_COST_RATIO\", \"Continuous\"),\n",
    "    (\"TOTAL_COSTS_PRODUCT\", \"Continuous\"),\n",
    "    (\"TOTAL_COSTS_PRODUCT_LOG\", \"Continuous\"),\n",
    "    (\"ADDITIONAL_COST\", \"Continuous\"),\n",
    "]\n",
    "df_info = MissingUniqueStatistics(df)\n",
    "df_col_to_type = pd.DataFrame(col_to_type, columns=[\"Variable\", \"Variable_Type\"])\n",
    "df_info = df_info.merge(df_col_to_type, on=\"Variable\").set_index(\"Variable\")\n",
    "print_if_verbose(df_info)\n",
    "\n",
    "numerical_columns = list(\n",
    "    df_info.loc[\n",
    "        (df_info.loc[:, \"Variable_Type\"] == \"Cardinal\")\n",
    "        | (df_info.loc[:, \"Variable_Type\"] == \"Continuous\")\n",
    "    ].index\n",
    ")\n",
    "assert len(numerical_columns) > 0\n",
    "\n",
    "categorical_columns = list(\n",
    "    df_info.loc[\n",
    "        (df_info.loc[:, \"Variable_Type\"] == \"Nominal\")\n",
    "        | (df_info.loc[:, \"Variable_Type\"] == \"Ordinal\")\n",
    "    ].index\n",
    ")\n",
    "assert len(categorical_columns) > 0\n",
    "\n",
    "\n",
    "assert len({x[0] for x in col_to_type}) == len(\n",
    "    [x[0] for x in col_to_type]\n",
    "), \"There are some duplicate columns in col_to_type list!\"\n",
    "assert not np.any(df_info.isna()), \"Some variable types are null!\"\n",
    "assert len(df_info.index) == df_info.shape[0], \"Some Rows are not unique!\"\n",
    "\n",
    "df_all_processed = df.copy()\n",
    "df_for_unlabeled_set = df_all_processed[np.isnan(df_all_processed[\"OFFER_STATUS\"])]\n",
    "df = df_all_processed[~np.isnan(df[\"OFFER_STATUS\"])]\n",
    "\n",
    "if training_mode:\n",
    "    x, y = df.drop([\"OFFER_STATUS\"], axis=1), df[\"OFFER_STATUS\"]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        x, y, test_size=test_size_in_split, stratify=y, random_state=42\n",
    "    )\n",
    "    if verbose:\n",
    "        histogram(X_train, Y_train)\n",
    "        histogram(X_test, Y_test)\n",
    "else:\n",
    "    X_train, Y_train = df.drop([\"OFFER_STATUS\"], axis=1), df[\"OFFER_STATUS\"]\n",
    "\n",
    "    X_test = df_for_unlabeled_set.drop([\"OFFER_STATUS\"], axis=1)\n",
    "    Y_test = df_for_unlabeled_set[\"OFFER_STATUS\"]\n",
    "\n",
    "\n",
    "print_if_verbose(\n",
    "    f\"df_all_processed:{df_all_processed.shape}\",\n",
    "    f\"df_for_unlabeled_set:{df_for_unlabeled_set.shape}\",\n",
    "    f\"X_train:{X_train.shape}\",\n",
    "    f\"Y_train:{Y_train.shape}\",\n",
    "    f\"X_test:{X_test.shape}\",\n",
    "    f\"Y_test:{Y_test.shape}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Columns: []\n",
      "[INFO] Outlier Detection has been completed.\n",
      "[INFO] Missing Value Imputation has begun.\n",
      "[INFO] Number of Zero_MR_variables_list: 16\n",
      "[INFO] Number of Low_MR_variables_list: 3\n",
      "[INFO] Number of Moderate_MR_variables_list: 7\n",
      "[INFO] Number of High_MR_variables_list: 0\n",
      "[INFO] Number of Extreme_MR_variables_list: 0\n",
      "[INFO] Missing Value Imputation has been completed.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"### Finding sparse columns\\n\\nsparse_columns = []\\nfor col in numerical_columns:\\n    if df[col].quantile(0.01) == df[col].quantile(0.25) == df[col].mode()[0]:\\n        sparse_columns.append(col)\\n\\nsparse_columns_2 = []\\nfor col in numerical_columns:\\n    if df[col].quantile(0.01) == df[col].quantile(0.25):\\n        sparse_columns_2.append(col)\\nassert sparse_columns == sparse_columns_2\\n\\nprint(\\\"Sparse Columns:\\\", sparse_columns)\\n\\ndef HardEdgeReduction(\\n    df, numerical_columns, sparse_columns, upper_quantile=0.99, lower_quantile=0.01\\n):\\n    \\\"\\\"\\\"\\n    Algorithm 'HER(Hard-Edges Method)' applies induction to the elements of a value line which are:\\n\\n        - lower than the 1th quantile to that quantile and\\n        - upper than the 99th quantile to that quantile.\\n\\n    Main aim is to diminish negative effects of outlier values on analytical operations being performed.\\n    \\\"\\\"\\\"\\n\\n    import pandas as pd\\n    import psutil, os, gc, time\\n\\n    print_if_verbose(\\\"HardEdgeReduction process has began:\\\\n\\\")\\n    proc = psutil.Process(os.getpid())\\n    gc.collect()\\n    mem_0 = proc.memory_info().rss\\n    start_time = time.time()\\n\\n    epsilon = 0.0001  # for zero divisions\\n\\n    # Define boundaries that we will use for Reduction operation\\n    df_outlier_cleaned = df.copy()\\n\\n    print_if_verbose(\\n        \\\"Detected outliers will be replaced with edged quantiles/percentiles: 1% and 99%!\\\\n\\\"\\n    )\\n    print_if_verbose(\\\"Total number of rows is: %s\\\\n\\\" % df_outlier_cleaned.shape[0])\\n\\n    outlier_boundries_dict = {}\\n\\n    for col in numerical_columns:\\n\\n        if col in sparse_columns:\\n            nonsparse_data = pd.DataFrame(\\n                df_outlier_cleaned[\\n                    df_outlier_cleaned[col] != df_outlier_cleaned[col].mode()[0]\\n                ][col]\\n            )\\n\\n            if (  # For lower threshold (left-hand-side)\\n                nonsparse_data[col].quantile(lower_quantile)\\n                < df_outlier_cleaned[col].mode()[0]\\n            ):  # Unexpected case\\n                lower_bound_sparse = nonsparse_data[col].quantile(lower_quantile)\\n            else:\\n                lower_bound_sparse = df_outlier_cleaned[col].mode()[0]\\n\\n            if (  # For upper threshold (right-hand-side)\\n                nonsparse_data[col].quantile(upper_quantile)\\n                < df_outlier_cleaned[col].mode()[0]\\n            ):  # Unexpected case\\n                upper_bound_sparse = df_outlier_cleaned[col].mode()[0]\\n            else:\\n                upper_bound_sparse = nonsparse_data[col].quantile(upper_quantile)\\n\\n            outlier_boundries_dict[col] = (lower_bound_sparse, upper_bound_sparse)\\n\\n            # Inform user about the cardinality of Outlier existence:\\n            number_of_outliers = len(\\n                df_outlier_cleaned[\\n                    (df_outlier_cleaned[col] < lower_bound_sparse)\\n                    | (df_outlier_cleaned[col] > upper_bound_sparse)\\n                ][col]\\n            )\\n            print_if_verbose(\\n                \\\"Sparse: Outlier number in {} is equal to: \\\".format(col),\\n                round(\\n                    number_of_outliers\\n                    / (nonsparse_data.shape[0] - nonsparse_data.isnull().sum()),\\n                    2,\\n                ),\\n            )\\n\\n            # Replace Outliers with Edges --> 1% and 99%:\\n            if number_of_outliers > 0:\\n\\n                # Replace 'left-hand-side' outliers with its 1% quantile value\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] < lower_bound_sparse, col\\n                ] = (\\n                    lower_bound_sparse - epsilon\\n                )  # --> MAIN DF CHANGED\\n\\n                # Replace 'right-hand-side' outliers with its 99% quantile value\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] > upper_bound_sparse, col\\n                ] = (\\n                    upper_bound_sparse + epsilon\\n                )  # --> MAIN DF CHANGED\\n\\n        else:  # Find Edges:\\n            number_of_outliers = len(\\n                df_outlier_cleaned[\\n                    (\\n                        df_outlier_cleaned[col]\\n                        < df_outlier_cleaned[col].quantile(lower_quantile)\\n                    )\\n                    | (\\n                        df_outlier_cleaned[col]\\n                        > df_outlier_cleaned[col].quantile(upper_quantile)\\n                    )\\n                ][col]\\n            )\\n            print_if_verbose(\\n                \\\"Other: Outlier number in {} is equal to: \\\".format(col),\\n                round(\\n                    number_of_outliers / (df[col].shape[0] - df[col].isnull().sum()), 2\\n                ),\\n            )\\n\\n            # Replace 'Standard' outliers:\\n            if number_of_outliers > 0:\\n                # Replace all outliers with its %99 quartile\\n                lower_bound_sparse = df_outlier_cleaned[col].quantile(lower_quantile)\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] < lower_bound_sparse, col\\n                ] = (lower_bound_sparse - epsilon)\\n\\n                upper_bound_sparse = df_outlier_cleaned[col].quantile(upper_quantile)\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] > upper_bound_sparse, col\\n                ] = (upper_bound_sparse + epsilon)\\n\\n            outlier_boundries_dict[col] = (lower_bound_sparse, upper_bound_sparse)\\n\\n    print_if_verbose(\\\"HardEdgeReduction process has been completed!\\\")\\n    print_if_verbose(\\\"--- in %s minutes ---\\\" % ((time.time() - start_time) / 60))\\n\\n    return df_outlier_cleaned, outlier_boundries_dict\\n\\n\\nX_train, outlier_boundries_dict = HardEdgeReduction(\\n    X_train, numerical_columns, sparse_columns\\n)\\n\\nprint_if_verbose(outlier_boundries_dict)\\n\\n##  Cleaning Outliers for Test Dataset\\n\\nepsilon = 0.0001  # for zero divisions\\n\\n# Define boundaries that we will use for Reduction operation\\nupper_quantile, lower_quantile = 0.99, 0.01\\n\\ndf_test_outlier_cleaned = X_test.copy()\\n\\nprint_if_verbose(\\n    \\\"Detected outliers being replaced with edged quantiles/percentiles: 1% and 99%!\\\"\\n)\\nprint_if_verbose(\\\"Total number of rows is: %s\\\\n\\\" % df_test_outlier_cleaned.shape[0])\\n\\nfor col in numerical_columns:\\n    lower_bound = outlier_boundries_dict[col][0]\\n    upper_bound = outlier_boundries_dict[col][1]\\n\\n    # Inform user about the cardinality of Outlier existence:\\n    number_of_outliers = len(\\n        df_test_outlier_cleaned[\\n            (df_test_outlier_cleaned[col] < lower_bound)\\n            | (df_test_outlier_cleaned[col] > upper_bound)\\n        ][col]\\n    )\\n    print_if_verbose(\\n        \\\"Outlier number in {} is equal to: \\\".format(col),\\n        round(\\n            number_of_outliers\\n            / (\\n                df_test_outlier_cleaned[col].shape[0]\\n                - df_test_outlier_cleaned[col].isnull().sum()\\n            ),\\n            2,\\n        ),\\n    )\\n\\n    # Replace Outliers with Edges --> 1% and 99%:\\n    if number_of_outliers > 0:\\n        # Replace 'left-hand-side' outliers with its 1% quantile value\\n        df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] < lower_bound, col] = (\\n            lower_bound - epsilon\\n        )  # --> MAIN DF CHANGED\\n\\n        # Replace 'right-hand-side' outliers with its 99% quantile value\\n        df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] > upper_bound, col] = (\\n            upper_bound + epsilon\\n        )  # --> MAIN DF CHANGED\\n\\nX_test = df_test_outlier_cleaned\\n\\nif verbose:\\n    box_plot(\\n        Y_train, numerical_columns, X_train\\n    )  ## Visualization After Cleaning Outlier\\n\\nprint(\\\"[INFO] Outlier Detection has been completed.\\\")\\n###############################################\\n\\n######## Missing Value Imputation #############\\nprint(\\\"[INFO] Missing Value Imputation has begun.\\\")\\nmissing_values_info = df_info[\\\"%_Missing_Value\\\"]\\n\\nZero_MR_variables_list = list(df_info[missing_values_info == 0].index)\\nLow_MR_variables_list = list(\\n    df_info[(missing_values_info > 0) & (missing_values_info <= 0.05)].index\\n)\\nModerate_MR_variables_list = list(\\n    df_info[(missing_values_info > 0.05) & (missing_values_info <= 0.275)].index\\n)\\nHigh_MR_variables_list = list(\\n    df_info[(missing_values_info > 0.275) & (missing_values_info <= 0.50)].index\\n)\\nExtreme_MR_variables_list = list(\\n    df_info[(missing_values_info > 0.50) & (missing_values_info <= 0.95)].index\\n)\\nDrop_MR_variables_list = list(df_info[missing_values_info > 0.95].index)\\n\\nprint(\\\"[INFO] Number of Zero_MR_variables_list:\\\", len(Zero_MR_variables_list))\\nprint(\\\"[INFO] Number of Low_MR_variables_list:\\\", len(Low_MR_variables_list))\\nprint(\\\"[INFO] Number of Moderate_MR_variables_list:\\\", len(Moderate_MR_variables_list))\\nprint(\\\"[INFO] Number of High_MR_variables_list:\\\", len(High_MR_variables_list))\\nprint(\\\"[INFO] Number of Extreme_MR_variables_list:\\\", len(Extreme_MR_variables_list))\\n\\n\\nassert len(Zero_MR_variables_list) + len(Low_MR_variables_list) + len(\\n    Moderate_MR_variables_list\\n) + len(High_MR_variables_list) + len(Extreme_MR_variables_list) == len(df_info)\\n\\n# Simple Imputer for Low Missing Values\\n\\n\\ndef SimpleImputer(df, data_info, variable_list):\\n    for col in variable_list:\\n\\n        if col in numerical_columns:\\n\\n            print_if_verbose(\\n                \\\"Total null values: {}\\\".format(df[[str(col)]].isnull().sum())\\n            )\\n\\n            average = float(df[col].mean())\\n            std = float(df[col].std())\\n            count_nan = int(df[col].isnull().sum())\\n            rand = np.random.normal(loc=average, scale=std, size=count_nan)\\n            slice_col = pd.Series(df[col].copy())\\n            slice_col[pd.isnull(slice_col)] = rand\\n            df[col] = slice_col\\n\\n            print_if_verbose(\\\"Numerical variable {} have been imputed.\\\".format(col))\\n\\n        else:\\n\\n            print_if_verbose(\\n                \\\"Total null values: {}\\\".format(df[[str(col)]].isnull().sum())\\n            )\\n            df.loc[df.loc[:, col].isnull(), col] = np.random.choice(\\n                sorted(list(df.loc[:, col].dropna().unique())),\\n                size=int(df.loc[df.loc[:, col].isnull(), col].shape[0]),\\n                p=[\\n                    pd.Series(\\n                        df.groupby(col).size() / df.loc[:, col].dropna().shape[0]\\n                    ).iloc[i]\\n                    for i in np.arange(0, len(df.loc[:, col].dropna().unique()))\\n                ],\\n            )\\n\\n            print_if_verbose(\\\"Categorical variable {} have been imputed.\\\".format(col))\\n\\n\\nprint_if_verbose(Low_MR_variables_list)\\n\\nSimpleImputer(X_train, df_info, Low_MR_variables_list)\\nSimpleImputer(X_test, df_info, Low_MR_variables_list)\\n\\nprint_if_verbose(MissingUniqueStatistics(X_train.loc[:, Low_MR_variables_list]))\\nprint_if_verbose(MissingUniqueStatistics(X_test.loc[:, Low_MR_variables_list]))\\n\\nprint(\\\"[INFO] Missing Value Imputation has been completed.\\\")\\n###############################################\";\n",
       "                var nbb_formatted_code = \"### Finding sparse columns\\n\\nsparse_columns = []\\nfor col in numerical_columns:\\n    if df[col].quantile(0.01) == df[col].quantile(0.25) == df[col].mode()[0]:\\n        sparse_columns.append(col)\\n\\nsparse_columns_2 = []\\nfor col in numerical_columns:\\n    if df[col].quantile(0.01) == df[col].quantile(0.25):\\n        sparse_columns_2.append(col)\\nassert sparse_columns == sparse_columns_2\\n\\nprint(\\\"Sparse Columns:\\\", sparse_columns)\\n\\n\\ndef HardEdgeReduction(\\n    df, numerical_columns, sparse_columns, upper_quantile=0.99, lower_quantile=0.01\\n):\\n    \\\"\\\"\\\"\\n    Algorithm 'HER(Hard-Edges Method)' applies induction to the elements of a value line which are:\\n\\n        - lower than the 1th quantile to that quantile and\\n        - upper than the 99th quantile to that quantile.\\n\\n    Main aim is to diminish negative effects of outlier values on analytical operations being performed.\\n    \\\"\\\"\\\"\\n\\n    import pandas as pd\\n    import psutil, os, gc, time\\n\\n    print_if_verbose(\\\"HardEdgeReduction process has began:\\\\n\\\")\\n    proc = psutil.Process(os.getpid())\\n    gc.collect()\\n    mem_0 = proc.memory_info().rss\\n    start_time = time.time()\\n\\n    epsilon = 0.0001  # for zero divisions\\n\\n    # Define boundaries that we will use for Reduction operation\\n    df_outlier_cleaned = df.copy()\\n\\n    print_if_verbose(\\n        \\\"Detected outliers will be replaced with edged quantiles/percentiles: 1% and 99%!\\\\n\\\"\\n    )\\n    print_if_verbose(\\\"Total number of rows is: %s\\\\n\\\" % df_outlier_cleaned.shape[0])\\n\\n    outlier_boundries_dict = {}\\n\\n    for col in numerical_columns:\\n\\n        if col in sparse_columns:\\n            nonsparse_data = pd.DataFrame(\\n                df_outlier_cleaned[\\n                    df_outlier_cleaned[col] != df_outlier_cleaned[col].mode()[0]\\n                ][col]\\n            )\\n\\n            if (  # For lower threshold (left-hand-side)\\n                nonsparse_data[col].quantile(lower_quantile)\\n                < df_outlier_cleaned[col].mode()[0]\\n            ):  # Unexpected case\\n                lower_bound_sparse = nonsparse_data[col].quantile(lower_quantile)\\n            else:\\n                lower_bound_sparse = df_outlier_cleaned[col].mode()[0]\\n\\n            if (  # For upper threshold (right-hand-side)\\n                nonsparse_data[col].quantile(upper_quantile)\\n                < df_outlier_cleaned[col].mode()[0]\\n            ):  # Unexpected case\\n                upper_bound_sparse = df_outlier_cleaned[col].mode()[0]\\n            else:\\n                upper_bound_sparse = nonsparse_data[col].quantile(upper_quantile)\\n\\n            outlier_boundries_dict[col] = (lower_bound_sparse, upper_bound_sparse)\\n\\n            # Inform user about the cardinality of Outlier existence:\\n            number_of_outliers = len(\\n                df_outlier_cleaned[\\n                    (df_outlier_cleaned[col] < lower_bound_sparse)\\n                    | (df_outlier_cleaned[col] > upper_bound_sparse)\\n                ][col]\\n            )\\n            print_if_verbose(\\n                \\\"Sparse: Outlier number in {} is equal to: \\\".format(col),\\n                round(\\n                    number_of_outliers\\n                    / (nonsparse_data.shape[0] - nonsparse_data.isnull().sum()),\\n                    2,\\n                ),\\n            )\\n\\n            # Replace Outliers with Edges --> 1% and 99%:\\n            if number_of_outliers > 0:\\n\\n                # Replace 'left-hand-side' outliers with its 1% quantile value\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] < lower_bound_sparse, col\\n                ] = (\\n                    lower_bound_sparse - epsilon\\n                )  # --> MAIN DF CHANGED\\n\\n                # Replace 'right-hand-side' outliers with its 99% quantile value\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] > upper_bound_sparse, col\\n                ] = (\\n                    upper_bound_sparse + epsilon\\n                )  # --> MAIN DF CHANGED\\n\\n        else:  # Find Edges:\\n            number_of_outliers = len(\\n                df_outlier_cleaned[\\n                    (\\n                        df_outlier_cleaned[col]\\n                        < df_outlier_cleaned[col].quantile(lower_quantile)\\n                    )\\n                    | (\\n                        df_outlier_cleaned[col]\\n                        > df_outlier_cleaned[col].quantile(upper_quantile)\\n                    )\\n                ][col]\\n            )\\n            print_if_verbose(\\n                \\\"Other: Outlier number in {} is equal to: \\\".format(col),\\n                round(\\n                    number_of_outliers / (df[col].shape[0] - df[col].isnull().sum()), 2\\n                ),\\n            )\\n\\n            # Replace 'Standard' outliers:\\n            if number_of_outliers > 0:\\n                # Replace all outliers with its %99 quartile\\n                lower_bound_sparse = df_outlier_cleaned[col].quantile(lower_quantile)\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] < lower_bound_sparse, col\\n                ] = (lower_bound_sparse - epsilon)\\n\\n                upper_bound_sparse = df_outlier_cleaned[col].quantile(upper_quantile)\\n                df_outlier_cleaned.loc[\\n                    df_outlier_cleaned[col] > upper_bound_sparse, col\\n                ] = (upper_bound_sparse + epsilon)\\n\\n            outlier_boundries_dict[col] = (lower_bound_sparse, upper_bound_sparse)\\n\\n    print_if_verbose(\\\"HardEdgeReduction process has been completed!\\\")\\n    print_if_verbose(\\\"--- in %s minutes ---\\\" % ((time.time() - start_time) / 60))\\n\\n    return df_outlier_cleaned, outlier_boundries_dict\\n\\n\\nX_train, outlier_boundries_dict = HardEdgeReduction(\\n    X_train, numerical_columns, sparse_columns\\n)\\n\\nprint_if_verbose(outlier_boundries_dict)\\n\\n##  Cleaning Outliers for Test Dataset\\n\\nepsilon = 0.0001  # for zero divisions\\n\\n# Define boundaries that we will use for Reduction operation\\nupper_quantile, lower_quantile = 0.99, 0.01\\n\\ndf_test_outlier_cleaned = X_test.copy()\\n\\nprint_if_verbose(\\n    \\\"Detected outliers being replaced with edged quantiles/percentiles: 1% and 99%!\\\"\\n)\\nprint_if_verbose(\\\"Total number of rows is: %s\\\\n\\\" % df_test_outlier_cleaned.shape[0])\\n\\nfor col in numerical_columns:\\n    lower_bound = outlier_boundries_dict[col][0]\\n    upper_bound = outlier_boundries_dict[col][1]\\n\\n    # Inform user about the cardinality of Outlier existence:\\n    number_of_outliers = len(\\n        df_test_outlier_cleaned[\\n            (df_test_outlier_cleaned[col] < lower_bound)\\n            | (df_test_outlier_cleaned[col] > upper_bound)\\n        ][col]\\n    )\\n    print_if_verbose(\\n        \\\"Outlier number in {} is equal to: \\\".format(col),\\n        round(\\n            number_of_outliers\\n            / (\\n                df_test_outlier_cleaned[col].shape[0]\\n                - df_test_outlier_cleaned[col].isnull().sum()\\n            ),\\n            2,\\n        ),\\n    )\\n\\n    # Replace Outliers with Edges --> 1% and 99%:\\n    if number_of_outliers > 0:\\n        # Replace 'left-hand-side' outliers with its 1% quantile value\\n        df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] < lower_bound, col] = (\\n            lower_bound - epsilon\\n        )  # --> MAIN DF CHANGED\\n\\n        # Replace 'right-hand-side' outliers with its 99% quantile value\\n        df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] > upper_bound, col] = (\\n            upper_bound + epsilon\\n        )  # --> MAIN DF CHANGED\\n\\nX_test = df_test_outlier_cleaned\\n\\nif verbose:\\n    box_plot(\\n        Y_train, numerical_columns, X_train\\n    )  ## Visualization After Cleaning Outlier\\n\\nprint(\\\"[INFO] Outlier Detection has been completed.\\\")\\n###############################################\\n\\n######## Missing Value Imputation #############\\nprint(\\\"[INFO] Missing Value Imputation has begun.\\\")\\nmissing_values_info = df_info[\\\"%_Missing_Value\\\"]\\n\\nZero_MR_variables_list = list(df_info[missing_values_info == 0].index)\\nLow_MR_variables_list = list(\\n    df_info[(missing_values_info > 0) & (missing_values_info <= 0.05)].index\\n)\\nModerate_MR_variables_list = list(\\n    df_info[(missing_values_info > 0.05) & (missing_values_info <= 0.275)].index\\n)\\nHigh_MR_variables_list = list(\\n    df_info[(missing_values_info > 0.275) & (missing_values_info <= 0.50)].index\\n)\\nExtreme_MR_variables_list = list(\\n    df_info[(missing_values_info > 0.50) & (missing_values_info <= 0.95)].index\\n)\\nDrop_MR_variables_list = list(df_info[missing_values_info > 0.95].index)\\n\\nprint(\\\"[INFO] Number of Zero_MR_variables_list:\\\", len(Zero_MR_variables_list))\\nprint(\\\"[INFO] Number of Low_MR_variables_list:\\\", len(Low_MR_variables_list))\\nprint(\\\"[INFO] Number of Moderate_MR_variables_list:\\\", len(Moderate_MR_variables_list))\\nprint(\\\"[INFO] Number of High_MR_variables_list:\\\", len(High_MR_variables_list))\\nprint(\\\"[INFO] Number of Extreme_MR_variables_list:\\\", len(Extreme_MR_variables_list))\\n\\n\\nassert len(Zero_MR_variables_list) + len(Low_MR_variables_list) + len(\\n    Moderate_MR_variables_list\\n) + len(High_MR_variables_list) + len(Extreme_MR_variables_list) == len(df_info)\\n\\n# Simple Imputer for Low Missing Values\\n\\n\\ndef SimpleImputer(df, data_info, variable_list):\\n    for col in variable_list:\\n\\n        if col in numerical_columns:\\n\\n            print_if_verbose(\\n                \\\"Total null values: {}\\\".format(df[[str(col)]].isnull().sum())\\n            )\\n\\n            average = float(df[col].mean())\\n            std = float(df[col].std())\\n            count_nan = int(df[col].isnull().sum())\\n            rand = np.random.normal(loc=average, scale=std, size=count_nan)\\n            slice_col = pd.Series(df[col].copy())\\n            slice_col[pd.isnull(slice_col)] = rand\\n            df[col] = slice_col\\n\\n            print_if_verbose(\\\"Numerical variable {} have been imputed.\\\".format(col))\\n\\n        else:\\n\\n            print_if_verbose(\\n                \\\"Total null values: {}\\\".format(df[[str(col)]].isnull().sum())\\n            )\\n            df.loc[df.loc[:, col].isnull(), col] = np.random.choice(\\n                sorted(list(df.loc[:, col].dropna().unique())),\\n                size=int(df.loc[df.loc[:, col].isnull(), col].shape[0]),\\n                p=[\\n                    pd.Series(\\n                        df.groupby(col).size() / df.loc[:, col].dropna().shape[0]\\n                    ).iloc[i]\\n                    for i in np.arange(0, len(df.loc[:, col].dropna().unique()))\\n                ],\\n            )\\n\\n            print_if_verbose(\\\"Categorical variable {} have been imputed.\\\".format(col))\\n\\n\\nprint_if_verbose(Low_MR_variables_list)\\n\\nSimpleImputer(X_train, df_info, Low_MR_variables_list)\\nSimpleImputer(X_test, df_info, Low_MR_variables_list)\\n\\nprint_if_verbose(MissingUniqueStatistics(X_train.loc[:, Low_MR_variables_list]))\\nprint_if_verbose(MissingUniqueStatistics(X_test.loc[:, Low_MR_variables_list]))\\n\\nprint(\\\"[INFO] Missing Value Imputation has been completed.\\\")\\n###############################################\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Finding sparse columns\n",
    "\n",
    "sparse_columns = []\n",
    "for col in numerical_columns:\n",
    "    if df[col].quantile(0.01) == df[col].quantile(0.25) == df[col].mode()[0]:\n",
    "        sparse_columns.append(col)\n",
    "\n",
    "sparse_columns_2 = []\n",
    "for col in numerical_columns:\n",
    "    if df[col].quantile(0.01) == df[col].quantile(0.25):\n",
    "        sparse_columns_2.append(col)\n",
    "assert sparse_columns == sparse_columns_2\n",
    "\n",
    "print(\"Sparse Columns:\", sparse_columns)\n",
    "\n",
    "\n",
    "def HardEdgeReduction(\n",
    "    df, numerical_columns, sparse_columns, upper_quantile=0.99, lower_quantile=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Algorithm 'HER(Hard-Edges Method)' applies induction to the elements of a value line which are:\n",
    "\n",
    "        - lower than the 1th quantile to that quantile and\n",
    "        - upper than the 99th quantile to that quantile.\n",
    "\n",
    "    Main aim is to diminish negative effects of outlier values on analytical operations being performed.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import psutil, os, gc, time\n",
    "\n",
    "    print_if_verbose(\"HardEdgeReduction process has began:\\n\")\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    gc.collect()\n",
    "    mem_0 = proc.memory_info().rss\n",
    "    start_time = time.time()\n",
    "\n",
    "    epsilon = 0.0001  # for zero divisions\n",
    "\n",
    "    # Define boundaries that we will use for Reduction operation\n",
    "    df_outlier_cleaned = df.copy()\n",
    "\n",
    "    print_if_verbose(\n",
    "        \"Detected outliers will be replaced with edged quantiles/percentiles: 1% and 99%!\\n\"\n",
    "    )\n",
    "    print_if_verbose(\"Total number of rows is: %s\\n\" % df_outlier_cleaned.shape[0])\n",
    "\n",
    "    outlier_boundries_dict = {}\n",
    "\n",
    "    for col in numerical_columns:\n",
    "\n",
    "        if col in sparse_columns:\n",
    "            nonsparse_data = pd.DataFrame(\n",
    "                df_outlier_cleaned[\n",
    "                    df_outlier_cleaned[col] != df_outlier_cleaned[col].mode()[0]\n",
    "                ][col]\n",
    "            )\n",
    "\n",
    "            if (  # For lower threshold (left-hand-side)\n",
    "                nonsparse_data[col].quantile(lower_quantile)\n",
    "                < df_outlier_cleaned[col].mode()[0]\n",
    "            ):  # Unexpected case\n",
    "                lower_bound_sparse = nonsparse_data[col].quantile(lower_quantile)\n",
    "            else:\n",
    "                lower_bound_sparse = df_outlier_cleaned[col].mode()[0]\n",
    "\n",
    "            if (  # For upper threshold (right-hand-side)\n",
    "                nonsparse_data[col].quantile(upper_quantile)\n",
    "                < df_outlier_cleaned[col].mode()[0]\n",
    "            ):  # Unexpected case\n",
    "                upper_bound_sparse = df_outlier_cleaned[col].mode()[0]\n",
    "            else:\n",
    "                upper_bound_sparse = nonsparse_data[col].quantile(upper_quantile)\n",
    "\n",
    "            outlier_boundries_dict[col] = (lower_bound_sparse, upper_bound_sparse)\n",
    "\n",
    "            # Inform user about the cardinality of Outlier existence:\n",
    "            number_of_outliers = len(\n",
    "                df_outlier_cleaned[\n",
    "                    (df_outlier_cleaned[col] < lower_bound_sparse)\n",
    "                    | (df_outlier_cleaned[col] > upper_bound_sparse)\n",
    "                ][col]\n",
    "            )\n",
    "            print_if_verbose(\n",
    "                \"Sparse: Outlier number in {} is equal to: \".format(col),\n",
    "                round(\n",
    "                    number_of_outliers\n",
    "                    / (nonsparse_data.shape[0] - nonsparse_data.isnull().sum()),\n",
    "                    2,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Replace Outliers with Edges --> 1% and 99%:\n",
    "            if number_of_outliers > 0:\n",
    "\n",
    "                # Replace 'left-hand-side' outliers with its 1% quantile value\n",
    "                df_outlier_cleaned.loc[\n",
    "                    df_outlier_cleaned[col] < lower_bound_sparse, col\n",
    "                ] = (\n",
    "                    lower_bound_sparse - epsilon\n",
    "                )  # --> MAIN DF CHANGED\n",
    "\n",
    "                # Replace 'right-hand-side' outliers with its 99% quantile value\n",
    "                df_outlier_cleaned.loc[\n",
    "                    df_outlier_cleaned[col] > upper_bound_sparse, col\n",
    "                ] = (\n",
    "                    upper_bound_sparse + epsilon\n",
    "                )  # --> MAIN DF CHANGED\n",
    "\n",
    "        else:  # Find Edges:\n",
    "            number_of_outliers = len(\n",
    "                df_outlier_cleaned[\n",
    "                    (\n",
    "                        df_outlier_cleaned[col]\n",
    "                        < df_outlier_cleaned[col].quantile(lower_quantile)\n",
    "                    )\n",
    "                    | (\n",
    "                        df_outlier_cleaned[col]\n",
    "                        > df_outlier_cleaned[col].quantile(upper_quantile)\n",
    "                    )\n",
    "                ][col]\n",
    "            )\n",
    "            print_if_verbose(\n",
    "                \"Other: Outlier number in {} is equal to: \".format(col),\n",
    "                round(\n",
    "                    number_of_outliers / (df[col].shape[0] - df[col].isnull().sum()), 2\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Replace 'Standard' outliers:\n",
    "            if number_of_outliers > 0:\n",
    "                # Replace all outliers with its %99 quartile\n",
    "                lower_bound_sparse = df_outlier_cleaned[col].quantile(lower_quantile)\n",
    "                df_outlier_cleaned.loc[\n",
    "                    df_outlier_cleaned[col] < lower_bound_sparse, col\n",
    "                ] = (lower_bound_sparse - epsilon)\n",
    "\n",
    "                upper_bound_sparse = df_outlier_cleaned[col].quantile(upper_quantile)\n",
    "                df_outlier_cleaned.loc[\n",
    "                    df_outlier_cleaned[col] > upper_bound_sparse, col\n",
    "                ] = (upper_bound_sparse + epsilon)\n",
    "\n",
    "            outlier_boundries_dict[col] = (lower_bound_sparse, upper_bound_sparse)\n",
    "\n",
    "    print_if_verbose(\"HardEdgeReduction process has been completed!\")\n",
    "    print_if_verbose(\"--- in %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "\n",
    "    return df_outlier_cleaned, outlier_boundries_dict\n",
    "\n",
    "\n",
    "X_train, outlier_boundries_dict = HardEdgeReduction(\n",
    "    X_train, numerical_columns, sparse_columns\n",
    ")\n",
    "\n",
    "print_if_verbose(outlier_boundries_dict)\n",
    "\n",
    "##  Cleaning Outliers for Test Dataset\n",
    "\n",
    "epsilon = 0.0001  # for zero divisions\n",
    "\n",
    "# Define boundaries that we will use for Reduction operation\n",
    "upper_quantile, lower_quantile = 0.99, 0.01\n",
    "\n",
    "df_test_outlier_cleaned = X_test.copy()\n",
    "\n",
    "print_if_verbose(\n",
    "    \"Detected outliers being replaced with edged quantiles/percentiles: 1% and 99%!\"\n",
    ")\n",
    "print_if_verbose(\"Total number of rows is: %s\\n\" % df_test_outlier_cleaned.shape[0])\n",
    "\n",
    "for col in numerical_columns:\n",
    "    lower_bound = outlier_boundries_dict[col][0]\n",
    "    upper_bound = outlier_boundries_dict[col][1]\n",
    "\n",
    "    # Inform user about the cardinality of Outlier existence:\n",
    "    number_of_outliers = len(\n",
    "        df_test_outlier_cleaned[\n",
    "            (df_test_outlier_cleaned[col] < lower_bound)\n",
    "            | (df_test_outlier_cleaned[col] > upper_bound)\n",
    "        ][col]\n",
    "    )\n",
    "    print_if_verbose(\n",
    "        \"Outlier number in {} is equal to: \".format(col),\n",
    "        round(\n",
    "            number_of_outliers\n",
    "            / (\n",
    "                df_test_outlier_cleaned[col].shape[0]\n",
    "                - df_test_outlier_cleaned[col].isnull().sum()\n",
    "            ),\n",
    "            2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Replace Outliers with Edges --> 1% and 99%:\n",
    "    if number_of_outliers > 0:\n",
    "        # Replace 'left-hand-side' outliers with its 1% quantile value\n",
    "        df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] < lower_bound, col] = (\n",
    "            lower_bound - epsilon\n",
    "        )  # --> MAIN DF CHANGED\n",
    "\n",
    "        # Replace 'right-hand-side' outliers with its 99% quantile value\n",
    "        df_test_outlier_cleaned.loc[df_test_outlier_cleaned[col] > upper_bound, col] = (\n",
    "            upper_bound + epsilon\n",
    "        )  # --> MAIN DF CHANGED\n",
    "\n",
    "X_test = df_test_outlier_cleaned\n",
    "\n",
    "if verbose:\n",
    "    box_plot(\n",
    "        Y_train, numerical_columns, X_train\n",
    "    )  ## Visualization After Cleaning Outlier\n",
    "\n",
    "print(\"[INFO] Outlier Detection has been completed.\")\n",
    "###############################################\n",
    "\n",
    "######## Missing Value Imputation #############\n",
    "print(\"[INFO] Missing Value Imputation has begun.\")\n",
    "missing_values_info = df_info[\"%_Missing_Value\"]\n",
    "\n",
    "Zero_MR_variables_list = list(df_info[missing_values_info == 0].index)\n",
    "Low_MR_variables_list = list(\n",
    "    df_info[(missing_values_info > 0) & (missing_values_info <= 0.05)].index\n",
    ")\n",
    "Moderate_MR_variables_list = list(\n",
    "    df_info[(missing_values_info > 0.05) & (missing_values_info <= 0.275)].index\n",
    ")\n",
    "High_MR_variables_list = list(\n",
    "    df_info[(missing_values_info > 0.275) & (missing_values_info <= 0.50)].index\n",
    ")\n",
    "Extreme_MR_variables_list = list(\n",
    "    df_info[(missing_values_info > 0.50) & (missing_values_info <= 0.95)].index\n",
    ")\n",
    "Drop_MR_variables_list = list(df_info[missing_values_info > 0.95].index)\n",
    "\n",
    "print(\"[INFO] Number of Zero_MR_variables_list:\", len(Zero_MR_variables_list))\n",
    "print(\"[INFO] Number of Low_MR_variables_list:\", len(Low_MR_variables_list))\n",
    "print(\"[INFO] Number of Moderate_MR_variables_list:\", len(Moderate_MR_variables_list))\n",
    "print(\"[INFO] Number of High_MR_variables_list:\", len(High_MR_variables_list))\n",
    "print(\"[INFO] Number of Extreme_MR_variables_list:\", len(Extreme_MR_variables_list))\n",
    "\n",
    "\n",
    "assert len(Zero_MR_variables_list) + len(Low_MR_variables_list) + len(\n",
    "    Moderate_MR_variables_list\n",
    ") + len(High_MR_variables_list) + len(Extreme_MR_variables_list) == len(df_info)\n",
    "\n",
    "# Simple Imputer for Low Missing Values\n",
    "\n",
    "\n",
    "def SimpleImputer(df, data_info, variable_list):\n",
    "    for col in variable_list:\n",
    "\n",
    "        if col in numerical_columns:\n",
    "\n",
    "            print_if_verbose(\n",
    "                \"Total null values: {}\".format(df[[str(col)]].isnull().sum())\n",
    "            )\n",
    "\n",
    "            average = float(df[col].mean())\n",
    "            std = float(df[col].std())\n",
    "            count_nan = int(df[col].isnull().sum())\n",
    "            rand = np.random.normal(loc=average, scale=std, size=count_nan)\n",
    "            slice_col = pd.Series(df[col].copy())\n",
    "            slice_col[pd.isnull(slice_col)] = rand\n",
    "            df[col] = slice_col\n",
    "\n",
    "            print_if_verbose(\"Numerical variable {} have been imputed.\".format(col))\n",
    "\n",
    "        else:\n",
    "\n",
    "            print_if_verbose(\n",
    "                \"Total null values: {}\".format(df[[str(col)]].isnull().sum())\n",
    "            )\n",
    "            df.loc[df.loc[:, col].isnull(), col] = np.random.choice(\n",
    "                sorted(list(df.loc[:, col].dropna().unique())),\n",
    "                size=int(df.loc[df.loc[:, col].isnull(), col].shape[0]),\n",
    "                p=[\n",
    "                    pd.Series(\n",
    "                        df.groupby(col).size() / df.loc[:, col].dropna().shape[0]\n",
    "                    ).iloc[i]\n",
    "                    for i in np.arange(0, len(df.loc[:, col].dropna().unique()))\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            print_if_verbose(\"Categorical variable {} have been imputed.\".format(col))\n",
    "\n",
    "\n",
    "print_if_verbose(Low_MR_variables_list)\n",
    "\n",
    "SimpleImputer(X_train, df_info, Low_MR_variables_list)\n",
    "SimpleImputer(X_test, df_info, Low_MR_variables_list)\n",
    "\n",
    "print_if_verbose(MissingUniqueStatistics(X_train.loc[:, Low_MR_variables_list]))\n",
    "print_if_verbose(MissingUniqueStatistics(X_test.loc[:, Low_MR_variables_list]))\n",
    "\n",
    "print(\"[INFO] Missing Value Imputation has been completed.\")\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"######## Categorical Value Encodings #########\\n### MEAN ENCODING\\n\\nclass KFoldTargetEncoderTrain(base.BaseEstimator,\\n                               base.TransformerMixin):\\n    def __init__(self,colnames,targetName,\\n                  n_fold=5, verbosity=True,\\n                  discardOriginal_col=False):\\n        self.colnames = colnames\\n        self.targetName = targetName\\n        self.n_fold = n_fold\\n        self.verbosity = verbosity\\n        self.discardOriginal_col = discardOriginal_col\\n    \\n    def fit(self, X, y=None):\\n        return self\\n    \\n    def transform(self,X):\\n        assert(type(self.targetName) == str)\\n        assert(type(self.colnames) == str)\\n        assert(self.colnames in X.columns)\\n        assert(self.targetName in X.columns)\\n        \\n        mean_of_target = X[self.targetName].mean()\\n        kf = KFold(n_splits = self.n_fold,\\n                   shuffle = True, random_state=42)\\n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\\n        X[col_mean_name] = np.nan\\n        for tr_ind, val_ind in kf.split(X):\\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\\n            X.loc[X.index[val_ind], col_mean_name] = \\\\\\n            X_val[self.colnames].map(X_tr.groupby(self.colnames)\\n                                     [self.targetName].mean())\\n            X[col_mean_name].fillna(mean_of_target, inplace = True)\\n        if self.verbosity:\\n            encoded_feature = X[col_mean_name].values\\n            print_if_verbose('Correlation between the new feature, {} and, {} is {}.'\\\\\\n                  .format(col_mean_name,self.targetName,\\n                          np.corrcoef(X[self.targetName].values,\\n                                      encoded_feature)[0][1]))\\n        if self.discardOriginal_col:\\n            X = X.drop(self.targetName, axis=1)\\n        return X\\ndef StringConverterTrain(df,target_name,variable_list):\\n    for col in variable_list:\\n        targetc = KFoldTargetEncoderTrain(col,target_name,n_fold=4)\\n        new_train = targetc.fit_transform(df)\\n    return new_train\\n\\n\\nnominal_variable = list(df_info[df_info[\\\"Variable_Type\\\"] == \\\"Nominal\\\"].index)\\nnominal_lst = [\\n    item\\n    for item in Moderate_MR_variables_list\\n    + High_MR_variables_list\\n    + Extreme_MR_variables_list\\n    if item in nominal_variable\\n]\\nif \\\"OFFER_STATUS\\\" in nominal_lst:\\n    nominal_lst.remove(\\\"OFFER_STATUS\\\")\\n\\nnominal_lst\\n\\ndf_trial = pd.concat([X_train, Y_train], axis=1).copy()\\ndf_output_train = StringConverterTrain(\\n    df=df_trial, target_name=\\\"OFFER_STATUS\\\", variable_list=nominal_lst\\n)\\n\\nfor item in nominal_lst:\\n    print_if_verbose(df_output_train.loc[:, [item + \\\"_Kfold_Target_Enc\\\"]].isnull().sum())\\n\\nfor item in nominal_lst:\\n    X_train[item] = df_output_train[item + \\\"_Kfold_Target_Enc\\\"]\\n\\n### Mean Encoding for nominal variables(non missing value)\\n\\ndf_encoding = pd.concat([X_train, Y_train], axis=1).copy()\\ncategorical_non_missing_columns = type_separator(df_encoding[get_non_null_columns(df_encoding)])[\\\"categorical\\\"]\\n\\nif \\\"OFFER_STATUS\\\" in categorical_non_missing_columns:\\n    categorical_non_missing_columns.remove(\\\"OFFER_STATUS\\\")\\nprint_if_verbose(categorical_non_missing_columns)\\n\\ndf_encoding_train = StringConverterTrain(\\n    df=df_encoding,\\n    target_name=\\\"OFFER_STATUS\\\",\\n    variable_list=categorical_non_missing_columns,\\n)\\n\\nfor item in categorical_non_missing_columns:\\n    X_train[item] = df_encoding_train[item + \\\"_Kfold_Target_Enc\\\"]\\n\\nfor item in categorical_non_missing_columns:\\n    print_if_verbose(df_encoding_train.loc[:, [item + \\\"_Kfold_Target_Enc\\\"]].isnull().sum())\\n\\nMissingUniqueStatistics(X_train)\\n\\n# **String Converter for Test Dataset**\\n\\ndf_output_test = X_test.copy()\\nmean_of_target = df_output_train[\\\"OFFER_STATUS\\\"].copy().mean()\\ntarget_mean_list = nominal_lst\\nfor col in target_mean_list:\\n    df_output_test[col] = df_output_test[col].map(\\n        df_output_train.groupby(col)[col + \\\"_Kfold_Target_Enc\\\"].mean()\\n    )\\n    df_output_test[col].fillna(mean_of_target, inplace=True)\\n\\nfor item in nominal_lst:\\n    print_if_verbose(df_output_test.loc[:, [item]].isnull().sum())\\n\\nX_test[nominal_lst] = df_output_test[nominal_lst]\\n\\nX_test_encoder = X_test.copy()\\nmean_of_target = df_encoding_train[\\\"OFFER_STATUS\\\"].copy().mean()\\ntarget_mean_list = categorical_non_missing_columns\\nfor col in target_mean_list:\\n    X_test_encoder[col + \\\"_Kfold_Target_Enc\\\"] = X_test_encoder[col].map(\\n        df_encoding_train.groupby(col)[col + \\\"_Kfold_Target_Enc\\\"].mean()\\n    )\\n    X_test_encoder[col + \\\"_Kfold_Target_Enc\\\"].fillna(mean_of_target, inplace=True)\\n\\nfor item in categorical_non_missing_columns:\\n    X_test[item] = X_test_encoder[item + \\\"_Kfold_Target_Enc\\\"]\\n\\nMissingUniqueStatistics(X_test[categorical_non_missing_columns])\\n\\n## Modal Based Imputation\\n\\ndef MBI(df, columns, train_or_test, lst_numerical):\\n\\n    data_binary_encoded = df.copy()\\n    le = LabelEncoder()\\n\\n    if columns:\\n        for col in columns:\\n            if train_or_test == \\\"test\\\":\\n                le.fit(X_train[col].copy().astype(str))\\n                data_binary_encoded[col] = le.transform(df[col].copy().astype(str))\\n            else:\\n                data_binary_encoded[col] = le.fit_transform(df[col].copy().astype(str))\\n\\n    data_scaled = data_binary_encoded.copy()\\n\\n    for col in numerical_columns:\\n\\n        scaler = StandardScaler()\\n\\n        if train_or_test == \\\"test\\\":\\n\\n            scaler.fit(np.array(X_train.loc[:, col]).reshape(-1, 1))\\n            data_scaled.loc[:, col] = scaler.transform(\\n                np.array(data_scaled.loc[:, col]).reshape(-1, 1)\\n            )\\n\\n        else:\\n            data_scaled.loc[:, col] = scaler.fit_transform(\\n                np.array(data_scaled.loc[:, col]).reshape(-1, 1)\\n            )\\n\\n    for col in lst_numerical:\\n\\n        target_dropped_fullcases = (\\n            data_scaled.drop(col, axis=1)\\n            .loc[\\n                :,\\n                list(\\n                    set(Zero_MR_variables_list + Low_MR_variables_list)\\n                    - set([\\\"CLASS\\\", \\\"KEY\\\", \\\"CLNHGVS\\\"])\\n                ),\\n            ]\\n            .copy()\\n        )\\n\\n        target = data_scaled.loc[:, col]\\n        null_mask = target.isna()\\n        print_if_verbose(col)\\n\\n        if col in numerical_columns:\\n\\n            mlp = MLPRegressor(\\n                hidden_layer_sizes=(\\n                    100,\\n                    10,\\n                ),\\n                activation=\\\"tanh\\\",\\n                solver=\\\"adam\\\",\\n                learning_rate=\\\"adaptive\\\",\\n                max_iter=1000,\\n                learning_rate_init=0.01,\\n                alpha=0.01,\\n                early_stopping=False,\\n            )\\n        else:\\n            mlp = MLPClassifier(\\n                hidden_layer_sizes=(\\n                    100,\\n                    10,\\n                ),\\n                activation=\\\"tanh\\\",\\n                solver=\\\"adam\\\",\\n                learning_rate=\\\"adaptive\\\",\\n                max_iter=1000,\\n                learning_rate_init=0.01,\\n                alpha=0.01,\\n                early_stopping=False,\\n            )\\n\\n        mlp.fit(target_dropped_fullcases[~null_mask], target[~null_mask])\\n        data_scaled.loc[null_mask, col] = mlp.predict(\\n            target_dropped_fullcases[null_mask]\\n        )\\n\\n    print_if_verbose(data_scaled.loc[:, lst_numerical].isnull().sum())\\n    return data_scaled\\n\\nmoderate_numerical_variables = [\\n    item for item in Moderate_MR_variables_list if item in numerical_columns\\n]\";\n",
       "                var nbb_formatted_code = \"######## Categorical Value Encodings #########\\n### MEAN ENCODING\\n\\n\\nclass KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\\n    def __init__(\\n        self, colnames, targetName, n_fold=5, verbosity=True, discardOriginal_col=False\\n    ):\\n        self.colnames = colnames\\n        self.targetName = targetName\\n        self.n_fold = n_fold\\n        self.verbosity = verbosity\\n        self.discardOriginal_col = discardOriginal_col\\n\\n    def fit(self, X, y=None):\\n        return self\\n\\n    def transform(self, X):\\n        assert type(self.targetName) == str\\n        assert type(self.colnames) == str\\n        assert self.colnames in X.columns\\n        assert self.targetName in X.columns\\n\\n        mean_of_target = X[self.targetName].mean()\\n        kf = KFold(n_splits=self.n_fold, shuffle=True, random_state=42)\\n        col_mean_name = self.colnames + \\\"_\\\" + \\\"Kfold_Target_Enc\\\"\\n        X[col_mean_name] = np.nan\\n        for tr_ind, val_ind in kf.split(X):\\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\\n            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(\\n                X_tr.groupby(self.colnames)[self.targetName].mean()\\n            )\\n            X[col_mean_name].fillna(mean_of_target, inplace=True)\\n        if self.verbosity:\\n            encoded_feature = X[col_mean_name].values\\n            print_if_verbose(\\n                \\\"Correlation between the new feature, {} and, {} is {}.\\\".format(\\n                    col_mean_name,\\n                    self.targetName,\\n                    np.corrcoef(X[self.targetName].values, encoded_feature)[0][1],\\n                )\\n            )\\n        if self.discardOriginal_col:\\n            X = X.drop(self.targetName, axis=1)\\n        return X\\n\\n\\ndef StringConverterTrain(df, target_name, variable_list):\\n    for col in variable_list:\\n        targetc = KFoldTargetEncoderTrain(col, target_name, n_fold=4)\\n        new_train = targetc.fit_transform(df)\\n    return new_train\\n\\n\\nnominal_variable = list(df_info[df_info[\\\"Variable_Type\\\"] == \\\"Nominal\\\"].index)\\nnominal_lst = [\\n    item\\n    for item in Moderate_MR_variables_list\\n    + High_MR_variables_list\\n    + Extreme_MR_variables_list\\n    if item in nominal_variable\\n]\\nif \\\"OFFER_STATUS\\\" in nominal_lst:\\n    nominal_lst.remove(\\\"OFFER_STATUS\\\")\\n\\nnominal_lst\\n\\ndf_trial = pd.concat([X_train, Y_train], axis=1).copy()\\ndf_output_train = StringConverterTrain(\\n    df=df_trial, target_name=\\\"OFFER_STATUS\\\", variable_list=nominal_lst\\n)\\n\\nfor item in nominal_lst:\\n    print_if_verbose(\\n        df_output_train.loc[:, [item + \\\"_Kfold_Target_Enc\\\"]].isnull().sum()\\n    )\\n\\nfor item in nominal_lst:\\n    X_train[item] = df_output_train[item + \\\"_Kfold_Target_Enc\\\"]\\n\\n### Mean Encoding for nominal variables(non missing value)\\n\\ndf_encoding = pd.concat([X_train, Y_train], axis=1).copy()\\ncategorical_non_missing_columns = type_separator(\\n    df_encoding[get_non_null_columns(df_encoding)]\\n)[\\\"categorical\\\"]\\n\\nif \\\"OFFER_STATUS\\\" in categorical_non_missing_columns:\\n    categorical_non_missing_columns.remove(\\\"OFFER_STATUS\\\")\\nprint_if_verbose(categorical_non_missing_columns)\\n\\ndf_encoding_train = StringConverterTrain(\\n    df=df_encoding,\\n    target_name=\\\"OFFER_STATUS\\\",\\n    variable_list=categorical_non_missing_columns,\\n)\\n\\nfor item in categorical_non_missing_columns:\\n    X_train[item] = df_encoding_train[item + \\\"_Kfold_Target_Enc\\\"]\\n\\nfor item in categorical_non_missing_columns:\\n    print_if_verbose(\\n        df_encoding_train.loc[:, [item + \\\"_Kfold_Target_Enc\\\"]].isnull().sum()\\n    )\\n\\nMissingUniqueStatistics(X_train)\\n\\n# **String Converter for Test Dataset**\\n\\ndf_output_test = X_test.copy()\\nmean_of_target = df_output_train[\\\"OFFER_STATUS\\\"].copy().mean()\\ntarget_mean_list = nominal_lst\\nfor col in target_mean_list:\\n    df_output_test[col] = df_output_test[col].map(\\n        df_output_train.groupby(col)[col + \\\"_Kfold_Target_Enc\\\"].mean()\\n    )\\n    df_output_test[col].fillna(mean_of_target, inplace=True)\\n\\nfor item in nominal_lst:\\n    print_if_verbose(df_output_test.loc[:, [item]].isnull().sum())\\n\\nX_test[nominal_lst] = df_output_test[nominal_lst]\\n\\nX_test_encoder = X_test.copy()\\nmean_of_target = df_encoding_train[\\\"OFFER_STATUS\\\"].copy().mean()\\ntarget_mean_list = categorical_non_missing_columns\\nfor col in target_mean_list:\\n    X_test_encoder[col + \\\"_Kfold_Target_Enc\\\"] = X_test_encoder[col].map(\\n        df_encoding_train.groupby(col)[col + \\\"_Kfold_Target_Enc\\\"].mean()\\n    )\\n    X_test_encoder[col + \\\"_Kfold_Target_Enc\\\"].fillna(mean_of_target, inplace=True)\\n\\nfor item in categorical_non_missing_columns:\\n    X_test[item] = X_test_encoder[item + \\\"_Kfold_Target_Enc\\\"]\\n\\nMissingUniqueStatistics(X_test[categorical_non_missing_columns])\\n\\n## Modal Based Imputation\\n\\n\\ndef MBI(df, columns, train_or_test, lst_numerical):\\n\\n    data_binary_encoded = df.copy()\\n    le = LabelEncoder()\\n\\n    if columns:\\n        for col in columns:\\n            if train_or_test == \\\"test\\\":\\n                le.fit(X_train[col].copy().astype(str))\\n                data_binary_encoded[col] = le.transform(df[col].copy().astype(str))\\n            else:\\n                data_binary_encoded[col] = le.fit_transform(df[col].copy().astype(str))\\n\\n    data_scaled = data_binary_encoded.copy()\\n\\n    for col in numerical_columns:\\n\\n        scaler = StandardScaler()\\n\\n        if train_or_test == \\\"test\\\":\\n\\n            scaler.fit(np.array(X_train.loc[:, col]).reshape(-1, 1))\\n            data_scaled.loc[:, col] = scaler.transform(\\n                np.array(data_scaled.loc[:, col]).reshape(-1, 1)\\n            )\\n\\n        else:\\n            data_scaled.loc[:, col] = scaler.fit_transform(\\n                np.array(data_scaled.loc[:, col]).reshape(-1, 1)\\n            )\\n\\n    for col in lst_numerical:\\n\\n        target_dropped_fullcases = (\\n            data_scaled.drop(col, axis=1)\\n            .loc[\\n                :,\\n                list(\\n                    set(Zero_MR_variables_list + Low_MR_variables_list)\\n                    - set([\\\"CLASS\\\", \\\"KEY\\\", \\\"CLNHGVS\\\"])\\n                ),\\n            ]\\n            .copy()\\n        )\\n\\n        target = data_scaled.loc[:, col]\\n        null_mask = target.isna()\\n        print_if_verbose(col)\\n\\n        if col in numerical_columns:\\n\\n            mlp = MLPRegressor(\\n                hidden_layer_sizes=(\\n                    100,\\n                    10,\\n                ),\\n                activation=\\\"tanh\\\",\\n                solver=\\\"adam\\\",\\n                learning_rate=\\\"adaptive\\\",\\n                max_iter=1000,\\n                learning_rate_init=0.01,\\n                alpha=0.01,\\n                early_stopping=False,\\n            )\\n        else:\\n            mlp = MLPClassifier(\\n                hidden_layer_sizes=(\\n                    100,\\n                    10,\\n                ),\\n                activation=\\\"tanh\\\",\\n                solver=\\\"adam\\\",\\n                learning_rate=\\\"adaptive\\\",\\n                max_iter=1000,\\n                learning_rate_init=0.01,\\n                alpha=0.01,\\n                early_stopping=False,\\n            )\\n\\n        mlp.fit(target_dropped_fullcases[~null_mask], target[~null_mask])\\n        data_scaled.loc[null_mask, col] = mlp.predict(\\n            target_dropped_fullcases[null_mask]\\n        )\\n\\n    print_if_verbose(data_scaled.loc[:, lst_numerical].isnull().sum())\\n    return data_scaled\\n\\n\\nmoderate_numerical_variables = [\\n    item for item in Moderate_MR_variables_list if item in numerical_columns\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## Categorical Value Encodings #########\n",
    "### MEAN ENCODING\n",
    "\n",
    "class KFoldTargetEncoderTrain(base.BaseEstimator,\n",
    "                               base.TransformerMixin):\n",
    "    def __init__(self,colnames,targetName,\n",
    "                  n_fold=5, verbosity=True,\n",
    "                  discardOriginal_col=False):\n",
    "        self.colnames = colnames\n",
    "        self.targetName = targetName\n",
    "        self.n_fold = n_fold\n",
    "        self.verbosity = verbosity\n",
    "        self.discardOriginal_col = discardOriginal_col\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        assert(type(self.targetName) == str)\n",
    "        assert(type(self.colnames) == str)\n",
    "        assert(self.colnames in X.columns)\n",
    "        assert(self.targetName in X.columns)\n",
    "        \n",
    "        mean_of_target = X[self.targetName].mean()\n",
    "        kf = KFold(n_splits = self.n_fold,\n",
    "                   shuffle = True, random_state=42)\n",
    "        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n",
    "        X[col_mean_name] = np.nan\n",
    "        for tr_ind, val_ind in kf.split(X):\n",
    "            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n",
    "            X.loc[X.index[val_ind], col_mean_name] = \\\n",
    "            X_val[self.colnames].map(X_tr.groupby(self.colnames)\n",
    "                                     [self.targetName].mean())\n",
    "            X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
    "        if self.verbosity:\n",
    "            encoded_feature = X[col_mean_name].values\n",
    "            print_if_verbose('Correlation between the new feature, {} and, {} is {}.'\\\n",
    "                  .format(col_mean_name,self.targetName,\n",
    "                          np.corrcoef(X[self.targetName].values,\n",
    "                                      encoded_feature)[0][1]))\n",
    "        if self.discardOriginal_col:\n",
    "            X = X.drop(self.targetName, axis=1)\n",
    "        return X\n",
    "def StringConverterTrain(df,target_name,variable_list):\n",
    "    for col in variable_list:\n",
    "        targetc = KFoldTargetEncoderTrain(col,target_name,n_fold=4)\n",
    "        new_train = targetc.fit_transform(df)\n",
    "    return new_train\n",
    "\n",
    "\n",
    "nominal_variable = list(df_info[df_info[\"Variable_Type\"] == \"Nominal\"].index)\n",
    "nominal_lst = [\n",
    "    item\n",
    "    for item in Moderate_MR_variables_list\n",
    "    + High_MR_variables_list\n",
    "    + Extreme_MR_variables_list\n",
    "    if item in nominal_variable\n",
    "]\n",
    "if \"OFFER_STATUS\" in nominal_lst:\n",
    "    nominal_lst.remove(\"OFFER_STATUS\")\n",
    "\n",
    "nominal_lst\n",
    "\n",
    "df_trial = pd.concat([X_train, Y_train], axis=1).copy()\n",
    "df_output_train = StringConverterTrain(\n",
    "    df=df_trial, target_name=\"OFFER_STATUS\", variable_list=nominal_lst\n",
    ")\n",
    "\n",
    "for item in nominal_lst:\n",
    "    print_if_verbose(df_output_train.loc[:, [item + \"_Kfold_Target_Enc\"]].isnull().sum())\n",
    "\n",
    "for item in nominal_lst:\n",
    "    X_train[item] = df_output_train[item + \"_Kfold_Target_Enc\"]\n",
    "\n",
    "### Mean Encoding for nominal variables(non missing value)\n",
    "\n",
    "df_encoding = pd.concat([X_train, Y_train], axis=1).copy()\n",
    "categorical_non_missing_columns = type_separator(df_encoding[get_non_null_columns(df_encoding)])[\"categorical\"]\n",
    "\n",
    "if \"OFFER_STATUS\" in categorical_non_missing_columns:\n",
    "    categorical_non_missing_columns.remove(\"OFFER_STATUS\")\n",
    "print_if_verbose(categorical_non_missing_columns)\n",
    "\n",
    "df_encoding_train = StringConverterTrain(\n",
    "    df=df_encoding,\n",
    "    target_name=\"OFFER_STATUS\",\n",
    "    variable_list=categorical_non_missing_columns,\n",
    ")\n",
    "\n",
    "for item in categorical_non_missing_columns:\n",
    "    X_train[item] = df_encoding_train[item + \"_Kfold_Target_Enc\"]\n",
    "\n",
    "for item in categorical_non_missing_columns:\n",
    "    print_if_verbose(df_encoding_train.loc[:, [item + \"_Kfold_Target_Enc\"]].isnull().sum())\n",
    "\n",
    "MissingUniqueStatistics(X_train)\n",
    "\n",
    "# **String Converter for Test Dataset**\n",
    "\n",
    "df_output_test = X_test.copy()\n",
    "mean_of_target = df_output_train[\"OFFER_STATUS\"].copy().mean()\n",
    "target_mean_list = nominal_lst\n",
    "for col in target_mean_list:\n",
    "    df_output_test[col] = df_output_test[col].map(\n",
    "        df_output_train.groupby(col)[col + \"_Kfold_Target_Enc\"].mean()\n",
    "    )\n",
    "    df_output_test[col].fillna(mean_of_target, inplace=True)\n",
    "\n",
    "for item in nominal_lst:\n",
    "    print_if_verbose(df_output_test.loc[:, [item]].isnull().sum())\n",
    "\n",
    "X_test[nominal_lst] = df_output_test[nominal_lst]\n",
    "\n",
    "X_test_encoder = X_test.copy()\n",
    "mean_of_target = df_encoding_train[\"OFFER_STATUS\"].copy().mean()\n",
    "target_mean_list = categorical_non_missing_columns\n",
    "for col in target_mean_list:\n",
    "    X_test_encoder[col + \"_Kfold_Target_Enc\"] = X_test_encoder[col].map(\n",
    "        df_encoding_train.groupby(col)[col + \"_Kfold_Target_Enc\"].mean()\n",
    "    )\n",
    "    X_test_encoder[col + \"_Kfold_Target_Enc\"].fillna(mean_of_target, inplace=True)\n",
    "\n",
    "for item in categorical_non_missing_columns:\n",
    "    X_test[item] = X_test_encoder[item + \"_Kfold_Target_Enc\"]\n",
    "\n",
    "MissingUniqueStatistics(X_test[categorical_non_missing_columns])\n",
    "\n",
    "## Modal Based Imputation\n",
    "\n",
    "def MBI(df, columns, train_or_test, lst_numerical):\n",
    "\n",
    "    data_binary_encoded = df.copy()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    if columns:\n",
    "        for col in columns:\n",
    "            if train_or_test == \"test\":\n",
    "                le.fit(X_train[col].copy().astype(str))\n",
    "                data_binary_encoded[col] = le.transform(df[col].copy().astype(str))\n",
    "            else:\n",
    "                data_binary_encoded[col] = le.fit_transform(df[col].copy().astype(str))\n",
    "\n",
    "    data_scaled = data_binary_encoded.copy()\n",
    "\n",
    "    for col in numerical_columns:\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        if train_or_test == \"test\":\n",
    "\n",
    "            scaler.fit(np.array(X_train.loc[:, col]).reshape(-1, 1))\n",
    "            data_scaled.loc[:, col] = scaler.transform(\n",
    "                np.array(data_scaled.loc[:, col]).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            data_scaled.loc[:, col] = scaler.fit_transform(\n",
    "                np.array(data_scaled.loc[:, col]).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "    for col in lst_numerical:\n",
    "\n",
    "        target_dropped_fullcases = (\n",
    "            data_scaled.drop(col, axis=1)\n",
    "            .loc[\n",
    "                :,\n",
    "                list(\n",
    "                    set(Zero_MR_variables_list + Low_MR_variables_list)\n",
    "                    - set([\"CLASS\", \"KEY\", \"CLNHGVS\"])\n",
    "                ),\n",
    "            ]\n",
    "            .copy()\n",
    "        )\n",
    "\n",
    "        target = data_scaled.loc[:, col]\n",
    "        null_mask = target.isna()\n",
    "        print_if_verbose(col)\n",
    "\n",
    "        if col in numerical_columns:\n",
    "\n",
    "            mlp = MLPRegressor(\n",
    "                hidden_layer_sizes=(\n",
    "                    100,\n",
    "                    10,\n",
    "                ),\n",
    "                activation=\"tanh\",\n",
    "                solver=\"adam\",\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=1000,\n",
    "                learning_rate_init=0.01,\n",
    "                alpha=0.01,\n",
    "                early_stopping=False,\n",
    "            )\n",
    "        else:\n",
    "            mlp = MLPClassifier(\n",
    "                hidden_layer_sizes=(\n",
    "                    100,\n",
    "                    10,\n",
    "                ),\n",
    "                activation=\"tanh\",\n",
    "                solver=\"adam\",\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=1000,\n",
    "                learning_rate_init=0.01,\n",
    "                alpha=0.01,\n",
    "                early_stopping=False,\n",
    "            )\n",
    "\n",
    "        mlp.fit(target_dropped_fullcases[~null_mask], target[~null_mask])\n",
    "        data_scaled.loc[null_mask, col] = mlp.predict(\n",
    "            target_dropped_fullcases[null_mask]\n",
    "        )\n",
    "\n",
    "    print_if_verbose(data_scaled.loc[:, lst_numerical].isnull().sum())\n",
    "    return data_scaled\n",
    "\n",
    "moderate_numerical_variables = [\n",
    "    item for item in Moderate_MR_variables_list if item in numerical_columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"moderate_numerical_variables\\n\\nassert (\\n    \\\"categorical\\\" not in type_separator(X_train)\\n    or not type_separator(X_train)[\\\"categorical\\\"]\\n), print_if_verbose(\\\"All categorical variables should have encoded, check encodings!\\\")\\nassert (\\n    \\\"categorical\\\" not in type_separator(X_test)\\n    or not type_separator(X_test)[\\\"categorical\\\"]\\n), print_if_verbose(\\\"All categorical variables should have encoded, check encodings!\\\")\\n\\nX_train_scaled = MBI(X_train, None, \\\"train\\\", moderate_numerical_variables)\\n\\nX_train_scaled\\n\\nX_test_scaled = MBI(X_test, None, \\\"test\\\", moderate_numerical_variables)\\n\\nX_test_scaled\\n\\nMissingUniqueStatistics(X_train_scaled)\\n\\nassert get_null_columns(X_train_scaled) == [], print_if_verbose(\\n    \\\"There are some missing values left!\\\"\\n)\\nassert get_null_columns(X_test_scaled) == [], print_if_verbose(\\n    \\\"There are some missing values left!\\\"\\n)\\n\\n\\n###############################################\";\n",
       "                var nbb_formatted_code = \"moderate_numerical_variables\\n\\nassert (\\n    \\\"categorical\\\" not in type_separator(X_train)\\n    or not type_separator(X_train)[\\\"categorical\\\"]\\n), print_if_verbose(\\\"All categorical variables should have encoded, check encodings!\\\")\\nassert (\\n    \\\"categorical\\\" not in type_separator(X_test)\\n    or not type_separator(X_test)[\\\"categorical\\\"]\\n), print_if_verbose(\\\"All categorical variables should have encoded, check encodings!\\\")\\n\\nX_train_scaled = MBI(X_train, None, \\\"train\\\", moderate_numerical_variables)\\n\\nX_train_scaled\\n\\nX_test_scaled = MBI(X_test, None, \\\"test\\\", moderate_numerical_variables)\\n\\nX_test_scaled\\n\\nMissingUniqueStatistics(X_train_scaled)\\n\\nassert get_null_columns(X_train_scaled) == [], print_if_verbose(\\n    \\\"There are some missing values left!\\\"\\n)\\nassert get_null_columns(X_test_scaled) == [], print_if_verbose(\\n    \\\"There are some missing values left!\\\"\\n)\\n\\n\\n###############################################\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moderate_numerical_variables\n",
    "\n",
    "assert (\n",
    "    \"categorical\" not in type_separator(X_train)\n",
    "    or not type_separator(X_train)[\"categorical\"]\n",
    "), print_if_verbose(\"All categorical variables should have encoded, check encodings!\")\n",
    "assert (\n",
    "    \"categorical\" not in type_separator(X_test)\n",
    "    or not type_separator(X_test)[\"categorical\"]\n",
    "), print_if_verbose(\"All categorical variables should have encoded, check encodings!\")\n",
    "\n",
    "X_train_scaled = MBI(X_train, None, \"train\", moderate_numerical_variables)\n",
    "\n",
    "X_train_scaled\n",
    "\n",
    "X_test_scaled = MBI(X_test, None, \"test\", moderate_numerical_variables)\n",
    "\n",
    "X_test_scaled\n",
    "\n",
    "MissingUniqueStatistics(X_train_scaled)\n",
    "\n",
    "assert get_null_columns(X_train_scaled) == [], print_if_verbose(\n",
    "    \"There are some missing values left!\"\n",
    ")\n",
    "assert get_null_columns(X_test_scaled) == [], print_if_verbose(\n",
    "    \"There are some missing values left!\"\n",
    ")\n",
    "\n",
    "\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train_scaled, Y_train)\n",
    "\n",
    "features = X_train_scaled.columns\n",
    "importances = rnd_clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "feat_importances = pd.Series(importances, index=features)\n",
    "feat_importances.nlargest(len(indices)).plot(kind=\"bar\", color=\"#79CCB3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>58 mins 04 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_iceking_xb133p</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>31.74 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         58 mins 04 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    3 days\n",
       "H2O_cluster_name:           H2O_from_python_iceking_xb133p\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    31.74 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ----------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>58 mins 07 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_iceking_xb133p</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>31.74 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         58 mins 07 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    3 days\n",
       "H2O_cluster_name:           H2O_from_python_iceking_xb133p\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    31.74 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ----------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model will be saved here:\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/model_withALL\"\n",
      "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n",
      "[INFO] Timestamp: 2022-01-29_18:21:15\n",
      "[INFO] AML Leaderboard "
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                                </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_2_AutoML_17_20220129_182118   </td><td style=\"text-align: right;\">0.809839</td><td style=\"text-align: right;\"> 0.385342</td><td style=\"text-align: right;\">0.935896</td><td style=\"text-align: right;\">              0.380876</td><td style=\"text-align: right;\">0.345134</td><td style=\"text-align: right;\">0.119118</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_3_AutoML_17_20220129_182118   </td><td style=\"text-align: right;\">0.809716</td><td style=\"text-align: right;\"> 0.384934</td><td style=\"text-align: right;\">0.936061</td><td style=\"text-align: right;\">              0.372267</td><td style=\"text-align: right;\">0.344993</td><td style=\"text-align: right;\">0.11902 </td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_7_AutoML_17_20220129_182118   </td><td style=\"text-align: right;\">0.809687</td><td style=\"text-align: right;\"> 0.384936</td><td style=\"text-align: right;\">0.936212</td><td style=\"text-align: right;\">              0.376767</td><td style=\"text-align: right;\">0.345055</td><td style=\"text-align: right;\">0.119063</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_4_AutoML_17_20220129_182118   </td><td style=\"text-align: right;\">0.809365</td><td style=\"text-align: right;\"> 0.385508</td><td style=\"text-align: right;\">0.935867</td><td style=\"text-align: right;\">              0.371232</td><td style=\"text-align: right;\">0.345202</td><td style=\"text-align: right;\">0.119164</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_3_AutoML_17_20220129_182118</td><td style=\"text-align: right;\">0.809289</td><td style=\"text-align: right;\"> 0.386617</td><td style=\"text-align: right;\">0.935803</td><td style=\"text-align: right;\">              0.381496</td><td style=\"text-align: right;\">0.345501</td><td style=\"text-align: right;\">0.119371</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_1_AutoML_17_20220129_182118   </td><td style=\"text-align: right;\">0.808943</td><td style=\"text-align: right;\"> 0.385722</td><td style=\"text-align: right;\">0.935752</td><td style=\"text-align: right;\">              0.367009</td><td style=\"text-align: right;\">0.345463</td><td style=\"text-align: right;\">0.119345</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_4_AutoML_17_20220129_182118</td><td style=\"text-align: right;\">0.808859</td><td style=\"text-align: right;\"> 0.386183</td><td style=\"text-align: right;\">0.935928</td><td style=\"text-align: right;\">              0.370963</td><td style=\"text-align: right;\">0.345305</td><td style=\"text-align: right;\">0.119236</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_7_AutoML_17_20220129_182118</td><td style=\"text-align: right;\">0.808694</td><td style=\"text-align: right;\"> 0.386663</td><td style=\"text-align: right;\">0.935268</td><td style=\"text-align: right;\">              0.381686</td><td style=\"text-align: right;\">0.345341</td><td style=\"text-align: right;\">0.11926 </td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_6_AutoML_17_20220129_182118   </td><td style=\"text-align: right;\">0.807609</td><td style=\"text-align: right;\"> 0.385613</td><td style=\"text-align: right;\">0.935052</td><td style=\"text-align: right;\">              0.365562</td><td style=\"text-align: right;\">0.3459  </td><td style=\"text-align: right;\">0.119647</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_2_AutoML_17_20220129_182118</td><td style=\"text-align: right;\">0.807503</td><td style=\"text-align: right;\"> 0.387193</td><td style=\"text-align: right;\">0.935108</td><td style=\"text-align: right;\">              0.379517</td><td style=\"text-align: right;\">0.345892</td><td style=\"text-align: right;\">0.119641</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] RESULTS:\n",
      " > Train data saved to:\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/train.csv\".\n",
      " > Test data saved to :\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/test.csv\".\n",
      " > H20 Model saved to :\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/model_withALL\".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"resultt = apply_h2o(\\n    data=[X_train_scaled, X_test_scaled, Y_train, Y_test],\\n    max_runtime_secs=5 * 60 * 60,\\n    training_with_all=True,\\n)\";\n",
       "                var nbb_formatted_code = \"resultt = apply_h2o(\\n    data=[X_train_scaled, X_test_scaled, Y_train, Y_test],\\n    max_runtime_secs=5 * 60 * 60,\\n    training_with_all=True,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultt = apply_h2o(\n",
    "    data=[X_train_scaled, X_test_scaled, Y_train, Y_test],\n",
    "    max_runtime_secs=5 * 60 * 60,\n",
    "    training_with_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_auto_sklearn=False\n",
    "if use_auto_sklearn:\n",
    "    x_train_data = X_train_scaled.copy()\n",
    "    x_test_data = X_test_scaled.copy()\n",
    "\n",
    "    model2 = AutoSklearnClassifier(\n",
    "        time_left_for_this_task=2 * 60,\n",
    "        # time_left_for_this_task=60,\n",
    "        seed=42,\n",
    "        per_run_time_limit=30,\n",
    "        n_jobs=-1,\n",
    "        metric=autosklearn.metrics.balanced_accuracy,\n",
    "        memory_limit=10 * 3072,\n",
    "        # resampling_strategy=\"cv\",\n",
    "        # resampling_strategy_arguments={\"cv\": {\"folds\": 5}, \"shuffle\": True},\n",
    "        # resampling_strategy_arguments={\"cv\": {\"folds\": 10}, \"shuffle\": True},\n",
    "        # resampling_strategy_arguments={\"train_size\": 0.67, \"shuffle\": True},\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    model2.fit(x_train_data, Y_train)\n",
    "    end = time.time()\n",
    "    y_hat = model2.predict(x_test_data)\n",
    "    # print(model2.sprint_statistics())\n",
    "    # print(model2.best_val_acc)\n",
    "    best_val_score_here = float(model2.sprint_statistics().split(\"\\n\")[3][25:])\n",
    "    print(model2.get_models_with_weights())\n",
    "    # model2.leaderboard()\n",
    "\n",
    "\n",
    "    # Evaluate\n",
    "\n",
    "    print(\"Elapsed Time:\", end - start)\n",
    "    # print(\"Dropped until:\", col)\n",
    "    print(\"Best Val Score:\", best_val_score_here)\n",
    "    val_acc = balanced_accuracy_score(y_true=Y_test, y_pred=y_hat)\n",
    "    print(\"Final BAC Score: %.3f\" % val_acc)\n",
    "\n",
    "    # Pickle\n",
    "    if not ((\"best_val_acc\" in vars() or \"best_val_acc\" in globals())):\n",
    "        best_val_acc = val_acc\n",
    "    file_name = f\"model_pickles/model_{int(val_acc*10**3)}\" + str(\n",
    "        datetime.datetime.now()\n",
    "    ).replace(\" \", \"_\")\n",
    "    if val_acc >= best_val_acc:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(model2, f)\n",
    "    else:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gggggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_for_boruta = pd.concat([X_train_scaled, X_test_scaled]).copy()\n",
    "Y_for_boruta = pd.concat([Y_train, Y_test]).copy()\n",
    "# define random forest classifier\n",
    "forest = RandomForestClassifier(n_jobs=-1, class_weight=\"balanced\")\n",
    "forest.fit(X_for_boruta, Y_for_boruta)\n",
    "\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(\n",
    "    forest,\n",
    "    n_estimators=\"auto\",\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "feat_selector.fit(\n",
    "    X_for_boruta.to_numpy(), Y_for_boruta.to_numpy()\n",
    ")  # find all relevant features\n",
    "print(feat_selector.support_)  # check selected features\n",
    "print(feat_selector.ranking_)  # check ranking of features\n",
    "# check selected features\n",
    "a = list(\n",
    "    zip(\n",
    "        list(feat_selector.ranking_),\n",
    "        list(feat_selector.support_),\n",
    "        list(X_for_boruta.columns),\n",
    "    )\n",
    ")\n",
    "\n",
    "[\", # \".join(map(str, [f'{x[2]}\", # ', x[0], x[1]])) for x in sorted(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Val Score: 0.70742\n",
    "Final BAC Score: 0.713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real predictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>2 hours 30 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_iceking_xb133p</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>30.89 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.10 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         2 hours 30 mins\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.0.2\n",
       "H2O_cluster_version_age:    3 days\n",
       "H2O_cluster_name:           H2O_from_python_iceking_xb133p\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    30.89 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, Infogram, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.10 final\n",
       "--------------------------  ----------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "stackedensemble prediction progress: |███████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">      p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.18431 </td><td style=\"text-align: right;\">0.81569 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.320166</td><td style=\"text-align: right;\">0.679834</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.285917</td><td style=\"text-align: right;\">0.714083</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.258053</td><td style=\"text-align: right;\">0.741947</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.410835</td><td style=\"text-align: right;\">0.589165</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.300396</td><td style=\"text-align: right;\">0.699604</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.320632</td><td style=\"text-align: right;\">0.679368</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.382059</td><td style=\"text-align: right;\">0.617941</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.266544</td><td style=\"text-align: right;\">0.733456</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.170701</td><td style=\"text-align: right;\">0.829299</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1    2517\n",
      "0    59  \n",
      "Name: prediction, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26018</th>\n",
       "      <td>1</td>\n",
       "      <td>26019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26032</th>\n",
       "      <td>1</td>\n",
       "      <td>26033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26049</th>\n",
       "      <td>1</td>\n",
       "      <td>26050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26057</th>\n",
       "      <td>1</td>\n",
       "      <td>26058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26085</th>\n",
       "      <td>1</td>\n",
       "      <td>26086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2576 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction     id\n",
       "5      1           6    \n",
       "8      1           9    \n",
       "13     1           14   \n",
       "34     1           35   \n",
       "35     1           36   \n",
       "...   ..           ..   \n",
       "26018  1           26019\n",
       "26032  1           26033\n",
       "26049  1           26050\n",
       "26057  1           26058\n",
       "26085  1           26086\n",
       "\n",
       "[2576 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"def real_predict_h2o(models_dir_path):\\n    from h2o.automl import H2OAutoML\\n    import h2o\\n\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n\\n    my_model = h2o.load_model(models_dir_path)\\n    df = pd.read_csv(\\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\")\\n\\n    ppppp = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n    ppppp = X_test_scaled\\n    x_realll = h2o.H2OFrame(ppppp)\\n\\n    ggl = my_model.predict(x_realll)\\n    gg = ggl[0].as_data_frame().values.flatten()\\n\\n    real_test = ppppp\\n    real_test[\\\"prediction\\\"] = gg\\n    test_set_id_col = df[np.isnan(df[\\\"OFFER_STATUS\\\"])][\\\"TEST_SET_ID\\\"]\\n    real_test = pd.concat([real_test[\\\"prediction\\\"], test_set_id_col], axis=1)\\n    real_test = real_test.rename(columns={\\\"TEST_SET_ID\\\": \\\"id\\\"})\\n\\n    real_test[\\\"prediction\\\"] = real_test[\\\"prediction\\\"].astype(int)\\n    real_test[\\\"id\\\"] = real_test[\\\"id\\\"].astype(int)\\n    print(ggl)\\n\\n    return real_test\\n\\n\\nlllresult = real_predict_h2o(\\n    # Submission 5\\n    # models_dir_path=\\\"h2o_models_with_data/results_ts_2022-01-29_13:09:41/model_bac_0.734/GBM_1_AutoML_31_20220129_130944\\\"\\n    # Submission 7\\n    models_dir_path=\\\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/model_withALL/StackedEnsemble_AllModels_2_AutoML_17_20220129_182118\\\"\\n)\\n\\nprint(lllresult.prediction.value_counts())\\n\\nlllresult.to_csv(\\\"predictions_versed_chimpanzee_7.csv\\\", header=True, index=False)\\n\\nlllresult\";\n",
       "                var nbb_formatted_code = \"def real_predict_h2o(models_dir_path):\\n    from h2o.automl import H2OAutoML\\n    import h2o\\n\\n    h2o.init(max_mem_size=\\\"32G\\\")\\n\\n    my_model = h2o.load_model(models_dir_path)\\n    df = pd.read_csv(\\\"interim_data/df_completed_1_2_3_with_mv_new.csv\\\")\\n\\n    ppppp = df_for_unlabeled_set.drop([\\\"OFFER_STATUS\\\"], axis=1)\\n    ppppp = X_test_scaled\\n    x_realll = h2o.H2OFrame(ppppp)\\n\\n    ggl = my_model.predict(x_realll)\\n    gg = ggl[0].as_data_frame().values.flatten()\\n\\n    real_test = ppppp\\n    real_test[\\\"prediction\\\"] = gg\\n    test_set_id_col = df[np.isnan(df[\\\"OFFER_STATUS\\\"])][\\\"TEST_SET_ID\\\"]\\n    real_test = pd.concat([real_test[\\\"prediction\\\"], test_set_id_col], axis=1)\\n    real_test = real_test.rename(columns={\\\"TEST_SET_ID\\\": \\\"id\\\"})\\n\\n    real_test[\\\"prediction\\\"] = real_test[\\\"prediction\\\"].astype(int)\\n    real_test[\\\"id\\\"] = real_test[\\\"id\\\"].astype(int)\\n    print(ggl)\\n\\n    return real_test\\n\\n\\nlllresult = real_predict_h2o(\\n    # Submission 5\\n    # models_dir_path=\\\"h2o_models_with_data/results_ts_2022-01-29_13:09:41/model_bac_0.734/GBM_1_AutoML_31_20220129_130944\\\"\\n    # Submission 7\\n    models_dir_path=\\\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/model_withALL/StackedEnsemble_AllModels_2_AutoML_17_20220129_182118\\\"\\n)\\n\\nprint(lllresult.prediction.value_counts())\\n\\nlllresult.to_csv(\\\"predictions_versed_chimpanzee_7.csv\\\", header=True, index=False)\\n\\nlllresult\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def real_predict_h2o(models_dir_path):\n",
    "    from h2o.automl import H2OAutoML\n",
    "    import h2o\n",
    "\n",
    "    h2o.init(max_mem_size=\"32G\")\n",
    "\n",
    "    my_model = h2o.load_model(models_dir_path)\n",
    "    df = pd.read_csv(\"interim_data/df_completed_1_2_3_with_mv_new.csv\")\n",
    "\n",
    "    ppppp = df_for_unlabeled_set.drop([\"OFFER_STATUS\"], axis=1)\n",
    "    ppppp = X_test_scaled\n",
    "    x_realll = h2o.H2OFrame(ppppp)\n",
    "\n",
    "    ggl = my_model.predict(x_realll)\n",
    "    gg = ggl[0].as_data_frame().values.flatten()\n",
    "\n",
    "    real_test = ppppp\n",
    "    real_test[\"prediction\"] = gg\n",
    "    test_set_id_col = df[np.isnan(df[\"OFFER_STATUS\"])][\"TEST_SET_ID\"]\n",
    "    real_test = pd.concat([real_test[\"prediction\"], test_set_id_col], axis=1)\n",
    "    real_test = real_test.rename(columns={\"TEST_SET_ID\": \"id\"})\n",
    "\n",
    "    real_test[\"prediction\"] = real_test[\"prediction\"].astype(int)\n",
    "    real_test[\"id\"] = real_test[\"id\"].astype(int)\n",
    "    print(ggl)\n",
    "\n",
    "    return real_test\n",
    "\n",
    "\n",
    "lllresult = real_predict_h2o(\n",
    "    # Submission 5\n",
    "    # models_dir_path=\"h2o_models_with_data/results_ts_2022-01-29_13:09:41/model_bac_0.734/GBM_1_AutoML_31_20220129_130944\"\n",
    "    # Submission 7\n",
    "    models_dir_path=\"./h2o_models_with_data/results_ts_2022-01-29_18:21:15/model_withALL/StackedEnsemble_AllModels_2_AutoML_17_20220129_182118\"\n",
    ")\n",
    "\n",
    "print(lllresult.prediction.value_counts())\n",
    "\n",
    "lllresult.to_csv(\"predictions_versed_chimpanzee_7.csv\", header=True, index=False)\n",
    "\n",
    "lllresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lllresult.as_data_frame().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ggggggggggggg).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_tmp = pd.read_csv(\"interim_data/df_completed_1_2_3_with_mv_new.csv\")\n",
    "    test_set_id_col = df_tmp[np.isnan(df_tmp[\"OFFER_STATUS\"])][\"TEST_SET_ID\"]\n",
    "\n",
    "    ## Predict BREAK - start\n",
    "    real_test = x_test_data\n",
    "    real_test[\"prediction\"] = model2.predict(real_test)\n",
    "    real_test = pd.concat([real_test[\"prediction\"], test_set_id_col], axis=1)\n",
    "    real_test = real_test.rename(columns={\"TEST_SET_ID\": \"id\"})\n",
    "    real_test[\"prediction\"] = real_test[\"prediction\"].astype(int)\n",
    "    real_test[\"id\"] = real_test[\"id\"].astype(int)\n",
    "\n",
    "    ## Predict BREAK - stop\n",
    "\n",
    "    #real_test.to_csv(\"predictions_versed_chimpanzee_5.csv\", header=True, index=False)\n",
    "    print(real_test[[\"prediction\"]].value_counts())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Versed Chimpanzee.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
